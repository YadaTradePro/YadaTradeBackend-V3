# -*- coding: utf-8 -*-
# routes/data_fetching_routes.py
# Ù…Ø³Ø¦ÙˆÙ„: ØªØ¹Ø±ÛŒÙ Ø§Ù†Ø¯Ù¾ÙˆÛŒÙ†Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ ÙˆØ§Ú©Ø´ÛŒ Ø¯Ø§Ø¯Ù‡ØŒ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ØŒ Ùˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ÛŒ ML
# Ø§ÛŒÙ† ÙØ§ÛŒÙ„ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø± Ø¨Ø§ Ø³Ø§Ø®ØªØ§Ø± Ø¬Ø¯ÛŒØ¯ Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª.

import logging
import traceback
from datetime import date
import jdatetime

# âœ… Ø§ØµÙ„Ø§Ø­: jsonify Ø§Ø² flask Ø­Ø°Ù Ø´Ø¯ Ø²ÛŒØ±Ø§ Ø¨Ø§ Flask-RESTX Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ø¢Ù† Ù†ÛŒØ³Øª Ùˆ Ø¨Ø§Ø¹Ø« Ø®Ø·Ø§ÛŒ TypeError Ù…ÛŒâ€ŒØ´ÙˆØ¯.
from flask_restx import Namespace, Resource, fields, reqparse

from extensions import db
from sqlalchemy.orm import sessionmaker, Session

# =========================
# Namespace Ø¨Ø±Ø§ÛŒ Flask-RESTX
# =========================
data_ns = Namespace('data', description='Ø¹Ù…Ù„ÛŒØ§Øª ÙˆØ§Ú©Ø´ÛŒ Ø¯Ø§Ø¯Ù‡ØŒ ØªØ­Ù„ÛŒÙ„ Ùˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ')

# =========================
# ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ
# =========================
def get_session_local() -> Session:
    """ÛŒÚ© Session Ù…Ø­Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ØªØ§ Ø§Ø² Ú©Ø§Ù†ØªÚ©Ø³Øª Flask Ù…Ø³ØªÙ‚Ù„ Ø¨Ø§Ø´Ø¯."""
    SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=db.engine)
    return SessionLocal()

def parse_date(value: str) -> date | None:
    """ØªØ§Ø±ÛŒØ® Ø±Ø§ Ø§Ø² ÙØ±Ù…Øª Ø±Ø´ØªÙ‡ (YYYY-MM-DD) Ø´Ù…Ø³ÛŒ ÛŒØ§ Ù…ÛŒÙ„Ø§Ø¯ÛŒ Ø¨Ù‡ Ø´ÛŒØ¡ date Ù…ÛŒÙ„Ø§Ø¯ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    if not isinstance(value, str) or not value:
        return None
    try:
        # 1. ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø±Ø³ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ØªØ§Ø±ÛŒØ® Ù…ÛŒÙ„Ø§Ø¯ÛŒ
        return date.fromisoformat(value)
    except ValueError:
        # 2. ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø±Ø³ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ØªØ§Ø±ÛŒØ® Ø´Ù…Ø³ÛŒ
        try:
            j_year, j_month, j_day = map(int, value.split('-'))
            return jdatetime.date(j_year, j_month, j_day).togregorian()
        except Exception:
            return None

# =========================
# --- API Models for Swagger/RESTX Documentation ---
# =========================
historical_data_model = analysis_ns.model('HistoricalData', {
    'symbol_id': fields.String(required=True, description='Stock symbol ID (Persian short name)'),
    'symbol_name': fields.String(description='Stock symbol name (Persian short name)'),
    'jdate': fields.String(description='Persian date (YYYY-MM-DD)'),
    'date': fields.String(description='Gregorian date (YYYY-MM-DD)'),
    'open': fields.Float(description='Opening price'),
    'high': fields.Float(description='Highest price'),
    'low': fields.Float(description='Lowest price'),
    'close': fields.Float(description='Closing price'),
    'final': fields.Float(description='Final price'),
    'yesterday_price': fields.Float(description='Yesterday\'s closing price'),
    'volume': fields.Integer(description='Trading volume'),
    'value': fields.Float(description='Trading value'),
    'num_trades': fields.Integer(description='Number of trades'),
    'plc': fields.Float(description='Price change (last closing)'),
    'plp': fields.Float(description='Price change percentage (last closing)'),
    'pcc': fields.Float(description='Price change (final closing)'),
    'pcp': fields.Float(description='Price change percentage (final closing)'),
    'mv': fields.Float(description='Market Value'),
    'eps': fields.Float(description='Earnings Per Share'),
    'pe': fields.Float(description='Price to Earnings Ratio'),
    'buy_count_i': fields.Integer(description='Number of real buyer accounts'),
    'buy_count_n': fields.Integer(description='Number of legal buyer accounts'),
    'sell_count_i': fields.Integer(description='Number of real seller accounts'),
    'sell_count_n': fields.Integer(description='Number of legal seller accounts'),
    'buy_i_volume': fields.Integer(description='Real buyer volume'),
    'buy_n_volume': fields.Integer(description='Legal buyer volume'),
    'sell_i_volume': fields.Integer(description='Real seller volume'),
    'sell_n_volume': fields.Integer(description='Legal seller volume'),
    'zd1': fields.Integer(description='Demand count 1'),
    'qd1': fields.Integer(description='Demand volume 1'),
    'pd1': fields.Float(description='Demand price 1'),
    'zo1': fields.Integer(description='Supply count 1'),
    'qo1': fields.Integer(description='Supply volume 1'),
    'po1': fields.Float(description='Supply price 1'),
    'zd2': fields.Integer(description='Demand count 2'),
    'qd2': fields.Integer(description='Demand volume 2'),
    'pd2': fields.Float(description='Demand price 2'),
    'zo2': fields.Integer(description='Supply count 2'),
    'qo2': fields.Integer(description='Supply volume 2'),
    'po2': fields.Float(description='Supply price 2'),
    'zd3': fields.Integer(description='Demand count 3'),
    'qd3': fields.Integer(description='Demand volume 3'),
    'pd3': fields.Float(description='Demand price 3'),
    'zo3': fields.Integer(description='Supply count 3'),
    'qo3': fields.Integer(description='Supply volume 3'),
    'po3': fields.Float(description='Supply price 3'),
    'zd4': fields.Integer(description='Demand count 4'),
    'qd4': fields.Integer(description='Demand volume 4'),
    'pd4': fields.Float(description='Demand price 4'),
    'zo4': fields.Integer(description='Supply count 4'),
    'qo4': fields.Integer(description='Supply volume 4'),
    'po4': fields.Float(description='Supply price 4'),
    'zd5': fields.Integer(description='Demand count 5'),
    'qd5': fields.Integer(description='Demand volume 5'),
    'pd5': fields.Float(description='Demand price 5'),
    'zo5': fields.Integer(description='Supply count 5'),
    'qo5': fields.Integer(description='Supply volume 5'),
    'po5': fields.Float(description='Supply price 5')
})

comprehensive_symbol_data_model = analysis_ns.model('ComprehensiveSymbolData', {
    'symbol_id': fields.String(required=True, description='Stock symbol ID (Persian short name)'),
    'symbol_name': fields.String(required=True, description='Stock symbol name (Persian short name)'),
    'company_name': fields.String(description='Company name'),
    'isin': fields.String(description='ISIN code'),
    'market_type': fields.String(description='Market type'),
    'flow': fields.String(description='Flow (e.g., 1 for main market, 2 for secondary)'),
    'industry': fields.String(description='Industry name'),
    'capital': fields.String(description='Company capital'),
    'legal_shareholder_percentage': fields.Float(description='Legal Shareholder Percentage'),
    'real_shareholder_percentage': fields.Float(description='Real Shareholder Percentage'),
    'float_shares': fields.Float(description='Float shares'),
    'base_volume': fields.Float(description='Base volume'),
    'group_name': fields.String(description='Group name'),
    'description': fields.String(description='Symbol description'),
    'last_historical_update_date': fields.String(description='Last historical update date (YYYY-MM-DD)')
})

# Model for TechnicalIndicatorData
technical_indicator_model = analysis_ns.model('TechnicalIndicatorData', {
    'symbol_id': fields.String(required=True, description='Ø´Ù†Ø§Ø³Ù‡ Ù†Ù…Ø§Ø¯'),
    'jdate': fields.String(required=True, description='ØªØ§Ø±ÛŒØ® Ø´Ù…Ø³ÛŒ (YYYY-MM-DD)'),
    'close_price': fields.Float(description='Ù‚ÛŒÙ…Øª Ù¾Ø§ÛŒØ§Ù†ÛŒ'),
    'RSI': fields.Float(description='Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± RSI'),
    'MACD': fields.Float(description='Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± MACD'),
    'MACD_Signal': fields.Float(description='Ø®Ø· Ø³ÛŒÚ¯Ù†Ø§Ù„ MACD'),
    'MACD_Hist': fields.Float(description='Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù… MACD'),
    'SMA_20': fields.Float(description='Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú© Ø³Ø§Ø¯Ù‡ Û²Û° Ø±ÙˆØ²Ù‡'),
    'SMA_50': fields.Float(description='Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú© Ø³Ø§Ø¯Ù‡ ÛµÛ° Ø±ÙˆØ²Ù‡'),
    'Bollinger_High': fields.Float(description='Ø¨Ø§Ù†Ø¯ Ø¨Ø§Ù„Ø§ÛŒ Ø¨ÙˆÙ„ÛŒÙ†Ú¯Ø±'),
    'Bollinger_Low': fields.Float(description='Ø¨Ø§Ù†Ø¯ Ù¾Ø§ÛŒÛŒÙ† Ø¨ÙˆÙ„ÛŒÙ†Ú¯Ø±'),
    'Bollinger_MA': fields.Float(description='Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú© Ø¨Ø§Ù†Ø¯ Ø¨ÙˆÙ„ÛŒÙ†Ú¯Ø±'),
    'Volume_MA_20': fields.Float(description='Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú© Ø­Ø¬Ù… Û²Û° Ø±ÙˆØ²Ù‡'),
    'ATR': fields.Float(description='Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± ATR'),
    # New indicators added to the model
    'Stochastic_K': fields.Float(description='Stochastic Oscillator %K'),
    'Stochastic_D': fields.Float(description='Stochastic Oscillator %D'),
    'squeeze_on': fields.Boolean(description='ÙˆØ¶Ø¹ÛŒØª Squeeze Momentum'),
    'halftrend_signal': fields.Integer(description='Ø³ÛŒÚ¯Ù†Ø§Ù„ HalfTrend (1 Ø¨Ø±Ø§ÛŒ Ø®Ø±ÛŒØ¯)'),
    'resistance_level_50d': fields.Float(description='Ø³Ø·Ø­ Ù…Ù‚Ø§ÙˆÙ…Øª ÛµÛ° Ø±ÙˆØ²Ù‡'),
    'resistance_broken': fields.Boolean(description='Ø¢ÛŒØ§ Ù…Ù‚Ø§ÙˆÙ…Øª Ø´Ú©Ø³ØªÙ‡ Ø´Ø¯Ù‡ Ø§Ø³Øª')
})

fundamental_data_model = analysis_ns.model('FundamentalData', {
    'symbol_id': fields.String(required=True, description='Stock symbol ID (Persian short name)'),
    'last_updated': fields.DateTime(description='Last update timestamp'),
    
    # --- Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ú©Ù„Ø§Ø³ÛŒÚ© ---
    'eps': fields.Float(description='Earnings Per Share'),
    'pe': fields.Float(description='Price-to-Earnings Ratio'),
    'group_pe_ratio': fields.Float(description='Group Price-to-Earnings Ratio'),
    'psr': fields.Float(description='Price-to-Sales Ratio (PSR)'),
    'p_s_ratio': fields.Float(description='Price-to-Sales Ratio (P/S)'),
    'market_cap': fields.Float(description='Market Capitalization'),
    'base_volume': fields.Float(description='Base Volume'),
    'float_shares': fields.Float(description='Float Shares'),

    # --- Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„ Ø±ÙˆØ²Ø§Ù†Ù‡ (Ø¬Ø±ÛŒØ§Ù† Ø³Ø±Ù…Ø§ÛŒÙ‡) ---
    'jdate': fields.String(description='Jalali date of record'),
    'date': fields.Date(description='Gregorian date of record'),
    'real_power_ratio': fields.Float(description='Real Power Ratio (buyer/seller strength)'),
    'volume_ratio_20d': fields.Float(description='Volume ratio compared to 20-day average'),
    'daily_liquidity': fields.Float(description='Daily liquidity (final_price Ã— volume)')
})

# NEW: Model for ML Predictions (ADDED)
ml_prediction_model = analysis_ns.model('MLPredictionModel', {
    'id': fields.Integer(readOnly=True, description='The unique identifier of the prediction'),
    'symbol_id': fields.String(required=True, description='The ID of the stock symbol'),
    'symbol_name': fields.String(required=True, description='The name of the stock symbol'),
    'prediction_date': fields.String(required=True, description='Gregorian date when the prediction was made (YYYY-MM-DD)'),
    'jprediction_date': fields.String(required=True, description='Jalali date when the prediction was made (YYYY-MM-DD)'),
    'prediction_period_days': fields.Integer(description='Number of days for the prediction horizon'),
    'predicted_trend': fields.String(required=True, description='Predicted trend: UP, DOWN, or NEUTRAL'),
    'prediction_probability': fields.Float(required=True, description='Probability/confidence of the predicted trend (0.0 to 1.0)'),
    'predicted_price_at_period_end': fields.Float(description='Optional: Predicted price at the end of the period'),
    'actual_price_at_period_end': fields.Float(description='Actual price at the end of the prediction period'),
    'actual_trend_outcome': fields.String(description='Actual trend outcome: UP, DOWN, or NEUTRAL'),
    'is_prediction_accurate': fields.Boolean(description='True if predicted_trend matches actual_trend_outcome'),
    'signal_source': fields.String(description='Source of the signal, e.g., ML-Trend'),
    'model_version': fields.String(description='Version of the ML model used for prediction'),
    'created_at': fields.String(description='Timestamp of creation'),
    'updated_at': fields.String(description='Timestamp of last update'),
})


# =================================================================================
# --- Parsers for API Endpoints ---
# =================================================================================
# Ù¾Ø§Ø±Ø³Ø± Ø¨Ø±Ø§ÛŒ Ø§Ù†Ø¯Ù¾ÙˆÛŒÙ†Øª Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ú©Ø§Ù…Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
rebuild_parser = reqparse.RequestParser()
rebuild_parser.add_argument('batch_size', type=int, default=50, help='ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø¯Ø± Ù‡Ø± Ø¨Ú† Ù¾Ø±Ø¯Ø§Ø²Ø´ÛŒ')
rebuild_parser.add_argument('commit_batch_size', type=int, default=100, help='ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´â€ŒØ´Ø¯Ù‡ Ù‚Ø¨Ù„ Ø§Ø² Ù‡Ø± commit')

# Ù¾Ø§Ø±Ø³Ø± Ø¨Ø±Ø§ÛŒ Ø§Ù†Ø¯Ù¾ÙˆÛŒÙ†Øªâ€ŒÙ‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ú©Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ù„ÛŒØ³Øª Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø±Ø§ Ø¯Ø±ÛŒØ§ÙØª Ú©Ù†Ù†Ø¯
analysis_parser = reqparse.RequestParser()
analysis_parser.add_argument('limit', type=int, required=False, help='Ù…Ø­Ø¯ÙˆØ¯ÛŒØª ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´')
analysis_parser.add_argument('days_limit', type=int, required=False, help='Ù…Ø­Ø¯ÙˆØ¯ÛŒØª ØªØ¹Ø¯Ø§Ø¯ Ø±ÙˆØ²Ù‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ù‡Ø± Ù†Ù…Ø§Ø¯')
analysis_parser.add_argument(
    'specific_symbols',
    type=list,
    location='json',
    required=False,
    help='Ù„ÛŒØ³ØªÛŒ Ø§Ø² Ù†Ø§Ù… ÛŒØ§ Ú©Ø¯ Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ Ø®Ø§Øµ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ (Ù…Ø«Ø§Ù„: ["Ø®ÙˆØ¯Ø±Ùˆ", "Ø®Ø³Ø§Ù¾Ø§"])'
)

# Ù¾Ø§Ø±Ø³Ø± Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ ÛŒÚ© Ù†Ù…Ø§Ø¯
historical_data_parser = reqparse.RequestParser()
historical_data_parser.add_argument('days', type=int, default=61, help='ØªØ¹Ø¯Ø§Ø¯ Ø±ÙˆØ²Ù‡Ø§ÛŒ Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ Ø§Ø®ÛŒØ± Ø¨Ø±Ø§ÛŒ ÙˆØ§Ú©Ø´ÛŒ')
historical_data_parser.add_argument('start_date', type=str, help='ØªØ§Ø±ÛŒØ® Ø´Ø±ÙˆØ¹ Ø¨Ø§Ø²Ù‡ ( Ø´Ù…Ø³ÛŒ ÛŒØ§ Ù…ÛŒÙ„Ø§Ø¯ÛŒ YYYY-MM-DD)')
historical_data_parser.add_argument('end_date', type=str, help='ØªØ§Ø±ÛŒØ® Ù¾Ø§ÛŒØ§Ù† Ø¨Ø§Ø²Ù‡ ( Ø´Ù…Ø³ÛŒ ÛŒØ§ Ù…ÛŒÙ„Ø§Ø¯ÛŒ YYYY-MM-DD)')

# Ù¾Ø§Ø±Ø³Ø± Ø¨Ø±Ø§ÛŒ Ø§Ù†Ø¯Ù¾ÙˆÛŒÙ†Øª ØªÙˆÙ„ÛŒØ¯ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ ML
ml_generate_parser = reqparse.RequestParser()
ml_generate_parser.add_argument('prediction_date', type=str, required=False, help='ØªØ§Ø±ÛŒØ® Ù…ÛŒÙ„Ø§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ (YYYY-MM-DD)')
ml_generate_parser.add_argument('prediction_period_days', type=int, default=7, help='Ø¯ÙˆØ±Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ù‡ Ø±ÙˆØ²')


# =================================================================================
# --- Import Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ (Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø§Ø®ØªØ§Ø± Ø¬Ø¯ÛŒØ¯) ---
# =================================================================================
from services.data_fetcher import run_full_rebuild
from services.fetch_latest_brsapi_eod import update_daily_eod_from_brsapi
from services.historical_data_service import get_historical_data_for_symbol
from services.data_processing_and_analysis import run_technical_analysis
from services.ml_prediction_service import (
    generate_and_save_predictions_for_watchlist,
    update_ml_prediction_outcomes,
    get_all_ml_predictions,
    get_ml_predictions_for_symbol,
)

# =================================================================================
# --- Endpoints ---
# =================================================================================

# -----------------------------------------------
# Û±. Ø§Ù†Ø¯Ù¾ÙˆÛŒÙ†Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ ÙˆØ§Ú©Ø´ÛŒ Ùˆ Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡
# -----------------------------------------------
@data_ns.route('/fetch/full-rebuild')
class FullRebuildResource(Resource):
    @data_ns.doc('run_full_rebuild')
    @data_ns.expect(rebuild_parser)
    def post(self):
        """
        Ø§Ø¬Ø±Ø§ÛŒ Ú†Ø±Ø®Ù‡ Ú©Ø§Ù…Ù„ Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø² TSETMC.
        Ø§ÛŒÙ† Ø¹Ù…Ù„ÛŒØ§Øª Ø³Ù†Ú¯ÛŒÙ†ØŒ ØªÙ…Ø§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡ Ø±Ø§ Ù¾Ø§Ú© Ú©Ø±Ø¯Ù‡ Ùˆ Ø§Ø² Ù†Ùˆ ÙˆØ§Ú©Ø´ÛŒ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        """
        logger = logging.getLogger(__name__)
        logger.info("ğŸŒ€ [API] Starting full data rebuild process...")
        try:
            args = rebuild_parser.parse_args()
            result = run_full_rebuild(
                batch_size=args['batch_size'],
                commit_batch_size=args['commit_batch_size']
            )
            return result, 200
        except Exception as e:
            logger.error(f"âŒ Fatal error in full-rebuild endpoint: {e}\n{traceback.format_exc()}")
            return {"status": "error", "message": f"An unexpected error occurred: {e}"}, 500







@data_ns.route('/fetch/daily-eod-update')
class DailyEODUpdateResource(Resource):
    @data_ns.doc('run_daily_eod_update')
    def post(self):
        """
        ÙˆØ§Ú©Ø´ÛŒ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø¢Ø®Ø±ÛŒÙ† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø±ÙˆØ²Ø§Ù†Ù‡ (EOD) Ùˆ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØµÙ Ø³ÙØ§Ø±Ø´ Ø§Ø² BRSAPI.
        Ø§ÛŒÙ† Ø¹Ù…Ù„ÛŒØ§Øª Ø¨Ø§ÛŒØ¯ Ø±ÙˆØ²Ø§Ù†Ù‡ Ù¾Ø³ Ø§Ø² Ù¾Ø§ÛŒØ§Ù† Ø¨Ø§Ø²Ø§Ø± Ø§Ø¬Ø±Ø§ Ø´ÙˆØ¯.
        """
        logger = logging.getLogger(__name__)
        logger.info("âš¡ï¸ [API] Starting daily EOD update from BRSAPI...")
        session = get_session_local()
        try:
            count, message = update_daily_eod_from_brsapi(session)
            return {"status": "success", "updated_symbols": count, "message": message}, 200
        except Exception as e:
            logger.error(f"âŒ Error in daily-eod-update endpoint: {e}\n{traceback.format_exc()}")
            return {"status": "error", "message": str(e)}, 500
        finally:
            session.close()






# -----------------------------------
# Û². Ø§Ù†Ø¯Ù¾ÙˆÛŒÙ†Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡
# -----------------------------------
@data_ns.route('/analysis/technical')
class RunTechnicalAnalysisResource(Resource):
    @data_ns.doc('run_technical_analysis')
    @data_ns.expect(analysis_parser)
    def post(self):
        """
        Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ùˆ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ú©Ù†Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§Ø¯Ù‡Ø§.
        Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ù… Ù†Ù…Ø§Ø¯Ù‡Ø§ ÛŒØ§ Ù„ÛŒØ³Øª Ù…Ø´Ø®ØµÛŒ Ø§Ø¬Ø±Ø§ Ø´ÙˆØ¯.
        """
        logger = logging.getLogger(__name__)
        logger.info("ğŸ“Š [API] Running comprehensive technical analysis...")
        session = get_session_local()
        try:
            args = analysis_parser.parse_args()
            processed_count, message = run_technical_analysis(
                db_session=session,
                limit=args.get('limit'),
                specific_symbols_list=args.get('specific_symbols'),
                days_limit=args.get('days_limit')
            )
            return {"status": "success", "processed_symbols": processed_count, "message": message}, 200
        except Exception as e:
            logger.error(f"âŒ Error in technical analysis endpoint: {e}\n{traceback.format_exc()}")
            return {"status": "error", "message": str(e)}, 500
        finally:
            session.close()



# ------------------------------------------
# Û³. Ø§Ù†Ø¯Ù¾ÙˆÛŒÙ†Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ ML
# ------------------------------------------
@data_ns.route('/ml-predictions/generate')
class GenerateMLPredictionsResource(Resource):
    @data_ns.doc('generate_ml_predictions')
    @data_ns.expect(ml_generate_parser)
    def post(self):
        """
        ØªÙˆÙ„ÛŒØ¯ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ÛŒ Ø±ÙˆÙ†Ø¯ Ø¢ÛŒÙ†Ø¯Ù‡ Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ ML.
        """
        logger = logging.getLogger(__name__)
        logger.info("ğŸ¤– [API] Generating ML trend predictions...")
        try:
            args = ml_generate_parser.parse_args()
            prediction_date = parse_date(args['prediction_date']) if args['prediction_date'] else date.today()

            success, message = generate_and_save_predictions_for_watchlist(
                prediction_date_greg=prediction_date,
                prediction_period_days=args['prediction_period_days']
            )
            if success:
                # âœ… Ø§ØµÙ„Ø§Ø­: Ø­Ø°Ù jsonify
                return {"status": "success", "message": message}, 200
            else:
                # âœ… Ø§ØµÙ„Ø§Ø­: Ø­Ø°Ù jsonify
                return {"status": "error", "message": message}, 400
        except Exception as e:
            logger.error(f"âŒ Error in generate-ml-predictions endpoint: {e}\n{traceback.format_exc()}")
            # âœ… Ø§ØµÙ„Ø§Ø­: Ø­Ø°Ù jsonify
            return {"status": "error", "message": str(e)}, 500

@data_ns.route('/ml-predictions/update-outcomes')
class UpdateMLOutcomesResource(Resource):
    @data_ns.doc('update_ml_outcomes')
    def post(self):
        """
        Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù†ØªØ§ÛŒØ¬ ÙˆØ§Ù‚Ø¹ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ÛŒ Ú¯Ø°Ø´ØªÙ‡ Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¯Ù‚Øª Ù…Ø¯Ù„.
        Ø§ÛŒÙ† Ø§Ù†Ø¯Ù¾ÙˆÛŒÙ†Øª Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ ØµÙˆØ±Øª Ø¯ÙˆØ±Ù‡â€ŒØ§ÛŒ (Ø±ÙˆØ²Ø§Ù†Ù‡) ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ø´ÙˆØ¯.
        """
        logger = logging.getLogger(__name__)
        logger.info("ğŸ¯ [API] Updating ML prediction outcomes...")
        try:
            success, message = update_ml_prediction_outcomes()
            if success:
                # âœ… Ø§ØµÙ„Ø§Ø­: Ø­Ø°Ù jsonify
                return {"status": "success", "message": message}, 200
            else:
                # âœ… Ø§ØµÙ„Ø§Ø­: Ø­Ø°Ù jsonify
                return {"status": "error", "message": message}, 400
        except Exception as e:
            logger.error(f"âŒ Error in update-ml-outcomes endpoint: {e}\n{traceback.format_exc()}")
            # âœ… Ø§ØµÙ„Ø§Ø­: Ø­Ø°Ù jsonify
            return {"status": "error", "message": str(e)}, 500

@data_ns.route('/ml-predictions/all')
class GetAllMLPredictionsResource(Resource):
    @data_ns.doc('get_all_ml_predictions')
    @data_ns.marshal_list_with(ml_prediction_model)
    def get(self):
        """Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª ØªÙ…Ø§Ù… Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ÛŒ ML Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³."""
        logger = logging.getLogger(__name__)
        logger.info("ğŸ“š [API] Fetching all ML predictions...")
        try:
            predictions = get_all_ml_predictions()
            return predictions, 200
        except Exception as e:
            logger.error(f"âŒ Error getting all ML predictions: {e}\n{traceback.format_exc()}")
            # âœ… Ø§ØµÙ„Ø§Ø­: Ø­Ø°Ù jsonify
            return {"status": "error", "message": str(e)}, 500

@data_ns.route('/ml-predictions/<string:symbol_id>')
class GetSymbolMLPredictionResource(Resource):
    @data_ns.doc('get_symbol_ml_prediction')
    @data_ns.marshal_with(ml_prediction_model)
    def get(self, symbol_id: str):
        """Ø¯Ø±ÛŒØ§ÙØª Ø¢Ø®Ø±ÛŒÙ† Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ ML Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ù†Ù…Ø§Ø¯ Ù…Ø´Ø®Øµ."""
        logger = logging.getLogger(__name__)
        logger.info(f"ğŸ” [API] Fetching latest ML prediction for symbol: {symbol_id}")
        try:
            prediction = get_ml_predictions_for_symbol(symbol_id)
            if prediction:
                return prediction, 200
            return {"message": f"No prediction found for symbol {symbol_id}"}, 404
        except Exception as e:
            logger.error(f"âŒ Error getting ML prediction for {symbol_id}: {e}\n{traceback.format_exc()}")
            # âœ… Ø§ØµÙ„Ø§Ø­: Ø­Ø°Ù jsonify
            return {"status": "error", "message": str(e)}, 500





# ------------------------------------
# Û´. Ø§Ù†Ø¯Ù¾ÙˆÛŒÙ†Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡
# ------------------------------------

@analysis_ns.route('/stock-history/<string:symbol_input>') # ØªØºÛŒÛŒØ± Ù†Ø§Ù… Ù…ØªØºÛŒØ± Ø¨Ù‡ symbol_input
@analysis_ns.param('symbol_input', 'Ø´Ù†Ø§Ø³Ù‡ ÛŒØ§ Ù†Ø§Ù… Ù†Ù…Ø§Ø¯ (Ù…Ø«Ø§Ù„: Ø®ÙˆØ¯Ø±Ùˆ)')
class StockHistoryResource(Resource):
    @analysis_ns.doc(security='Bearer Auth', parser=historical_data_parser)
    @jwt_required()
    @analysis_ns.response(200, 'Historical data fetched successfully')
    @analysis_ns.response(400, 'Invalid date format')
    @analysis_ns.response(404, 'No data found for symbol')
    def get(self, symbol_input): # ØªØºÛŒÛŒØ± Ù†Ø§Ù… Ù…ØªØºÛŒØ± Ø¨Ù‡ symbol_input
        """
        ÙˆØ§Ú©Ø´ÛŒ Ø³Ø§Ø¨Ù‚Ù‡ Ù…Ø¹Ø§Ù…Ù„Ø§Øª (Historical Data) ÛŒÚ© Ù†Ù…Ø§Ø¯ Ù…Ø´Ø®Øµ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª ÙÛŒÙ„ØªØ± Ø²Ù…Ø§Ù†ÛŒ.
        """
        try:
            args = historical_data_parser.parse_args()
            days = args['days']
            start_date_str = args['start_date']
            end_date_str = args['end_date']
            
            start_date = parse_date(start_date_str)
            end_date = parse_date(end_date_str)

            if (start_date_str and start_date is None) or (end_date_str and end_date is None):
                analysis_ns.abort(400, "Invalid date format. Please use YYYY-MM-DD (Gregorian or Jalali).")

            # ğŸš€ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ ØªØ§Ø¨Ø¹ Ø³Ø±ÙˆÛŒØ³
            history_data = get_historical_data_for_symbol(
                symbol_input, # Ø§Ø² symbol_input Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
                start_date=start_date, 
                end_date=end_date, 
                days=days
            )
            
            if history_data is None:
                current_app.logger.error(f"Service returned None for {symbol_input}")
                analysis_ns.abort(500, "Internal server error during data retrieval. Service returned None.")

            if not history_data:
                # Ø§ÛŒÙ† Ø®Ø· Ø¨Ø§Ø¹Ø« Ø§ÛŒØ¬Ø§Ø¯ 404 Ù…ÛŒâ€ŒØ´ÙˆØ¯.
                analysis_ns.abort(404, f"No historical data found for symbol: {symbol_input} in the specified range.")

            return {"history": history_data}, 200
            
        except HTTPException as e:
            # âœ… FIX: Ø§Ú¯Ø± Ø®Ø·Ø§ ÛŒÚ© Ø®Ø·Ø§ÛŒ HTTP (Ù…Ø«Ù„ 404 ÛŒØ§ 400) Ø¨Ø§Ø´Ø¯ØŒ Ø¢Ù† Ø±Ø§ Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø¨Ø§Ù„Ø§ Ù…ÛŒâ€ŒØ§Ù†Ø¯Ø§Ø²ÛŒÙ….
            raise e
            
        except Exception as e:
            # Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ø¯ÛŒÚ¯Ø± (Ù…Ø«Ù„ Ø®Ø·Ø§ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ ÛŒØ§ Ù…Ù†Ø·Ù‚ÛŒ)
            current_app.logger.error(f"An unexpected critical error occurred for {symbol_input}: {e}", exc_info=True)
            analysis_ns.abort(500, f"An unexpected critical error occurred: {str(e)}")


            

# ---------------------------
# Health Check
# ---------------------------
@data_ns.route('/health')
class HealthResource(Resource):
    def get(self):
        """Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¶Ø¹ÛŒØª Ø³Ù„Ø§Ù…Øª Ø³Ø±ÙˆÛŒØ³ Ùˆ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³."""
        db_ok = False
        try:
            db.session.execute('SELECT 1')
            db_ok = True
        except Exception as e:
            logging.getLogger(__name__).error(f"Health check DB connection failed: {e}")

        # âœ… Ø§ØµÙ„Ø§Ø­: Ø­Ø°Ù jsonify
        return {
            "status": "ok",
            "message": "API is running.",
            "db_connection": "successful" if db_ok else "failed"
        }, 200
