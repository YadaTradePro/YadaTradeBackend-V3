# -*- coding: utf-8 -*-
# train_model.py - Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ÙˆÙ†Ø¯ Ø³Ù‡Ø§Ù…

import pandas as pd
import numpy as np
import jdatetime
from datetime import datetime
import logging
import os
import joblib
import sys
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler
from sqlalchemy.orm import sessionmaker
from config import Config
from sqlalchemy import create_engine, text

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯â€ŒÙ†ÙˆÛŒØ³ÛŒ ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø³ÛŒØ±Ø¯Ù‡ÛŒ ---
current_script_dir = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(current_script_dir)

if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

# ØªØ´Ø®ÛŒØµ Ù…Ø­ÛŒØ·
MODELS_DIR = '/app/models' if os.path.exists('/app') else os.path.join(PROJECT_ROOT, "models")
os.makedirs(MODELS_DIR, exist_ok=True)

print("âœ… Ù…Ø­ÛŒØ· ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯:")
print("ðŸ“ Ù…Ø³ÛŒØ± Ù¾Ø±ÙˆÚ˜Ù‡:", PROJECT_ROOT)
print("ðŸ¤– Ù…Ø³ÛŒØ± Ù…Ø¯Ù„â€ŒÙ‡Ø§:", MODELS_DIR)
print("ðŸ  Ù…Ø­ÛŒØ·:", "Docker" if os.path.exists('/app') else "Local Machine")

# Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø±
SERVICES_PATH = os.path.join(PROJECT_ROOT, 'services')
DATA_PATH = os.path.join(PROJECT_ROOT, 'data')

if SERVICES_PATH not in sys.path:
    sys.path.insert(0, SERVICES_PATH)

# Ø§ÛŒÙ…Ù¾ÙˆØ±Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§
try:
    from models import HistoricalData, ComprehensiveSymbolData
except ImportError as e:
    logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§ÛŒÙ…Ù¾ÙˆØ±Øª Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§: {e}")
    sys.exit(1)

# Ø¯ÛŒØªØ§Ø¨ÛŒØ³
DATABASE_URL = f"sqlite:///{os.path.join(PROJECT_ROOT, 'app.db')}"
engine = create_engine(DATABASE_URL)
Session = sessionmaker(bind=engine)

# --- Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ ---
def calculate_rsi(series, window=14):
    if not isinstance(series, pd.Series):
        series = pd.Series(series)
    delta = series.diff()
    gains = delta.where(delta > 0, 0)
    losses = -delta.where(delta < 0, 0)
    avg_gain = gains.ewm(com=window - 1, min_periods=window).mean()
    avg_loss = losses.ewm(com=window - 1, min_periods=window).mean()
    rs = avg_gain / avg_loss.replace(0, np.nan)
    rsi = 100 - (100 / (1 + rs))
    return rsi

def calculate_macd(series, fast_window=12, slow_window=26, signal_window=9):
    if not isinstance(series, pd.Series):
        series = pd.Series(series)
    ema_fast = series.ewm(span=fast_window, min_periods=fast_window).mean()
    ema_slow = series.ewm(span=slow_window, min_periods=slow_window).mean()
    macd_line = ema_fast - ema_slow
    signal_line = macd_line.ewm(span=signal_window, min_periods=signal_window).mean()
    macd_histogram = macd_line - signal_line
    return macd_line, signal_line, macd_histogram

def calculate_sma(series, window=20):
    if not isinstance(series, pd.Series):
        series = pd.Series(series)
    return series.rolling(window=window).mean()

def calculate_volume_ma(series, window=5):
    if not isinstance(series, pd.Series):
        series = pd.Series(series)
    return series.rolling(window=window).mean()

def calculate_atr(high, low, close, window=14):
    tr1 = high - low
    tr2 = abs(high - close.shift())
    tr3 = abs(low - close.shift())
    true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
    atr = true_range.ewm(com=window - 1, min_periods=window).mean()
    return atr

# --- Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ ---
def _perform_feature_engineering(df_symbol_hist, symbol_id_for_logging="N/A"):
    # ðŸ’¡ Ø§ØµÙ„Ø§Ø­ 1: df_symbol_hist Ø§Ú©Ù†ÙˆÙ† Ø¯Ø§Ø±Ø§ÛŒ Ø³ØªÙˆÙ† 'gregorian_date' Ø§Ø³Øª.
    # Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø±Ø§ ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ§Ø±ÛŒØ® Ù†Ù…Ø§ÛŒÙ‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´ÙˆÙ†Ø¯.
    # Ø´Ø§Ø®Øµ ØªØ§Ø±ÛŒØ® Ø¨Ø§ÛŒØ¯ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯ Ø¨Ø§Ø´Ø¯ ØªØ§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± Ø®Ø·Ø§ Ø±Ø® Ù†Ø¯Ù‡Ø¯.
    df_processed = df_symbol_hist.sort_values(by='gregorian_date').set_index('gregorian_date').copy()

    # Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§
    df_processed['rsi'] = calculate_rsi(df_processed['close'])
    macd_line, signal_line, _ = calculate_macd(df_processed['close'])
    df_processed['macd'] = macd_line
    df_processed['signal_line'] = signal_line
    df_processed['sma_20'] = calculate_sma(df_processed['close'], window=20)
    df_processed['sma_50'] = calculate_sma(df_processed['close'], window=50)
    df_processed['volume_ma_5_day'] = calculate_volume_ma(df_processed['volume'], window=5)
    df_processed['atr'] = calculate_atr(df_processed['high'], df_processed['low'], df_processed['close'])

    # Stochastic Oscillator
    window_stoch = 14
    df_processed['lowest_low_stoch'] = df_processed['low'].rolling(window=window_stoch).min()
    df_processed['highest_high_stoch'] = df_processed['high'].rolling(window=window_stoch).max()
    denominator_stoch = df_processed['highest_high_stoch'] - df_processed['lowest_low_stoch']
    df_processed['%K'] = ((df_processed['close'] - df_processed['lowest_low_stoch']) / denominator_stoch.replace(0, np.nan)) * 100
    df_processed['%D'] = df_processed['%K'].rolling(window=3).mean()

    # OBV
    close_shifted = df_processed['close'].shift(1)
    volume_numeric = pd.to_numeric(df_processed['volume'], errors='coerce').fillna(0)
    df_processed['obv'] = (np.where(df_processed['close'] > close_shifted, volume_numeric,
                                   np.where(df_processed['close'] < close_shifted, -volume_numeric, 0))).cumsum()

    # ØªØºÛŒÛŒØ±Ø§Øª Ù‚ÛŒÙ…Øª Ùˆ Ø­Ø¬Ù…
    df_processed['price_change_1d'] = df_processed['close'].pct_change()
    df_processed['volume_change_1d'] = df_processed['volume'].pct_change()
    df_processed['price_change_3d'] = df_processed['close'].pct_change(periods=3)
    df_processed['volume_change_3d'] = df_processed['volume'].pct_change(periods=3)
    df_processed['price_change_5d'] = df_processed['close'].pct_change(periods=5)
    df_processed['volume_change_5d'] = df_processed['volume'].pct_change(periods=5)

    # Ù†Ø³Ø¨Øª Ù‚Ø¯Ø±Øª Ø®Ø±ÛŒØ¯Ø§Ø± Ø­Ù‚ÛŒÙ‚ÛŒ
    buy_i_vol = pd.to_numeric(df_processed['buy_i_volume'], errors='coerce').fillna(0)
    sell_i_vol = pd.to_numeric(df_processed['sell_i_volume'], errors='coerce').fillna(0)
    buy_count_i = pd.to_numeric(df_processed['buy_count_i'], errors='coerce').fillna(0)
    sell_count_i = pd.to_numeric(df_processed['sell_count_i'], errors='coerce').fillna(0)
    denominator_buy_power = (sell_i_vol * sell_count_i)
    df_processed['individual_buy_power_ratio'] = (buy_i_vol * buy_count_i) / denominator_buy_power.replace(0, np.nan)

    df_processed.replace([np.inf, -np.inf], np.nan, inplace=True)
    df_processed = df_processed.ffill().bfill().fillna(0)

    # Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
    feature_columns = [
        'open', 'high', 'low', 'close', 'volume', 'num_trades',
        'rsi', 'macd', 'signal_line', 'sma_20', 'sma_50', 'volume_ma_5_day', 'atr',
        '%K', '%D', 'obv',
        'price_change_1d', 'volume_change_1d',
        'price_change_3d', 'volume_change_3d',
        'price_change_5d', 'volume_change_5d',
        'individual_buy_power_ratio',
        'buy_count_i', 'sell_count_i', 'buy_i_volume', 'sell_i_volume',
        'zd1', 'qd1', 'pd1', 'zo1', 'qo1', 'po1',
        'zd2', 'qd2', 'pd2', 'zo2', 'qo2', 'po2',
        'zd3', 'qd3', 'pd3', 'zo3', 'qo3', 'po3',
        'zd4', 'qd4', 'pd4', 'zo4', 'qo4', 'po4',
        'zd5', 'qd5', 'pd5', 'zo5', 'qo5', 'po5'
    ]
    
    # ðŸ’¡ Ø§ØµÙ„Ø§Ø­ 2: Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ø´Ø§Ø®Øµ 'gregorian_date' Ø¨Ù‡ Ø³ØªÙˆÙ† Ø¨Ø±Ø§ÛŒ Ø§Ø¯ØºØ§Ù…
    features_df = df_processed[feature_columns].copy().reset_index()
    features_df.fillna(0, inplace=True)
    features_df.replace([np.inf, -np.inf], 0, inplace=True)
    return features_df

# --- Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ ---
def train_model():
    logger.info("Ø¯Ø± Ø­Ø§Ù„ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ùˆ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ ...")
    CHUNK_SIZE = 10000
    all_chunks_df = []
    try:
        query = "SELECT * FROM stock_data ORDER BY symbol_id, date"
        with engine.connect() as conn:
            result_proxy = conn.execute(text(query))
            while True:
                chunk_records = result_proxy.fetchmany(CHUNK_SIZE)
                if not chunk_records:
                    break
                chunk_df = pd.DataFrame(chunk_records, columns=result_proxy.keys())
                logger.info(f"Ø¯Ø³ØªÙ‡ Ø¬Ø¯ÛŒØ¯ Ø¨Ø§ {len(chunk_df)} Ø±Ø¯ÛŒÙ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯.")
                all_chunks_df.append(chunk_df)

        if not all_chunks_df:
            logger.error("Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù†ÛŒØ³Øª.")
            return

        df_hist = pd.concat(all_chunks_df, ignore_index=True)
        logger.info(f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù†Ù‚Ø§Ø· Ø¯Ø§Ø¯Ù‡: {len(df_hist)}")
        df_hist['gregorian_date'] = pd.to_datetime(df_hist['date'])
        df_hist.drop(columns=['_sa_instance_state'], errors='ignore', inplace=True)

        numeric_cols = ['open', 'high', 'low', 'close', 'final', 'yesterday_price', 'volume', 'value', 'num_trades',
                         'plc', 'plp', 'pcc', 'pcp', 'mv',
                         'buy_count_i', 'buy_count_n', 'sell_count_i', 'sell_count_n',
                         'buy_i_volume', 'buy_n_volume', 'sell_i_volume', 'sell_n_volume',
                         'zd1', 'qd1', 'pd1', 'zo1', 'qo1', 'po1',
                         'zd2', 'qd2', 'pd2', 'zo2', 'qo2', 'po2',
                         'zd3', 'qd3', 'pd3', 'zo3', 'qo3', 'po3',
                         'zd4', 'qd4', 'pd4', 'zo4', 'qo4', 'po4',
                         'zd5', 'qd5', 'pd5', 'zo5', 'qo5', 'po5']
        for col in numeric_cols:
            if col in df_hist.columns:
                df_hist[col] = pd.to_numeric(df_hist[col], errors='coerce')

        df_hist.dropna(subset=['close', 'volume', 'high', 'low', 'open'], inplace=True)
        
        # ðŸ’¡ Ø§ØµÙ„Ø§Ø­ Ú©Ù„ÛŒØ¯ÛŒ 3: Ø³Ø§Ø®Øª Ø´Ø§Ø®Øµ Ú†Ù†Ø¯ Ø³Ø·Ø­ÛŒ Ø§Ø² symbol_id Ùˆ gregorian_date Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯ Ø¨ÙˆØ¯Ù†
        # Ø§ÛŒÙ† Ú©Ø§Ø± Ù‡Ø± Ø±Ø¯ÛŒÙ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯ Ø¨Ø§ ØªØ±Ú©ÛŒØ¨ Ù†Ù…Ø§Ø¯ Ùˆ ØªØ§Ø±ÛŒØ® Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        df_hist.set_index(['symbol_id', 'gregorian_date'], inplace=True)
        # ðŸ“¢ Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ú©Ø§Ù…Ù„Ø§Ù‹ ØªÚ©Ø±Ø§Ø±ÛŒ (Ø¯Ø± ÛŒÚ© Ù†Ù…Ø§Ø¯ Ùˆ ÛŒÚ© ØªØ§Ø±ÛŒØ®)ØŒ ØªÙ†Ù‡Ø§ ÛŒÚ©ÛŒ Ø§Ø² Ø¢Ù†â€ŒÙ‡Ø§ Ø­ÙØ¸ Ù…ÛŒâ€ŒØ´ÙˆØ¯
        df_hist.drop_duplicates(inplace=True) 

        # Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ
        all_features_df = pd.DataFrame()
        # ðŸ’¡ Ø§ØµÙ„Ø§Ø­ 4: Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ symbol_id Ø§Ø² Ø·Ø±ÛŒÙ‚ Ø´Ø§Ø®Øµ MultiIndex (Ø±ÙØ¹ KeyError)
        for symbol_id in df_hist.index.get_level_values('symbol_id').unique():
            # ðŸ’¡ Ø§ØµÙ„Ø§Ø­ 5: Ø§Ù†ØªØ®Ø§Ø¨ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…Ø§Ø¯ Ùˆ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ Ø¨Ù‡ Ø³ØªÙˆÙ† (Ø±ÙØ¹ Ø®Ø·Ø§ÛŒ reindex)
            # .loc[symbol_id] Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÛŒÚ© Ù†Ù…Ø§Ø¯ Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ .reset_index() symbol_id Ùˆ gregorian_date 
            # Ø±Ø§ Ø¨Ù‡ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ù…ÙˆÙ„ÛŒ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
            df_symbol = df_hist.loc[symbol_id].copy().reset_index()
            
            # Ø§ØµÙ„Ø§Ø­: Ø¨Ø±Ø±Ø³ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§ÙÛŒ Ø¯Ø§Ø¯Ù‡ Ù‚Ø¨Ù„ Ø§Ø² Ù¾Ø±Ø¯Ø§Ø²Ø´
            if len(df_symbol) < 60:
                logger.warning(f"Ù¾Ø±Ø´ Ø§Ø² Ù†Ù…Ø§Ø¯ {symbol_id}: Ø¯Ø§Ø¯Ù‡ Ú©Ø§ÙÛŒ ({len(df_symbol)} Ø±ÙˆØ²) Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
                continue
                
            features_df = _perform_feature_engineering(df_symbol, symbol_id)
            if features_df.empty:
                continue
                
            # ðŸ’¡ Ø§ØµÙ„Ø§Ø­ 6: Ø§Ø¯ØºØ§Ù… jdate Ùˆ close_hist Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² merge
            # features_df Ø§Ø² Ù‚Ø¨Ù„ Ø´Ø§Ù…Ù„ 'gregorian_date' Ø§Ø³Øª (Ø§Ø² reset_index Ø¯Ø± ØªØ§Ø¨Ø¹ Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ)
            
            # ÙÙ‚Ø· Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ Ø§Ø¯ØºØ§Ù… Ø±Ø§ Ø§Ø² df_symbol Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯
            jdate_close_df = df_symbol[['gregorian_date', 'jdate', 'close']].copy()
            jdate_close_df.rename(columns={'close': 'close_hist'}, inplace=True)
            
            # Ø§Ø¯ØºØ§Ù… Ø¨Ø± Ø§Ø³Ø§Ø³ gregorian_date
            features_df = pd.merge(
                features_df, 
                jdate_close_df, 
                on='gregorian_date', 
                how='left'
            )
            
            features_df['symbol_id'] = symbol_id
            
            # ðŸ’¡ Ø§ØµÙ„Ø§Ø­ 7: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ignore_index=True Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªØ¯Ø§Ø®Ù„ Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ Ø¯Ø± concate Ù†Ù‡Ø§ÛŒÛŒ
            all_features_df = pd.concat([all_features_df, features_df], ignore_index=True)

        if all_features_df.empty:
            logger.error("Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø§Ù‚ÛŒ Ù†Ù…Ø§Ù†Ø¯Ù‡.")
            return

        # ðŸ’¡ Ø§ØµÙ„Ø§Ø­ 8: ØªÙ†Ø¸ÛŒÙ… Ø´Ø§Ø®Øµ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ symbol_id Ùˆ gregorian_date
        all_features_df.set_index(['symbol_id', 'gregorian_date'], inplace=True)
        all_features_df.sort_index(inplace=True) 

        all_features_df['future_close'] = all_features_df.groupby(level='symbol_id')['close_hist'].shift(-7)
        all_features_df['percentage_change'] = ((all_features_df['future_close'] - all_features_df['close_hist']) / all_features_df['close_hist']) * 100
        
        # Ø§ØµÙ„Ø§Ø­: Ø­Ø°Ù Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø±Ø§ÛŒ NaN Ù¾Ø³ Ø§Ø² Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ Ùˆ Ø¨Ø±Ú†Ø³Ø¨â€ŒÚ¯Ø°Ø§Ø±ÛŒ
        initial_count = len(all_features_df)
        all_features_df.dropna(subset=['percentage_change'], inplace=True)
        dropped_count = initial_count - len(all_features_df)
        logger.info(f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù†Ù‚Ø§Ø· Ø¯Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ù¾Ø³ Ø§Ø² ØªØ¹Ø±ÛŒÙ Ø¨Ø±Ú†Ø³Ø¨: {len(all_features_df)} (Ø­Ø°Ù Ø´Ø¯Ù‡: {dropped_count})")

        if all_features_df.empty:
            logger.error("Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø§Ù‚ÛŒ Ù†Ù…Ø§Ù†Ø¯Ù‡.")
            return

        lower_bound = all_features_df['percentage_change'].quantile(0.33)
        upper_bound = all_features_df['percentage_change'].quantile(0.66)
        def get_trend(change):
            if change > upper_bound:
                return 'Uptrend'
            elif change < lower_bound:
                return 'Downtrend'
            else:
                return 'Sideways'
        all_features_df['trend'] = all_features_df['percentage_change'].apply(get_trend)

        logger.info(f"Ø¢Ø³ØªØ§Ù†Ù‡ Ù†Ø²ÙˆÙ„ÛŒ (Quantile 33%): {lower_bound:.2f}%")
        logger.info(f"Ø¢Ø³ØªØ§Ù†Ù‡ ØµØ¹ÙˆØ¯ÛŒ (Quantile 66%): {upper_bound:.2f}%")
        logger.info("ØªÙˆØ²ÛŒØ¹ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ (Ù¾Ø³ Ø§Ø² Ø¨Ø±Ú†Ø³Ø¨â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø¨Ø§ Ú©ÙˆØ§Ù†ØªØ§ÛŒÙ„):")
        logger.info(all_features_df['trend'].value_counts(normalize=True))

        # ðŸ’¡ Ø§ØµÙ„Ø§Ø­ 9: Ø­Ø°Ù Ø³Ø·Ø­ Ø´Ø§Ø®Øµ 'symbol_id' Ø§Ø² X Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ
        X = all_features_df.drop(columns=['jdate', 'close_hist', 'future_close', 'percentage_change', 'trend'])
        # X Ø¨Ø§ÛŒØ¯ Ø´Ø§Ù…Ù„ 'gregorian_date' Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø¨Ø®Ø´ÛŒ Ø§Ø² Ø´Ø§Ø®Øµ MultiIndex Ø¨Ø§Ø´Ø¯
        X = X.droplevel(level='symbol_id') 
        y = all_features_df['trend']
        y.index = y.index.droplevel(level='symbol_id')


        # ØªØ±Ú©ÛŒØ¨ X Ùˆ y
        df_combined = X.copy()
        df_combined["target"] = y
        # ðŸ’¡ Ø§ØµÙ„Ø§Ø­ 10: unique_dates Ø§Ø² Ø´Ø§Ø®Øµ ÙØ¹Ù„ÛŒ X Ùˆ y (Ú©Ù‡ ÙÙ‚Ø· 'gregorian_date' Ø§Ø³Øª) Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
        unique_dates = df_combined.index.unique().sort_values() 

        initial_train_window_days = getattr(Config, "ML_INITIAL_TRAIN_DAYS", 252)
        test_window_days = getattr(Config, "ML_TEST_DAYS", 21)
        step_window_days = getattr(Config, "ML_STEP_DAYS", 21)

        fold_reports, fold_accuracies = [], []
        latest_model_path = os.path.join(MODELS_DIR, 'latest_model.joblib')
        latest_scaler_path = os.path.join(MODELS_DIR, 'latest_scaler.joblib')
        latest_feature_names_path = os.path.join(MODELS_DIR, 'latest_feature_names.joblib')
        latest_class_labels_path = os.path.join(MODELS_DIR, 'latest_class_labels.joblib')

        logger.info("Ø¯Ø± Ø­Ø§Ù„ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ ML Ø¨Ø§ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Walk-Forward...")

        if len(unique_dates) < initial_train_window_days + test_window_days:
            logger.info("Ø¯Ø§Ø¯Ù‡ Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Walk-Forward ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯. Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø± Ø±ÙˆÛŒ Ú©Ù„ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡...")
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            model = RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced', n_jobs=-1)
            model.fit(X_scaled, y)
            final_model, final_scaler = model, scaler
        else:
            start_idx_for_test_window = initial_train_window_days
            while start_idx_for_test_window + test_window_days <= len(unique_dates):
                train_end_date = unique_dates[start_idx_for_test_window - 1]
                test_start_date = unique_dates[start_idx_for_test_window]
                test_end_date = unique_dates[min(start_idx_for_test_window + test_window_days - 1, len(unique_dates) - 1)]

                # Ø§ØµÙ„Ø§Ø­ Ø¨Ø±Ø§ÛŒ Ø±ÙØ¹ Ø®Ø·Ø§ÛŒ Ù†Ø§Ù‡Ù…Ú¯Ø§Ù…ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² loc Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ§Ø±ÛŒØ®
                train_data = df_combined.loc[df_combined.index <= train_end_date].copy()
                test_data = df_combined.loc[(df_combined.index >= test_start_date) & (df_combined.index <= test_end_date)].copy()
                
                # Ø§ØµÙ„Ø§Ø­: Ù…Ø·Ù…Ø¦Ù† Ù…ÛŒâ€ŒØ´ÙˆÛŒÙ… Ú©Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ùˆ ØªØ³Øª Ø®Ø§Ù„ÛŒ Ù†Ø¨Ø§Ø´Ù†Ø¯.
                if train_data.empty or test_data.empty:
                    logger.warning(f"Ø¯Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ ÛŒØ§ ØªØ³Øª Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²Ù‡ {train_end_date} ØªØ§ {test_end_date} Ø®Ø§Ù„ÛŒ Ø§Ø³Øª. Ù¾Ø±Ø´ Ø¨Ù‡ Ù…Ø±Ø­Ù„Ù‡ Ø¨Ø¹Ø¯.")
                    start_idx_for_test_window += step_window_days
                    continue

                X_train_fold = train_data.drop(columns=["target"])
                y_train_fold = train_data["target"]
                X_test_fold = test_data.drop(columns=["target"])
                y_test_fold = test_data["target"]

                # Ø§ØµÙ„Ø§Ø­ Ø¨Ø±Ø§ÛŒ Ø±ÙØ¹ Ø®Ø·Ø§ÛŒ Ù†Ø§Ù‡Ù…Ú¯Ø§Ù…ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§:
                # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ X Ùˆ y Ø¯Ø± Ù‡Ø± fold Ù‡Ù…Ú¯Ø§Ù… Ù‡Ø³ØªÙ†Ø¯
                combined_train_fold = pd.concat([X_train_fold, y_train_fold], axis=1).dropna()
                X_train_fold_clean = combined_train_fold.drop(columns=["target"])
                y_train_fold_clean = combined_train_fold["target"]

                combined_test_fold = pd.concat([X_test_fold, y_test_fold], axis=1).dropna()
                X_test_fold_clean = combined_test_fold.drop(columns=["target"])
                y_test_fold_clean = combined_test_fold["target"]

                # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙ…ÛŒØ² Ø´Ø¯Ù‡ Ø®Ø§Ù„ÛŒ Ù†ÛŒØ³ØªÙ†Ø¯
                if X_train_fold_clean.empty or X_test_fold_clean.empty:
                    logger.warning(f"Ø¯Ø§Ø¯Ù‡ ØªÙ…ÛŒØ² Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²Ù‡ {train_end_date} ØªØ§ {test_end_date} Ø®Ø§Ù„ÛŒ Ø§Ø³Øª. Ù¾Ø±Ø´ Ø¨Ù‡ Ù…Ø±Ø­Ù„Ù‡ Ø¨Ø¹Ø¯.")
                    start_idx_for_test_window += step_window_days
                    continue

                scaler = StandardScaler()
                X_train_scaled = scaler.fit_transform(X_train_fold_clean)
                X_test_scaled = scaler.transform(X_test_fold_clean)

                model = RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced', n_jobs=-1)
                
                # Ø§ØµÙ„Ø§Ø­: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙ…ÛŒØ² Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ fit
                model.fit(X_train_scaled, y_train_fold_clean)
                
                # Ø§ØµÙ„Ø§Ø­: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙ…ÛŒØ² Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ predict
                y_pred_fold = model.predict(X_test_scaled)
                
                fold_reports.append(classification_report(y_test_fold_clean, y_pred_fold, output_dict=True, zero_division=0))
                fold_accuracies.append(accuracy_score(y_test_fold_clean, y_pred_fold))
                
                logger.info(f"âœ… Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²Ù‡ ØªØ³Øª {test_start_date.strftime('%Y-%m-%d')} ØªØ§ {test_end_date.strftime('%Y-%m-%d')} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")
                logger.info(f"Ø¯Ù‚Øª: {fold_accuracies[-1]:.2%}")
                
                start_idx_for_test_window += step_window_days
            
            # Ù…Ø¯Ù„ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø² Ø¢Ø®Ø±ÛŒÙ† Fold
            final_model, final_scaler = model, scaler

        # Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ
        joblib.dump(final_model, latest_model_path)
        # ðŸ’¡ Ø§ØµÙ„Ø§Ø­ 11: Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ X Ù¾Ø³ Ø§Ø² droplevel Ø¯ÛŒÚ¯Ø± Ø´Ø§Ù…Ù„ 'symbol_id' Ù†ÛŒØ³Øª.
        joblib.dump(X.columns.tolist(), latest_feature_names_path) 
        joblib.dump(final_model.classes_.tolist(), latest_class_labels_path)
        joblib.dump(final_scaler, latest_scaler_path)
        logger.info("âœ… Ù…Ø¯Ù„ Ùˆ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¬Ø§Ù†Ø¨ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯.")

        # Ù„Ø§Ú¯ ÙÛŒÚ†Ø± Ø§ÛŒÙ…Ù¾ÙˆØ±ØªÙ†Ø³
        try:
            importances = pd.Series(final_model.feature_importances_, index=X.columns)
            logger.info("ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù… Ù…Ø¯Ù„:")
            logger.info(importances.sort_values(ascending=False).head(20))
        except Exception:
            pass

    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ Ø¯Ø± ÙØ±Ø¢ÛŒÙ†Ø¯ Ø¢Ù…ÙˆØ²Ø´: {e}", exc_info=True)

if __name__ == "__main__":
    train_model()
