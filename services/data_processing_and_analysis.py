# -*- coding: utf-8 -*-
# services/data_processing_and_analysis.py
# Ù…Ø³Ø¦ÙˆÙ„ÛŒØª: Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ùˆ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³.

import logging
import gc
import psutil
import time
import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta
from typing import Dict, List, Optional, Tuple, Any, Union
from contextlib import contextmanager
import threading

from sqlalchemy import func, distinct, text
from sqlalchemy.orm import sessionmaker, Session
from sqlalchemy.exc import SQLAlchemyError
from sqlalchemy import or_

from flask import current_app

# --- ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡ ---
from extensions import db
from models import (
    HistoricalData,
    TechnicalIndicatorData,
    CandlestickPatternDetection,
    ComprehensiveSymbolData
)

from services.technical_analysis_utils import (
        calculate_all_indicators,
        check_candlestick_patterns
    )

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯ÛŒÙ†Ú¯ ---
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# --- Ø«Ø§Ø¨Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø­Ø§ÙØ¸Ù‡ Ùˆ Ø¨Ú† ---
DEFAULT_BATCH_SIZE = 200 # for DB bulk ops & symbol processing
MEMORY_LIMIT_MB = 1500 # warn threshold

# -----------------------------------------------------------
# Ø¨Ø®Ø´ Û±: Ù…Ø¯ÛŒØ±ÛŒØª ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡ Session (Ù†Ø³Ø®Ù‡ Lazy-Loaded)
# -----------------------------------------------------------

SessionLocal: Optional[sessionmaker] = None
_session_lock = threading.Lock()

def _get_session_local() -> sessionmaker:
    """
    Ø§ÛŒØ¬Ø§Ø¯ Ùˆ Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† SessionMaker Ø¨Ù‡ ØµÙˆØ±Øª Lazy (ØªÙ†Ø¨Ù„) Ùˆ Thread-Safe.
    """
    global SessionLocal
    
    if SessionLocal:
        return SessionLocal
    
    with _session_lock:
        if SessionLocal is None:
            try:
                # ğŸ’¥ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ db.engine Ø¯Ø± Ø²Ù…Ø§Ù† Ù†ÛŒØ§Ø²
                SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=db.engine)
                logger.info("âœ… SessionLocal (sessionmaker) Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø´Ø¯.")
            except Exception as e:
                logger.error(f"âŒ Ø§Ù…Ú©Ø§Ù† Ø§ØªØµØ§Ù„ Ø¨Ù‡ db.engine Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®Øª SessionLocal ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯: {e}", exc_info=True)
                raise RuntimeError(f"Ø§Ù…Ú©Ø§Ù† Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ SessionLocal ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯: {e}")
            
        return SessionLocal


@contextmanager
def session_scope(external_session: Optional[Session] = None) -> Session:
    """
    Ù…Ø¯ÛŒØ±ÛŒØª Ù‡ÙˆØ´Ù…Ù†Ø¯ Session Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Flask-context ÛŒØ§ Ø®Ø§Ø±Ø¬ Ø§Ø² Ø¢Ù†.
    """
    session = None
    try:
        if external_session:
            # ğŸ’¡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Session Ù…ÙˆØ¬ÙˆØ¯
            logger.debug("Using external session (from Flask context).")
            yield external_session
        else:
            # ğŸ’¡ Ø§ÛŒØ¬Ø§Ø¯ Session Ø¬Ø¯ÛŒØ¯
            factory = _get_session_local()
            if not factory:
                raise RuntimeError("SessionLocal factory could not be initialized.")
                
            session = factory()
            logger.debug("Creating new local session for background task.")
            
            yield session
            
            # ğŸš¨ ØªÙ†Ù‡Ø§ Ø²Ù…Ø§Ù†ÛŒ Commit Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú©Ù‡ Ø®ÙˆØ¯Ù…Ø§Ù† Session Ø±Ø§ Ø³Ø§Ø®ØªÙ‡ Ø¨Ø§Ø´ÛŒÙ… (Ù†Ù‡ Session ÙˆØ±ÙˆØ¯ÛŒ Ø§Ø² Flask)
            logger.debug("Committing final local session.")
            session.commit() # ğŸ‘ˆ Ù…Ø¯ÛŒØ±ÛŒØª Ø®ÙˆØ¯Ú©Ø§Ø± Commit Ù†Ù‡Ø§ÛŒÛŒ
            
    except Exception as e:
        logger.error(f"Error occurred in session scope: {e}. Rolling back.", exc_info=True)
        if session: 
            session.rollback() # ğŸ‘ˆ Ù…Ø¯ÛŒØ±ÛŒØª Ø®ÙˆØ¯Ú©Ø§Ø± Rollback
        raise e 
    finally:
        if session: 
            logger.debug("Closing local session.")
            session.close() # ğŸ‘ˆ Ù…Ø¯ÛŒØ±ÛŒØª Ø®ÙˆØ¯Ú©Ø§Ø± Close


# -----------------------------------------------------------
# ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø­Ø§ÙØ¸Ù‡
# -----------------------------------------------------------

def check_memory_usage_mb() -> float:
    """Return current process memory usage in MB (if psutil available)."""
    try:
        if psutil:
            proc = psutil.Process()
            mem = proc.memory_info().rss / (1024 * 1024)
            return mem
        else:
            return 0.0
    except Exception as e:
        logger.debug("Memory check failed: %s", e)
        return 0.0

def cleanup_memory():
    """Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡"""
    try:
        gc.collect()
        current_memory = check_memory_usage_mb()
        if current_memory > MEMORY_LIMIT_MB:
            logger.warning(f"âš ï¸ Ù…ØµØ±Ù Ø­Ø§ÙØ¸Ù‡ Ø¨Ø§Ù„Ø§: {current_memory:.2f} MB")
    except Exception as e:
        logger.debug(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡: {e}")


# -----------------------------------------------------------
# ØªÙˆØ§Ø¨Ø¹ Ø¬Ø¯ÛŒØ¯ Ù…Ø¯ÛŒØ±ÛŒØª Ø¯ÛŒØªØ§Ø¨ÛŒØ³ (Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ)
# -----------------------------------------------------------

def clear_and_vacuum_table(session: Session, model_class: Any):
    """
    Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ú©Ø§Ù…Ù„ Ù…Ø­ØªÙˆØ§ÛŒ ÛŒÚ© Ø¬Ø¯ÙˆÙ„ (Delete) Ùˆ Ø³Ù¾Ø³ Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ (VACUUM/OPTIMIZE)
    ğŸš¨ Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ù¾Ø³ Ø§Ø² Ø­Ø°ÙØŒ ÛŒÚ© Commit ØµØ±ÛŒØ­ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ ØªØ§ ÙØ¶Ø§ÛŒ Ø¯ÛŒØ³Ú© Ø±Ø§ Ø¢Ø²Ø§Ø¯ Ú©Ù†Ø¯.
    """
    table_name = model_class.__tablename__
    logger.info(f"ğŸ—‘ï¸ Ø´Ø±ÙˆØ¹ Ø¹Ù…Ù„ÛŒØ§Øª Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ø¬Ø¯ÙˆÙ„: **{table_name}**")

    try:
        # 1. Ø­Ø°Ù Ú©Ø§Ù…Ù„ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§
        delete_count = session.query(model_class).delete()
        logger.info(f"âœ… {delete_count} Ø±Ú©ÙˆØ±Ø¯ Ø§Ø² Ø¬Ø¯ÙˆÙ„ {table_name} Ø­Ø°Ù Ø´Ø¯.")
        
        # 2. Ø«Ø¨Øª Ø­Ø°Ù (Commit ØµØ±ÛŒØ­) - Ø¶Ø±ÙˆØ±ÛŒ Ø¨Ø±Ø§ÛŒ Ø¢Ø²Ø§Ø¯ Ø´Ø¯Ù† ÙØ¶Ø§ÛŒ Ø¯ÛŒØ³Ú© Ù‚Ø¨Ù„ Ø§Ø² Ø¯Ø±Ø¬ Ù…Ø¬Ø¯Ø¯
        session.commit()
        logger.debug(f"ğŸ’¾ Commit Ø­Ø°Ù Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ø¬Ø¯ÙˆÙ„ {table_name} Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")
        
        # 3. Ø§Ø¬Ø±Ø§ÛŒ VACUUM/OPTIMIZE Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²Ù¾Ø³â€ŒÚ¯ÛŒØ±ÛŒ ÙØ¶Ø§ÛŒ Ø¢Ø²Ø§Ø¯ Ø´Ø¯Ù‡
        dialect = session.bind.dialect.name
        
        if dialect == 'postgresql':
            # VACUUM FULL Ø¯Ø± PostgreSQL Ù†ÛŒØ§Ø² Ø¨Ù‡ Commit Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ø¯Ø§Ø±Ø¯ Ùˆ Ø²Ù…Ø§Ù†â€ŒØ¨Ø± Ø§Ø³Øª.
            # Ø¨Ø±Ø§ÛŒ Ø­ÙØ¸ ØªØ±Ø§Ú©Ù†Ø´ØŒ Ø§Ø² VACUUM Ù…Ø¹Ù…ÙˆÙ„ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ùˆ ÙÙ‚Ø· Delete Ø±Ø§ Commit Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….
            try:
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©Ø§Ù†Ú©Ø´Ù† Ù…Ø¬Ø²Ø§ Ø¨Ø±Ø§ÛŒ VACUUM FULL
                engine = session.bind
                connection = engine.raw_connection()
                try:
                    cursor = connection.cursor()
                    cursor.execute(f"VACUUM FULL ANALYZE {table_name};")
                    connection.commit()
                    logger.info(f"âœ… PostgreSQL **VACUUM FULL ANALYZE** Ø¨Ø± Ø±ÙˆÛŒ {table_name} Ø§Ø¬Ø±Ø§ Ø´Ø¯ (Ø¯Ø± Ú©Ø§Ù†Ú©Ø´Ù† Ù…Ø¬Ø²Ø§).")
                finally:
                    connection.close()
            except Exception as e:
                 logger.error(f"âŒ Ø®Ø·Ø§ÛŒ VACUUM FULL Ø¯Ø± PostgreSQL: {e}")
                 # Ø¯Ø± ØµÙˆØ±Øª Ø´Ú©Ø³Øª VACUUMØŒ Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ….
        
        elif dialect in ('mysql', 'sqlite'):
            with session.bind.begin() as connection:
                if dialect == 'mysql':
                    optimize_command = text(f"OPTIMIZE TABLE {table_name};")
                    connection.execute(optimize_command)
                    logger.info(f"âœ… MySQL **OPTIMIZE TABLE** Ø¨Ø± Ø±ÙˆÛŒ {table_name} Ø§Ø¬Ø±Ø§ Ø´Ø¯.")
                elif dialect == 'sqlite':
                    vacuum_command = text("VACUUM;")
                    connection.execute(vacuum_command)
                    logger.info(f"âœ… SQLite **VACUUM** Ø§Ø¬Ø±Ø§ Ø´Ø¯.")
        else:
            logger.warning(f"âš ï¸ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ {dialect} Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯.")

        logger.info(f"ğŸ‰ Ø¹Ù…Ù„ÛŒØ§Øª Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¬Ø¯ÙˆÙ„ {table_name} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯.")

    except SQLAlchemyError as e:
        logger.error(f"âŒ Ø®Ø·Ø§ÛŒ SQLAlchemy Ø¯Ø± Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ ÛŒØ§ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¬Ø¯ÙˆÙ„ {table_name}: {e}", exc_info=True)
        session.rollback()
        raise

# -----------------------------------------------------------
# ØªØ§Ø¨Ø¹ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†ØªØ§ÛŒØ¬ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„
# -----------------------------------------------------------

def save_technical_indicators(db_session: Session, symbol_id: Union[int, str], df: pd.DataFrame):
    """
    Ø°Ø®ÛŒØ±Ù‡ (Ø¯Ø±Ø¬ ÛŒØ§ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ) Ù†ØªØ§ÛŒØ¬ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„.
    Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Commit ÛŒØ§ Rollback Ù†Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ ÙØ±Ø¶ Ø¨Ø± Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ ØªØ§Ø¨Ø¹ ÙØ±Ø§Ø®ÙˆØ§Ù†Ù†Ø¯Ù‡ Ø§ÛŒÙ† Ú©Ø§Ø± Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.
    """
    symbol_id_str = str(symbol_id)
    
    logger.debug(f"ğŸ’¾ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§Ø¯: {symbol_id_str}")

    if 'symbol_id' not in df.columns:
        df['symbol_id'] = symbol_id_str
    else:
        df['symbol_id'] = df['symbol_id'].astype(str)

    # ğŸ’¡ Ø§ØµÙ„Ø§Ø­: Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù…Ø´Ú©Ù„Ø§Øª ØªÚ©Ø±Ø§Ø±ÛŒØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² drop_duplicates
    df_unique = df.drop_duplicates(subset=['symbol_id', 'jdate'], keep='last').copy()
    
    # ğŸ’¡ Ø§ØµÙ„Ø§Ø­: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² MACD_Hist Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø³ØªÙˆÙ† Ø­ÛŒØ§ØªÛŒ Ø¯Ø± Ú©Ù†Ø§Ø± RSI
    df_to_save = df_unique.dropna(subset=['RSI', 'MACD_Histogram', 'jdate']) 

    if df_to_save.empty:
        logger.debug(f"âš ï¸ Ù‡ÛŒÚ† Ø³Ø·Ø± Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± {symbol_id_str} ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´Øª.")
        return
        
    updates_count = 0
    inserts_count = 0
    
    records_dict = df_to_save.to_dict('records')
    
    # ğŸ’¡ Ø§ØµÙ„Ø§Ø­: ÙÚ† Ú©Ø±Ø¯Ù† ÙÙ‚Ø· Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§ ØªØ§Ø±ÛŒØ®â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± DataFrame
    jdates_in_df = df_to_save['jdate'].unique().tolist()
    
    existing_indicators_query = db_session.query(TechnicalIndicatorData).filter(
        TechnicalIndicatorData.symbol_id == symbol_id_str,
        TechnicalIndicatorData.jdate.in_(jdates_in_df) # ğŸ‘ˆ Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† ÙÚ†
    )
    
    existing_map = {
        indicator.jdate: indicator 
        for indicator in existing_indicators_query
    }

    for row in records_dict:
        jdate = row.get('jdate')
        if not jdate:
            continue

        # ğŸ’¡ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ØªØ·Ø§Ø¨Ù‚ Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ Ø¨Ø§ Ù…Ø¯Ù„ Ø¯ÛŒØªØ§Ø¨ÛŒØ³
        data_to_save = {
            'close_price': row.get('close'),
            'RSI': row.get('RSI'),
            'MACD': row.get('MACD'),
            'MACD_Signal': row.get('MACD_Signal'),
            'MACD_Hist': row.get('MACD_Histogram'),
            'SMA_20': row.get('SMA_20'),
            'SMA_50': row.get('SMA_50'),
            'Bollinger_High': row.get('Bollinger_Upper'),
            'Bollinger_Low': row.get('Bollinger_Lower'),
            'Bollinger_MA': row.get('Bollinger_Middle'),
            'Volume_MA_20': row.get('Volume_MA_20'),
            'ATR': row.get('ATR'),
            'Stochastic_K': row.get('Stochastic_K'),
            'Stochastic_D': row.get('Stochastic_D'),
            'squeeze_on': bool(row.get('squeeze_on')),
            'halftrend_signal': row.get('halftrend_signal'),
            'resistance_level_50d': row.get('resistance_level_50d'),
            'resistance_broken': bool(row.get('resistance_broken')),
            'updated_at': datetime.now()
        }

        existing = existing_map.get(jdate)

        if existing:
            # âœ… Update
            for key, value in data_to_save.items():
                setattr(existing, key, value)
            updates_count += 1
        else:
            # âœ… Insert
            data_to_save.update({
                'symbol_id': row.get('symbol_id', symbol_id_str),
                'jdate': jdate,
                'created_at': datetime.now()
            })
            indicator = TechnicalIndicatorData(**data_to_save)
            db_session.add(indicator)
            inserts_count += 1

    if inserts_count > 0 or updates_count > 0:
        logger.info(f"âœ… Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ù†Ù…Ø§Ø¯ {symbol_id_str} Ø¨Ù‡ Session Ø§Ø¶Ø§ÙÙ‡/Ø¢Ù¾Ø¯ÛŒØª Ø´Ø¯Ù†Ø¯. (Ø¯Ø±Ø¬: {inserts_count}ØŒ Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ: {updates_count})")
    else:
        logger.debug(f"â„¹ï¸ Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡ Ø¬Ø¯ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§Ø¯ {symbol_id_str} ÛŒØ§ÙØª Ù†Ø´Ø¯.")


# -----------------------------------------------------------
# ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ (Ø¨Ø§ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù† Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ùˆ Vacuum)
# -----------------------------------------------------------

def run_technical_analysis(
    db_session: Optional[Session] = None,
    limit: int = None, 
    symbols_list: list = None, 
    batch_size: int = DEFAULT_BATCH_SIZE
) -> Tuple[int, str]:
    """
    Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ø¯Ø± Ø¨Ú†â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú©.
    ğŸ”„ Ø§Ø¨ØªØ¯Ø§ Ø¬Ø¯ÙˆÙ„ TechnicalIndicatorData Ø±Ø§ Ú©Ø§Ù…Ù„ Ù¾Ø§Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ VACUUM Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """
    # ğŸ’¡ Ø§Ú¯Ø± session ÙˆØ±ÙˆØ¯ÛŒ Ø¯Ø§Ø¯Ù‡ Ù†Ø´Ø¯Ù‡ØŒ Ø§Ø² session_scope Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ØŒ Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ† ØµÙˆØ±ØªØŒ Ø§Ø² session ÙˆØ±ÙˆØ¯ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†.
    # Ø§ÛŒÙ† Ø³Ø§Ø®ØªØ§Ø± ØªØ¶Ù…ÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ session_scope Commit/Rollback/Close Ø±Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ú©Ù†Ø¯.
    with session_scope(external_session=db_session) as session:
        try:
            logger.info("ğŸ“ˆ Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„...")

            # ğŸ’¥ Ø¨Ø®Ø´ Ø¬Ø¯ÛŒØ¯: Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ (TechnicalIndicatorData)
            clear_and_vacuum_table(session, TechnicalIndicatorData)

            # âš™ï¸ Ø¨Ø®Ø´ ÛŒØ§ÙØªÙ† Ù†Ù…Ø§Ø¯Ù‡Ø§ (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±)
            independent_session = None
            try:
                factory = _get_session_local()
                independent_session = factory()
                symbol_query = independent_session.query(ComprehensiveSymbolData.symbol_id)
                if symbols_list:
                    symbols_list_str = [str(sym) for sym in symbols_list]
                    symbol_query = symbol_query.filter(ComprehensiveSymbolData.symbol_id.in_(symbols_list_str))
                all_symbols = [row[0] for row in symbol_query.all()]
                if not all_symbols:
                    historical_query = independent_session.query(distinct(HistoricalData.symbol_id))
                    all_symbols = [row[0] for row in historical_query.all()]
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± session Ù…Ø³ØªÙ‚Ù„: {e}")
                symbol_query = session.query(ComprehensiveSymbolData.symbol_id)
                all_symbols = [row[0] for row in symbol_query.all()] if symbol_query else []
            finally:
                if independent_session:
                    independent_session.close()

            total_symbols = len(all_symbols)
            if limit is not None:
                all_symbols = all_symbols[:limit]
                total_symbols = len(all_symbols)

            processed_count = 0
            success_count = 0
            error_count = 0

            for i in range(0, total_symbols, batch_size):
                batch_symbols = all_symbols[i:i + batch_size]
                logger.info(f"ğŸ“¦ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ú† {i // batch_size + 1}: Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ {i + 1} ØªØ§ {min(i + batch_size, total_symbols)}")

                # âš™ï¸ Ú©ÙˆØ¦Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ø¨Ú† (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±)
                query = session.query(
                    HistoricalData.symbol_id, HistoricalData.symbol_name, HistoricalData.date, HistoricalData.jdate, 
                    HistoricalData.open, HistoricalData.close, HistoricalData.high, HistoricalData.low, 
                    HistoricalData.volume, HistoricalData.final, HistoricalData.yesterday_price, HistoricalData.plc, 
                    HistoricalData.plp, HistoricalData.pcc, HistoricalData.pcp, HistoricalData.mv, 
                    HistoricalData.buy_count_i, HistoricalData.buy_count_n, HistoricalData.sell_count_i, 
                    HistoricalData.sell_count_n, HistoricalData.buy_i_volume, HistoricalData.buy_n_volume, 
                    HistoricalData.sell_i_volume, HistoricalData.sell_n_volume
                ).filter(HistoricalData.symbol_id.in_(batch_symbols)).order_by(HistoricalData.symbol_id, HistoricalData.date)
                historical_data = query.all()

                if not historical_data:
                    logger.warning(f"âš ï¸ Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ú† {i // batch_size + 1} ÛŒØ§ÙØª Ù†Ø´Ø¯.")
                    continue
                
                columns = [
                    'symbol_id', 'symbol_name', 'date', 'jdate', 'open', 'close', 'high', 'low', 'volume',
                    'final', 'yesterday_price', 'plc', 'plp', 'pcc', 'pcp', 'mv',
                    'buy_count_i', 'buy_count_n', 'sell_count_i', 'sell_count_n',
                    'buy_i_volume', 'buy_n_volume', 'sell_i_volume', 'sell_n_volume'
                ]
                df = pd.DataFrame(historical_data, columns=columns)
                
                grouped = df.groupby('symbol_id')

                for symbol_id, group_df in grouped:
                    processed_count += 1
                    try:
                        if len(group_df) < 5: continue
                        
                        df_indicators = calculate_all_indicators(group_df.copy())
                        save_technical_indicators(session, symbol_id, df_indicators)
                        success_count += 1

                        if processed_count % 10 == 0:
                            logger.info(f"ğŸ“Š Ù¾ÛŒØ´Ø±ÙØª ØªØ­Ù„ÛŒÙ„: {processed_count}/{total_symbols} Ù†Ù…Ø§Ø¯")

                    except Exception as e:
                        error_count += 1
                        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ù†Ù…Ø§Ø¯ {symbol_id}: {e}", exc_info=True)

                # ğŸ’¥ Commit Ø¨Ú†Ù‡â€ŒØ§ÛŒ
                try:
                    session.commit() # ğŸ‘ˆ Commit Ù¾Ø³ Ø§Ø² Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆÙÙ‚ÛŒØª Ø¢Ù…ÛŒØ² ØªÙ…Ø§Ù… Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ Ø¨Ú†
                    logger.info(f"ğŸ’¾ Ø¨Ú† {i // batch_size + 1} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Commit Ø´Ø¯.")
                except Exception as e:
                    session.rollback() # Ø§Ú¯Ø± Commit Ø¨Ú†Ù‡â€ŒØ§ÛŒ Ø´Ú©Ø³Øª Ø®ÙˆØ±Ø¯ØŒ Rollback Ú©Ù†
                    logger.error(f"âŒ Ø®Ø·Ø§ÛŒ Commit Ø¯Ø± Ø¨Ú† {i // batch_size + 1}: {e}. Rollback Ø´Ø¯.", exc_info=True)

                del df
                del historical_data
                cleanup_memory()

            logger.info(f"âœ… ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ú©Ø§Ù…Ù„ Ø´Ø¯. Ù…ÙˆÙÙ‚: {success_count} | Ø®Ø·Ø§: {error_count}")
            return success_count, f"ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ Ø´Ø¯. {success_count} Ù…ÙˆÙÙ‚ØŒ {error_count} Ø®Ø·Ø§"

        except Exception as e:
            error_msg = f"âŒ Ø®Ø·Ø§ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„: {e}"
            logger.error(error_msg, exc_info=True)
            # Rollback Ø¯Ø± Ø§ÛŒÙ† Ø³Ø·Ø­ ØªÙˆØ³Ø· session_scope Ù…Ø¯ÛŒØ±ÛŒØª Ù…ÛŒâ€ŒØ´ÙˆØ¯
            return 0, error_msg


# -----------------------------------------------------------
# ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ (Ø¨Ø§ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù† Vacuum)
# -----------------------------------------------------------

def run_candlestick_detection(
    db_session: Optional[Session] = None,
    limit: int = None, 
    symbols_list: list = None
) -> int:
    """
    Ø§Ø¬Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ.
    ğŸ”„ Ø§Ø¨ØªØ¯Ø§ Ø¬Ø¯ÙˆÙ„ CandlestickPatternDetection Ø±Ø§ Ú©Ø§Ù…Ù„ Ù¾Ø§Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ VACUUM Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """
    with session_scope(external_session=db_session) as session:
        try:
            logger.info("ğŸ•¯ï¸ Ø´Ø±ÙˆØ¹ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ...")
            
            # ğŸ’¥ Ø¨Ø®Ø´ Ø¬Ø¯ÛŒØ¯: Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ (CandlestickPatternDetection)
            clear_and_vacuum_table(session, CandlestickPatternDetection)
            
            # âš™ï¸ Ø¨Ø®Ø´ ÛŒØ§ÙØªÙ† Ù†Ù…Ø§Ø¯Ù‡Ø§ (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±)
            independent_session = None
            try:
                factory = _get_session_local()
                independent_session = factory()
                base_query = independent_session.query(ComprehensiveSymbolData.symbol_id)
                if symbols_list:
                    symbols_list_str = [str(sym) for sym in symbols_list]
                    base_query = base_query.filter(ComprehensiveSymbolData.symbol_id.in_(symbols_list_str))
                symbol_ids_to_process = [str(s[0]) for s in base_query.all()]
                if not symbol_ids_to_process:
                    historical_query = independent_session.query(distinct(HistoricalData.symbol_id))
                    symbol_ids_to_process = [str(s[0]) for s in historical_query.all()]
            except Exception as e:
                # ... Fallback logic ...
                logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± session Ù…Ø³ØªÙ‚Ù„: {e}")
                base_query = session.query(ComprehensiveSymbolData.symbol_id)
                symbol_ids_raw = [s[0] for s in base_query.all()] if base_query else []
                symbol_ids_to_process = [str(symbol_id) for symbol_id in symbol_ids_raw]
            finally:
                if independent_session:
                    independent_session.close()

            if not symbol_ids_to_process:
                logger.warning("âš ï¸ Ù‡ÛŒÚ† Ù†Ù…Ø§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
                return 0
                
            if limit is not None:
                symbol_ids_to_process = symbol_ids_to_process[:limit]
                
            logger.info(f"ğŸ” ÛŒØ§ÙØª Ø´Ø¯ {len(symbol_ids_to_process)} Ù†Ù…Ø§Ø¯ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ")

            success_count = 0
            records_to_insert = []
            processed_count = 0

            # âš™ï¸ Ø­Ù„Ù‚Ù‡ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†Ù…Ø§Ø¯Ù‡Ø§ (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø¯Ø± Ù…Ù†Ø·Ù‚ Ø§ØµÙ„ÛŒ)
            for symbol_id in symbol_ids_to_process:
                try:
                    historical_data_query = session.query(HistoricalData).filter(
                        HistoricalData.symbol_id == symbol_id
                    ).order_by(HistoricalData.date.desc()).limit(30)
                    
                    historical_data = historical_data_query.all() 
                    
                    if len(historical_data) < 5: 
                        logger.debug(f"âš ï¸ Ø¯Ø§Ø¯Ù‡ Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§Ø¯ {symbol_id} ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
                        continue 

                    df = pd.DataFrame([row.__dict__ for row in historical_data])
                    if '_sa_instance_state' in df.columns:
                        df = df.drop(columns=['_sa_instance_state']) 
                    df.sort_values(by='date', inplace=True) 

                    today_record_dict = df.iloc[-1].to_dict()
                    yesterday_record_dict = df.iloc[-2].to_dict()
                    
                    patterns = check_candlestick_patterns(today_record_dict, yesterday_record_dict, df)
                    
                    if patterns:
                        now = datetime.now()
                        current_jdate = today_record_dict['jdate']
                        for pattern in patterns:
                            records_to_insert.append({
                                'symbol_id': symbol_id,
                                'jdate': current_jdate,
                                'pattern_name': pattern,
                                'created_at': now, 
                                'updated_at': now
                            })
                        success_count += 1
                        
                    processed_count += 1
                    if processed_count % 100 == 0:
                        logger.info(f"ğŸ•¯ï¸ Ù¾ÛŒØ´Ø±ÙØª ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ: {processed_count}/{len(symbol_ids_to_process)} Ù†Ù…Ø§Ø¯")

                except Exception as e:
                    logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§Ø¯ {symbol_id}: {e}", exc_info=True)
            
            logger.info(f"âœ… ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ {success_count} Ù†Ù…Ø§Ø¯ (Ø¨Ø§ {len(records_to_insert)} Ø§Ù„Ú¯Ùˆ) Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")
                        
            # 3. Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
            if records_to_insert:
                logger.info(f"ğŸ’¾ Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±Ø¬ {len(records_to_insert)} Ø±Ú©ÙˆØ±Ø¯ Ø¬Ø¯ÛŒØ¯...")
                
                # ğŸ” Ø¨Ø±Ø±Ø³ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§ Ù‚Ø¨Ù„ Ø§Ø² Ø¯Ø±Ø¬ (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±)
                unique_records = {}
                duplicates_count = 0
                for record in records_to_insert:
                    key = (record['symbol_id'], record['jdate'], record['pattern_name'])
                    if key in unique_records:
                        duplicates_count += 1
                    else:
                        unique_records[key] = record
                
                if duplicates_count > 0:
                    records_to_insert = list(unique_records.values())
                    logger.warning(f"âš ï¸ {duplicates_count} Ø±Ú©ÙˆØ±Ø¯ ØªÚ©Ø±Ø§Ø±ÛŒ Ø­Ø°Ù Ø´Ø¯. {len(records_to_insert)} Ø±Ú©ÙˆØ±Ø¯ Ø¨Ø±Ø§ÛŒ Ø¯Ø±Ø¬ Ø¨Ø§Ù‚ÛŒ Ù…Ø§Ù†Ø¯.")
                
                # Ø¯Ø±Ø¬ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
                session.bulk_insert_mappings(CandlestickPatternDetection, records_to_insert)
                # ğŸ’¡ Commit Ù†Ù‡Ø§ÛŒÛŒ ØªÙˆØ³Ø· session_scope Ø¯Ø± Ø§Ù†ØªÙ‡Ø§ÛŒ ØªØ§Ø¨Ø¹ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯
                logger.info(f"âœ… {len(records_to_insert)} Ø§Ù„Ú¯ÙˆÛŒ Ø´Ù…Ø¹ÛŒ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø±Ø¬ Ø´Ø¯.")
                
            else:
                logger.info("â„¹ï¸ Ù‡ÛŒÚ† Ø§Ù„Ú¯ÙˆÛŒ Ø´Ù…Ø¹ÛŒ Ø¬Ø¯ÛŒØ¯ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")

            return success_count

        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ: {e}", exc_info=True)
            # Rollback Ø¯Ø± Ø§ÛŒÙ† Ø³Ø·Ø­ ØªÙˆØ³Ø· session_scope Ù…Ø¯ÛŒØ±ÛŒØª Ù…ÛŒâ€ŒØ´ÙˆØ¯
            return 0
