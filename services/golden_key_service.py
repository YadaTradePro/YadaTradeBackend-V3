# services/golden_key_service.py

from extensions import db
from models import (
    ComprehensiveSymbolData,
    FundamentalData,
    HistoricalData,
    GoldenKeyResult,
)
import pandas as pd
import logging
from datetime import datetime
import jdatetime
import numpy as np
import json
from sqlalchemy import func

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯ÛŒÙ†Ú¯ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù…Ø§Ú˜ÙˆÙ„
logger = logging.getLogger(__name__)
logger.addHandler(logging.NullHandler())

# ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† ØªÙˆØ§Ø¨Ø¹ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ Ø¶Ø±ÙˆØ±ÛŒ
from services.technical_analysis_utils import get_today_jdate_str
# ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† ØªÙˆØ§Ø¨Ø¹ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ Ø§Ø®ØªÛŒØ§Ø±ÛŒ
try:
    from services.technical_analysis_utils import calculate_smart_money_flow, check_candlestick_patterns
except Exception:
    calculate_smart_money_flow = None
    check_candlestick_patterns = None
    logger.debug("Optional utility functions (calculate_smart_money_flow, check_candlestick_patterns) not found.")

# ---------------------------------------------------------
# Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø­Ù„ÛŒ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Ù¾Ø§ÛŒØ¯Ø§Ø±ÛŒ Ø¨ÛŒØ´ØªØ±
# ---------------------------------------------------------

def _to_series(x):
    if isinstance(x, pd.Series):
        return x.astype(float)
    return pd.Series(x).astype(float)

def compute_sma(series, window=20):
    s = _to_series(series)
    if s.empty or len(s) < window:
        return pd.Series([np.nan] * len(s), index=s.index)
    return s.rolling(window=window, min_periods=1).mean()

def compute_rsi(series, window=14):
    s = _to_series(series)
    if s.empty:
        return pd.Series([], dtype=float)
    delta = s.diff()
    up = delta.clip(lower=0)
    down = -delta.clip(upper=0)
    roll_up = up.ewm(span=window, adjust=False).mean()
    roll_down = down.ewm(span=window, adjust=False).mean()
    rs = roll_up / roll_down.replace(0, np.nan)
    rsi = 100 - (100 / (1 + rs))
    rsi.iloc[:window] = np.nan
    return rsi

def compute_macd(series, fast=12, slow=26, signal=9):
    s = _to_series(series)
    if s.empty:
        return pd.Series([]), pd.Series([]), pd.Series([])
    ema_fast = s.ewm(span=fast, adjust=False).mean()
    ema_slow = s.ewm(span=slow, adjust=False).mean()
    macd_line = ema_fast - ema_slow
    signal_line = macd_line.ewm(span=signal, adjust=False).mean()
    histogram = macd_line - signal_line
    return macd_line, signal_line, histogram

def compute_atr(high, low, close, window=14):
    h, l, c = _to_series(high), _to_series(low), _to_series(close)
    if h.empty or l.empty or c.empty:
        return pd.Series([], dtype=float)
    tr1 = h - l
    tr2 = (h - c.shift()).abs()
    tr3 = (l - c.shift()).abs()
    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
    atr = tr.ewm(span=window, adjust=False).mean()
    return atr

# ---------------------------------------------------------
# ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„ØªØ±Ù‡Ø§
# ---------------------------------------------------------

def is_resistance_breakout(df_high, current_close, days_window=20):
    if len(df_high) < days_window + 1: return False
    recent_highs = df_high.iloc[-days_window-1:-1].max()
    return pd.notna(current_close) and pd.notna(recent_highs) and current_close > recent_highs

def is_high_volume(current_volume, avg_volume, multiplier=1.5):
    return pd.notna(current_volume) and pd.notna(avg_volume) and current_volume > (avg_volume * multiplier)

def is_rsi_oversold(rsi_value, threshold=30):
    return pd.notna(rsi_value) and rsi_value < threshold

def is_macd_buy_signal(macd_line, signal_line):
    if len(macd_line) < 2 or len(signal_line) < 2: return False
    current_macd, current_signal = macd_line.iloc[-1], signal_line.iloc[-1]
    prev_macd, prev_signal = macd_line.iloc[-2], signal_line.iloc[-2]
    if pd.isna(current_macd) or pd.isna(current_signal) or pd.isna(prev_macd) or pd.isna(prev_signal): return False
    return (current_macd > current_signal) and (prev_macd <= prev_signal)

# ---------------------------------------------------------
# ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ ØªØ­Ù„ÛŒÙ„ Ùˆ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù„ÛŒØ¯ Ø·Ù„Ø§ÛŒÛŒ
# ---------------------------------------------------------

def run_golden_key_analysis_and_save(top_n_symbols=8):
    logger.info("Starting Golden Key analysis and saving process.")
    today_jdate_str = get_today_jdate_str()
    all_symbols = ComprehensiveSymbolData.query.all()

    if not all_symbols:
        logger.warning("No symbols found in ComprehensiveSymbolData. Cannot run Golden Key analysis.")
        return False, "No symbols found to analyze."

    # --- Ú¯Ø§Ù… Û±: Ø­Ø°Ù ØªÙ…Ø§Ù… Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ Ø§Ø² Ø¬Ø¯ÙˆÙ„ GoldenKeyResult ---
    try:
        deleted_count = db.session.query(GoldenKeyResult).delete()
        db.session.commit()
        logger.info(f"Successfully deleted {deleted_count} old records from GoldenKeyResult.")
    except Exception as e:
        db.session.rollback()
        logger.error(f"Error deleting all previous Golden Key results: {e}", exc_info=True)
        return False, f"DB cleanup failed: {str(e)}"
    # ---------------------------------------------------------------
    
    # Ù…Ù†Ø·Ù‚ Ø­Ø°Ù ØµÙ†Ø¯ÙˆÙ‚â€ŒÙ‡Ø§ Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ Ù†Ø¯Ø§Ø±Ø¯ Ú†ÙˆÙ† Ú©Ù„ Ø¬Ø¯ÙˆÙ„ Ù¾Ø§Ú© Ø´Ø¯Ù‡ Ø§Ø³Øª.
    fund_keywords = ["ØµÙ†Ø¯ÙˆÙ‚", "Ø³Ø±Ù…Ø§ÛŒÙ‡ Ú¯Ø°Ø§Ø±ÛŒ", "Ø§Ø¹ØªØ¨Ø§Ø±", "Ø¢ØªÛŒÙ‡", "ÛŒÚ©ØªØ§", "Ø¯Ø§Ø±Ø§ÛŒÛŒ", "Ø§Ø®ØªØµØ§ØµÛŒ","Ú©Ù…Ù†Ø¯", "Ù¾Ø§Ø¯Ø§Ø´", "Ø§Ø±Ù…ØºØ§Ù†"]
    fund_symbol_ids_to_delete = [s.symbol_id for s in all_symbols if any(keyword in s.symbol_name for keyword in fund_keywords)]
    
    current_day_results = []

    for symbol_data in all_symbols:
        symbol_id = symbol_data.symbol_id
        symbol_name = symbol_data.symbol_name

        if symbol_data.symbol_id in fund_symbol_ids_to_delete:
            continue

        # --- Ø­Ø°Ù ÙˆØ§Ú©Ø´ÛŒ P/E Ú¯Ø±ÙˆÙ‡ Ú©Ù‡ Ø¯ÛŒÚ¯Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯ ---
        # fundamental_record = FundamentalData.query.filter_by(symbol_id=symbol_data.symbol_id).first()
        # current_group_pe = fundamental_record.group_pe_ratio if fundamental_record and pd.notna(fundamental_record.group_pe_ratio) else np.nan
        # -------------------------------------------------

        historical_records = HistoricalData.query.filter_by(symbol_id=symbol_data.symbol_id).order_by(HistoricalData.jdate.asc()).all()
        if not historical_records or len(historical_records) < 60:
            continue

        df = pd.DataFrame([r.__dict__ for r in historical_records])
        numeric_cols = ['open', 'high', 'low', 'close', 'volume', 'buy_i_volume', 'buy_count_i', 'sell_i_volume', 'sell_count_i']
        
        existing_numeric_cols = [col for col in numeric_cols if col in df.columns]
        
        for col in existing_numeric_cols:
            df[col] = pd.to_numeric(df[col], errors='coerce')
            
        df.dropna(subset=['open', 'high', 'low', 'close', 'volume'], inplace=True)
        if len(df) < 60: continue

        # --- Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ ---
        rsi_series = compute_rsi(df['close'])
        macd_line, signal_line, _ = compute_macd(df['close'])
        sma_20 = compute_sma(df['close'], window=20)
        sma_50 = compute_sma(df['close'], window=50)
        volume_ma_20 = compute_sma(df['volume'], window=20)
        atr_series = compute_atr(df['high'], df['low'], df['close'], window=14)
        
        # --- Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¢Ø®Ø±ÛŒÙ† Ù…Ù‚Ø§Ø¯ÛŒØ± ---
        last_row = df.iloc[-1]
        current_close, current_volume = last_row['close'], last_row['volume']
        latest_rsi = rsi_series.iloc[-1] if not rsi_series.empty else np.nan
        latest_sma_20, latest_sma_50 = sma_20.iloc[-1], sma_50.iloc[-1]
        latest_vol_ma_20 = volume_ma_20.iloc[-1]
        
        smf_df = calculate_smart_money_flow(df) if calculate_smart_money_flow else pd.DataFrame()
        latest_ibp = smf_df['individual_buy_power'].iloc[-1] if not smf_df.empty else np.nan

        satisfied_filters, total_score, reason_phrases = [], 0, []

        # --- ØªØ¹Ø±ÛŒÙ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ùˆ Ø¨Ù‡Ø¨ÙˆØ¯ÛŒØ§ÙØªÙ‡ (ÙÛŒÙ„ØªØ± P/E Ø­Ø°Ù Ø´Ø¯Ù‡ Ø§Ø³Øª) ---
        filter_definitions = {
            "ÙˆØ±ÙˆØ¯ Ù¾ÙˆÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¯Ø± Ú©Ù Ù‚ÛŒÙ…ØªÛŒ": {
                "func": lambda: is_rsi_oversold(latest_rsi, 35) and pd.notna(latest_ibp) and latest_ibp > 2.0,
                "score": 20, "reason": "ÙˆØ±ÙˆØ¯ Ù¾ÙˆÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø³Ù†Ú¯ÛŒÙ† Ø¯Ø± Ù†Ø§Ø­ÛŒÙ‡ Ø§Ø´Ø¨Ø§Ø¹ ÙØ±ÙˆØ´"
            },
            "Ú©Ù†Ø¯Ù„ Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ ØµØ¹ÙˆØ¯ÛŒ Ø¨Ø§ Ø­Ø¬Ù… Ø¨Ø§Ù„Ø§": {
                "func": lambda: (last_row['close'] - last_row['open']) > 0.6 * (last_row['high'] - last_row['low']) and \
                                 current_close > last_row['open'] and \
                                 is_high_volume(current_volume, latest_vol_ma_20, 2.0),
                "score": 18, "reason": "Ú©Ù†Ø¯Ù„ Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ ØµØ¹ÙˆØ¯ÛŒ Ø¨Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ú†Ø´Ù…Ú¯ÛŒØ± Ø­Ø¬Ù…"
            },
            "ØªØ«Ø¨ÛŒØª ØªÙ‚Ø§Ø·Ø¹ Ø·Ù„Ø§ÛŒÛŒ": {
                "func": lambda: latest_sma_20 > latest_sma_50 and (sma_20.iloc[-2] <= sma_50.iloc[-2]) and \
                                 current_close > latest_sma_20,
                "score": 15, "reason": "ØªØ«Ø¨ÛŒØª ØªÙ‚Ø§Ø·Ø¹ Ø·Ù„Ø§ÛŒÛŒ Ùˆ Ù‚Ø±Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù‚ÛŒÙ…Øª Ø¨Ø§Ù„Ø§ÛŒ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†â€ŒÙ‡Ø§"
            },
            "Ø´Ú©Ø³Øª Ù…Ù‚Ø§ÙˆÙ…Øª Ø¨Ø§ Ù‚Ø¯Ø±Øª Ø®Ø±ÛŒØ¯Ø§Ø±": {
                "func": lambda: is_resistance_breakout(df['high'], current_close, 40) and pd.notna(latest_ibp) and latest_ibp > 1.5,
                "score": 15, "reason": "Ø´Ú©Ø³Øª Ù…Ù‚Ø§ÙˆÙ…Øª Ù…Ù‡Ù… Û´Û° Ø±ÙˆØ²Ù‡ Ø¨Ø§ Ø­Ù…Ø§ÛŒØª Ù¾ÙˆÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯"
            },
            "ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ù…Ø«Ø¨Øª RSI + Ø§ÙØ²Ø§ÛŒØ´ Ø­Ø¬Ù…": {
                "func": lambda: is_rsi_oversold(latest_rsi, 40) and is_high_volume(current_volume, latest_vol_ma_20, 1.8) and \
                                 (df['low'].iloc[-10:].min() < df['low'].iloc[-20:-10].min()) and \
                                 (rsi_series.iloc[-10:].min() > rsi_series.iloc[-20:-10].min()),
                "score": 12, "reason": "ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ù…Ø«Ø¨Øª RSI Ø¯Ø± Ø±ÙˆÙ†Ø¯ Ù†Ø²ÙˆÙ„ÛŒ Ø¨Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ø­Ø¬Ù…"
            },
            "ÙØ´Ø±Ø¯Ú¯ÛŒ Ù†ÙˆØ³Ø§Ù† (Ù…Ù‚Ø¯Ù…Ù‡ Ø­Ø±Ú©Øª Ø§Ù†ÙØ¬Ø§Ø±ÛŒ)": {
                "func": lambda: (atr_series.iloc[-1] < atr_series.iloc[-20:].min() * 1.1) and (latest_rsi > 50),
                "score": 10, "reason": "Ú©Ø§Ù‡Ø´ Ø´Ø¯ÛŒØ¯ Ù†ÙˆØ³Ø§Ù†Ø§Øª Ùˆ Ù‚Ø±Ø§Ø±Ú¯ÛŒØ±ÛŒ Ø¯Ø± Ø¢Ø³ØªØ§Ù†Ù‡ ÛŒÚ© Ø­Ø±Ú©Øª Ù‚ÙˆÛŒ"
            },
            "Ø¹Ø¨ÙˆØ± MACD Ø§Ø² Ø®Ø· Ø³ÛŒÚ¯Ù†Ø§Ù„": {
                "func": lambda: is_macd_buy_signal(macd_line, signal_line) and macd_line.iloc[-1] < 0,
                "score": 8, "reason": "ØªÙ‚Ø§Ø·Ø¹ ØµØ¹ÙˆØ¯ÛŒ MACD Ø¯Ø± Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ù…Ù†ÙÛŒ (Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ø§Ø²Ú¯Ø´Øª)"
            }
        }

        for fname, finfo in filter_definitions.items():
            try:
                if finfo['func']():
                    satisfied_filters.append(fname)
                    total_score += finfo['score']
                    reason_phrases.append(finfo['reason'])
            except Exception:
                continue

        if total_score <= 0: continue

        reason_str = ", ".join(reason_phrases)
        
        result_payload = {
            "symbol_id": symbol_id, "symbol_name": symbol_name, "jdate": today_jdate_str,
            "score": int(total_score), "satisfied_filters": json.dumps(satisfied_filters, ensure_ascii=False),
            "reason": reason_str, "recommendation_price": current_close,
            "recommendation_jdate": today_jdate_str, "timestamp": datetime.now(),
        }
        current_day_results.append(result_payload)

    # --- Ú¯Ø§Ù… Û²: Ù…Ù†Ø·Ù‚ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ (Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¢Ù¾Ø¯ÛŒØª ÛŒØ§ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÙˆØ¬ÙˆØ¯ÛŒØª) ---
    current_day_results.sort(key=lambda x: x['score'], reverse=True)

    for idx, r in enumerate(current_day_results):
        r['is_golden_key'] = (idx < top_n_symbols and r['score'] > 10)
        if r['score'] >= 30: r['signal_status'] = "ğŸ“ˆ Ø³ÛŒÚ¯Ù†Ø§Ù„ Ù‚ÙˆÛŒ Ø®Ø±ÛŒØ¯"
        elif r['score'] >= 15: r['signal_status'] = "âš ï¸ Ø§Ø­ØªÙ…Ø§Ù„ Ø±Ø´Ø¯"
        else: r['signal_status'] = "âŒ Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø¶Ø¹ÛŒÙ"
        
        final_reason_str = f"ÙˆØ¶Ø¹ÛŒØª Ø³ÛŒÚ¯Ù†Ø§Ù„: {r['signal_status']}, Ø¯Ù„Ø§ÛŒÙ„: {r['reason']}"

        new_obj = GoldenKeyResult(
            symbol_id=r['symbol_id'], symbol_name=r['symbol_name'], jdate=r['jdate'],
            score=r['score'], satisfied_filters=r['satisfied_filters'], reason=final_reason_str,
            recommendation_price=r['recommendation_price'], recommendation_jdate=r['jdate'],
            is_golden_key=r['is_golden_key'], status=r['signal_status'],
            # ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø­Ø°Ù Ø´Ø¯Ù‡ Ø¯ÛŒÚ¯Ø± Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ù†Ø¯Ø§Ø±Ù†Ø¯
            timestamp=r['timestamp']
        )
        db.session.add(new_obj)

    try:
        db.session.commit()
        logger.info(f"Golden Key analysis completed. Total {len(current_day_results)} new results saved.")
        return True, f"Golden Key analysis completed. Total {len(current_day_results)} new results saved."
    except Exception as e:
        db.session.rollback()
        logger.error(f"Error committing Golden Key results: {e}", exc_info=True)
        return False, f"DB commit failed: {str(e)}"

# ---------------------------------------------------------
# API Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ù†ØªØ§ÛŒØ¬ (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±)
# ---------------------------------------------------------
def get_golden_key_results(filters=None, top_n=8):
    # Ø§Ø² Ø¢Ù†Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ù…Ø§ ÙÙ‚Ø· Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ù…Ø±ÙˆØ² Ø±Ø§ Ø¯Ø§Ø±ÛŒÙ…ØŒ Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ max(jdate) Ù†ÛŒØ³Øª.
    # Ø§Ù…Ø§ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ API Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø¢Ù† Ø±Ø§ Ù†Ú¯Ù‡ Ø¯Ø§Ø´Øª.
    latest_date_result = db.session.query(func.max(GoldenKeyResult.jdate)).scalar() 
    if not latest_date_result:
        return {"top_stocks": [], "technical_filters": get_golden_key_filter_definitions(), "last_updated": "Ù†Ø§Ù…Ø´Ø®Øµ"}

    query = GoldenKeyResult.query.filter_by(jdate=latest_date_result)
    
    # ... (Ø¨Ù‚ÛŒÙ‡ Ù…Ù†Ø·Ù‚ ÙˆØ§Ú©Ø´ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§)

    if filters:
        wanted = [f.strip() for f in filters.split(',') if f.strip()]
        all_rows = query.all()
        matched = [r for r in all_rows if all(w in json.loads(r.satisfied_filters or '[]') for w in wanted)]
        rows = matched
    else:
        rows = query.filter_by(is_golden_key=True).order_by(GoldenKeyResult.score.desc()).all()
        if not rows:
            rows = query.order_by(GoldenKeyResult.score.desc()).limit(top_n).all()

    output = [{
        "symbol": r.symbol_name, "symbol_id": r.symbol_id, "jdate": r.jdate,
        "satisfied_filters_list": json.loads(r.satisfied_filters or '[]'),
        "total_score": r.score, "reason": r.reason, "entry_price": r.recommendation_price,
        "status": r.status, "timestamp": r.timestamp.isoformat() if r.timestamp else None
    } for r in rows]
    
    return {"top_stocks": output, "technical_filters": get_golden_key_filter_definitions(), "last_updated": latest_date_result}


def get_golden_key_filter_definitions():
    return [
        {"name": "ÙˆØ±ÙˆØ¯ Ù¾ÙˆÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¯Ø± Ú©Ù Ù‚ÛŒÙ…ØªÛŒ", "category": "Ø¬Ø±ÛŒØ§Ù† ÙˆØ¬ÙˆÙ‡", "description": "Ù‚Ø¯Ø±Øª Ø®Ø±ÛŒØ¯Ø§Ø± Ø­Ù‚ÛŒÙ‚ÛŒ Ø¨ÛŒØ´ Ø§Ø² Û² Ø¨Ø±Ø§Ø¨Ø± ÙØ±ÙˆØ´Ù†Ø¯Ù‡ Ùˆ RSI Ø¯Ø± Ù†Ø§Ø­ÛŒÙ‡ Ø§Ø´Ø¨Ø§Ø¹ ÙØ±ÙˆØ´."},
        {"name": "Ú©Ù†Ø¯Ù„ Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ ØµØ¹ÙˆØ¯ÛŒ Ø¨Ø§ Ø­Ø¬Ù… Ø¨Ø§Ù„Ø§", "category": "Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ú©Ù†Ø¯Ù„ÛŒ", "description": "Ø¨Ø¯Ù†Ù‡ Ú©Ù†Ø¯Ù„ Ø¨ÛŒØ´ Ø§Ø² Û¶Û°Ùª Ú©Ù„ Ú©Ù†Ø¯Ù„ Ø±Ø§ ØªØ´Ú©ÛŒÙ„ Ø¯Ø§Ø¯Ù‡ Ùˆ Ø­Ø¬Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª Û² Ø¨Ø±Ø§Ø¨Ø± Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Û²Û° Ø±ÙˆØ²Ù‡ Ø§Ø³Øª."},
        {"name": "ØªØ«Ø¨ÛŒØª ØªÙ‚Ø§Ø·Ø¹ Ø·Ù„Ø§ÛŒÛŒ", "category": "Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†â€ŒÙ‡Ø§", "description": "Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú© Û²Û° Ø±ÙˆØ²Ù‡ØŒ ÛµÛ° Ø±ÙˆØ²Ù‡ Ø±Ø§ Ø¨Ù‡ Ø¨Ø§Ù„Ø§ Ù‚Ø·Ø¹ Ú©Ø±Ø¯Ù‡ Ùˆ Ù‚ÛŒÙ…Øª Ø¨Ø§Ù„Ø§ÛŒ Ù‡Ø± Ø¯Ùˆ ØªØ«Ø¨ÛŒØª Ø´Ø¯Ù‡."},
        {"name": "Ø´Ú©Ø³Øª Ù…Ù‚Ø§ÙˆÙ…Øª Ø¨Ø§ Ù‚Ø¯Ø±Øª Ø®Ø±ÛŒØ¯Ø§Ø±", "category": "Ø±ÙˆÙ†Ø¯ Ù‚ÛŒÙ…Øª", "description": "Ù‚ÛŒÙ…Øª Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ø³Ù‚Ù Û´Û° Ø±ÙˆØ²Ù‡ Ø®ÙˆØ¯ Ø±Ø§ Ø´Ú©Ø³ØªÙ‡ Ùˆ Ù‚Ø¯Ø±Øª Ø®Ø±ÛŒØ¯Ø§Ø± Ø­Ù‚ÛŒÙ‚ÛŒ Ø¨ÛŒØ´ Ø§Ø² Û±.Ûµ Ø§Ø³Øª."},
        {"name": "ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ù…Ø«Ø¨Øª RSI + Ø§ÙØ²Ø§ÛŒØ´ Ø­Ø¬Ù…", "category": "ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ", "description": "Ù‚ÛŒÙ…Øª Ú©Ù Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ± Ø³Ø§Ø®ØªÙ‡ ÙˆÙ„ÛŒ RSI Ú©Ù Ø¨Ø§Ù„Ø§ØªØ± Ø²Ø¯Ù‡ Ùˆ Ø­Ø¬Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§ÙØ²Ø§ÛŒØ´ ÛŒØ§ÙØªÙ‡."},
        {"name": "ÙØ´Ø±Ø¯Ú¯ÛŒ Ù†ÙˆØ³Ø§Ù† (Ù…Ù‚Ø¯Ù…Ù‡ Ø­Ø±Ú©Øª Ø§Ù†ÙØ¬Ø§Ø±ÛŒ)", "category": "Ù†ÙˆØ³Ø§Ù†", "description": "Ù†ÙˆØ³Ø§Ù†Ø§Øª Ø³Ù‡Ù… (ATR) Ø¨Ù‡ Ú©Ù…ØªØ±ÛŒÙ† Ø­Ø¯ Ø®ÙˆØ¯ Ø¯Ø± Û²Û° Ø±ÙˆØ² Ø§Ø®ÛŒØ± Ø±Ø³ÛŒØ¯Ù‡ Ùˆ RSI Ø¨Ø§Ù„Ø§ÛŒ ÛµÛ° Ø§Ø³Øª."},
        {"name": "Ø¹Ø¨ÙˆØ± MACD Ø§Ø² Ø®Ø· Ø³ÛŒÚ¯Ù†Ø§Ù„", "category": "ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ", "description": "Ø®Ø· MACD Ø®Ø· Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø±Ø§ Ø¯Ø± Ù†Ø§Ø­ÛŒÙ‡ Ù…Ù†ÙÛŒ Ø¨Ù‡ Ø³Ù…Øª Ø¨Ø§Ù„Ø§ Ù‚Ø·Ø¹ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª."},
    ]