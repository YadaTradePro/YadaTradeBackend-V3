# -*- coding: utf-8 -*-
# services/weekly_watchlist_service.py
from extensions import db
from models import HistoricalData, ComprehensiveSymbolData, TechnicalIndicatorData, FundamentalData, WeeklyWatchlistResult, SignalsPerformance, AggregatedPerformance, GoldenKeyResult, CandlestickPatternDetection, MLPrediction, FinancialRatiosData
from flask import current_app
import pandas as pd
from datetime import datetime, timedelta, date
import jdatetime
import uuid
from sqlalchemy import func, text
import logging
import json
import numpy as np
from types import SimpleNamespace
from models import DailySectorPerformance
from typing import List, Dict, Tuple, Optional # NEW: Added for professional type hinting
import time


# NEW: Import for market sentiment analysis
from services.index_data_fetcher import get_market_indices

# Import utility functions
from services.technical_analysis_utils import get_today_jdate_str, normalize_value, calculate_rsi, calculate_macd, calculate_sma, calculate_bollinger_bands, calculate_volume_ma, calculate_atr, calculate_smart_money_flow, check_candlestick_patterns, convert_gregorian_to_jalali, calculate_z_score

# Import analysis_service for aggregated performance calculation
from services import performance_service

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯ÛŒÙ†Ú¯ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù…Ø§Ú˜ÙˆÙ„
logger = logging.getLogger(__name__)

# IMPROVEMENT: Lookback period and minimum history days 
# adjusted for better indicator quality
TECHNICAL_DATA_LOOKBACK_DAYS = 120
MIN_REQUIRED_HISTORY_DAYS = 50

# REVISED: New filter weights dictionary with descriptions for clarity
FILTER_WEIGHTS = {
    # --- High-Impact Leading & Breakout Signals ---
    "Power_Thrust_Signal": {
        "weight": 5,
        "description": "Ø³ÛŒÚ¯Ù†Ø§Ù„ Ù‚Ø¯Ø±Øª: ØªØ±Ú©ÛŒØ¨ÛŒ Ø§Ø² Ø­Ø¬Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù†ÙØ¬Ø§Ø±ÛŒØŒ ÙˆØ±ÙˆØ¯ Ù¾ÙˆÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø³Ù†Ú¯ÛŒÙ† Ø¯Ø± ÛŒÚ© Ø±ÙˆØ² Ù…Ø«Ø¨Øª."
    },
    "RSI_Positive_Divergence": {
        "weight": 5,
        "description": "ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ù…Ø«Ø¨Øª Ø¯Ø± RSIØŒ ÛŒÚ© Ø³ÛŒÚ¯Ù†Ø§Ù„ Ù¾ÛŒØ´Ø±Ùˆ Ù‚ÙˆÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø­ØªÙ…Ø§Ù„ Ø¨Ø§Ø²Ú¯Ø´Øª Ø±ÙˆÙ†Ø¯ Ù†Ø²ÙˆÙ„ÛŒ Ø¨Ù‡ ØµØ¹ÙˆØ¯ÛŒ."
    },
    "Resistance_Broken": {
        "weight": 5,
        "description": "Ø´Ú©Ø³Øª ÛŒÚ© Ø³Ø·Ø­ Ù…Ù‚Ø§ÙˆÙ…Øª Ú©Ù„ÛŒØ¯ÛŒØŒ Ù†Ø´Ø§Ù†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ù‚Ø¯Ø±Øª Ø®Ø±ÛŒØ¯Ø§Ø±Ø§Ù† Ùˆ Ù¾ØªØ§Ù†Ø³ÛŒÙ„ Ø´Ø±ÙˆØ¹ ÛŒÚ© Ø­Ø±Ú©Øª ØµØ¹ÙˆØ¯ÛŒ Ø¬Ø¯ÛŒØ¯."
    },
    "Static_Resistance_Broken": {
        "weight": 5,
        "description": "Ø´Ú©Ø³Øª ÛŒÚ© Ù…Ù‚Ø§ÙˆÙ…Øª Ù…Ù‡Ù… Ø§Ø³ØªØ§ØªÛŒÚ© (Ú©Ù„Ø§Ø³ÛŒÚ©)ØŒ Ø³ÛŒÚ¯Ù†Ø§Ù„ÛŒ Ø¨Ø³ÛŒØ§Ø± Ù…Ø¹ØªØ¨Ø± Ø¨Ø±Ø§ÛŒ Ø§Ø¯Ø§Ù…Ù‡ Ø±Ø´Ø¯."
    },
    "Squeeze_Momentum_Fired_Long": {
        "weight": 4,
        "description": "Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± Squeeze Momentum Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø®Ø±ÛŒØ¯ (Ø®Ø±ÙˆØ¬ Ø§Ø² ÙØ´Ø±Ø¯Ú¯ÛŒ) ØµØ§Ø¯Ø± Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª Ú©Ù‡ Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡ Ø§Ø­ØªÙ…Ø§Ù„ ÛŒÚ© Ø­Ø±Ú©Øª Ø§Ù†ÙØ¬Ø§Ø±ÛŒ Ø§Ø³Øª."
    },
    "Stochastic_Bullish_Cross_Oversold": {
        "weight": 4,
        "description": "ØªÙ‚Ø§Ø·Ø¹ ØµØ¹ÙˆØ¯ÛŒ Ø¯Ø± Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± Ø§Ø³ØªÙˆÚ©Ø§Ø³ØªÛŒÚ© Ø¯Ø± Ù†Ø§Ø­ÛŒÙ‡ Ø§Ø´Ø¨Ø§Ø¹ ÙØ±ÙˆØ´ØŒ ÛŒÚ© Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø®Ø±ÛŒØ¯ Ú©Ù„Ø§Ø³ÛŒÚ© Ùˆ Ù…Ø¹ØªØ¨Ø±."
    },
    "Consolidation_Breakout_Candidate": {
        "weight": 3,
        "description": "Ø³Ù‡Ù… Ø¯Ø± ÙØ§Ø² ØªØ±Ø§Ú©Ù… Ùˆ Ù†ÙˆØ³Ø§Ù† Ú©Ù… Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡ ÛŒÚ© Ø­Ø±Ú©Øª Ù‚ÙˆÛŒ (Ø´Ú©Ø³Øª) Ø§Ø³Øª."
    },
    "Near_Static_Support": {
        "weight": 3,
        "description": "Ù‚ÛŒÙ…Øª Ø¯Ø± Ù†Ø²Ø¯ÛŒÚ©ÛŒ ÛŒÚ© Ø³Ø·Ø­ Ø­Ù…Ø§ÛŒØªÛŒ Ø§Ø³ØªØ§ØªÛŒÚ© Ù…Ø¹ØªØ¨Ø± Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯ Ú©Ù‡ Ø±ÛŒØ³Ú© Ø¨Ù‡ Ø±ÛŒÙˆØ§Ø±Ø¯ Ù…Ù†Ø§Ø³Ø¨ÛŒ Ø¨Ø±Ø§ÛŒ ÙˆØ±ÙˆØ¯ ÙØ±Ø§Ù‡Ù… Ù…ÛŒâ€ŒÚ©Ù†Ø¯."
    },
    "Bollinger_Lower_Band_Touch": {
        "weight": 2,
        "description": "Ù‚ÛŒÙ…Øª Ø¨Ù‡ Ø¨Ø§Ù†Ø¯ Ù¾Ø§ÛŒÛŒÙ† Ø¨ÙˆÙ„ÛŒÙ†Ú¯Ø± Ø¨Ø±Ø®ÙˆØ±Ø¯ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª Ú©Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ù†Ø´Ø§Ù†Ù‡ Ø¨Ø§Ø²Ú¯Ø´Øª Ú©ÙˆØªØ§Ù‡â€ŒÙ…Ø¯Øª Ù‚ÛŒÙ…Øª Ø¨Ø§Ø´Ø¯."
    },

    # --- Trend Confirmation & Strength Signals ---
    "IsInLeadingSector": {
        "weight": 4,
        "description": "Ø³Ù‡Ù… Ù…ØªØ¹Ù„Ù‚ Ø¨Ù‡ ÛŒÚ©ÛŒ Ø§Ø² ØµÙ†Ø§ÛŒØ¹ Ù¾ÛŒØ´Ø±Ùˆ Ùˆ Ù…ÙˆØ±Ø¯ ØªÙˆØ¬Ù‡ Ø¨Ø§Ø²Ø§Ø± Ø§Ø³Øª Ú©Ù‡ Ø´Ø§Ù†Ø³ Ù…ÙˆÙÙ‚ÛŒØª Ø±Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯."
    },
    "Positive_Real_Money_Flow_Trend_10D": {
        "weight": 3,
        "description": "Ø¨Ø±Ø¢ÛŒÙ†Ø¯ ÙˆØ±ÙˆØ¯ Ù¾ÙˆÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯ (Ø­Ù‚ÛŒÙ‚ÛŒ) Ø¯Ø± Û±Û° Ø±ÙˆØ² Ú¯Ø°Ø´ØªÙ‡ Ù…Ø«Ø¨Øª Ø¨ÙˆØ¯Ù‡ Ø§Ø³Øª."
    },
    "Heavy_Individual_Buy_Pressure": {
        "weight": 3,
        "description": "Ø³Ø±Ø§Ù†Ù‡ Ø®Ø±ÛŒØ¯ Ø­Ù‚ÛŒÙ‚ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± Ø±ÙˆØ² Ø¢Ø®Ø± Ø¨Ù‡ Ø·ÙˆØ± Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ÛŒ Ø¨Ø§Ù„Ø§ØªØ± Ø§Ø² Ø³Ø±Ø§Ù†Ù‡ ÙØ±ÙˆØ´ Ø¨ÙˆØ¯Ù‡ Ø§Ø³Øª (ÙˆØ±ÙˆØ¯ Ù¾ÙˆÙ„ Ø³Ù†Ú¯ÛŒÙ†)."
    },
    "MACD_Bullish_Cross_Confirmed": {
        "weight": 2,
        "description": "Ø®Ø· MACD Ø®Ø· Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù‡ Ø³Ù…Øª Ø¨Ø§Ù„Ø§ Ù‚Ø·Ø¹ Ú©Ø±Ø¯Ù‡ Ú©Ù‡ ØªØ§ÛŒÛŒØ¯ÛŒ Ø¨Ø± Ø´Ø±ÙˆØ¹ Ø±ÙˆÙ†Ø¯ ØµØ¹ÙˆØ¯ÛŒ Ø§Ø³Øª."
    },
    "HalfTrend_Buy_Signal": {
        "weight": 2,
        "description": "Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± HalfTrend Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø®Ø±ÛŒØ¯ ØµØ§Ø¯Ø± Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª."
    },
    "High_Volume_On_Up_Day": {
        "weight": 2,
        "description": "Ø­Ø¬Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¯Ø± ÛŒÚ© Ø±ÙˆØ² Ù…Ø«Ø¨Øª Ø¨Ù‡ Ø·ÙˆØ± Ù…Ø¹Ù†Ø§Ø¯Ø§Ø±ÛŒ Ø§ÙØ²Ø§ÛŒØ´ ÛŒØ§ÙØªÙ‡ Ú©Ù‡ Ù†Ø´Ø§Ù† Ø§Ø² Ø­Ù…Ø§ÛŒØª Ù‚ÙˆÛŒ Ø§Ø² Ø±Ø´Ø¯ Ù‚ÛŒÙ…Øª Ø¯Ø§Ø±Ø¯."
    },
    "Price_Above_SMA50": {
        "weight": 1,
        "description": "Ù‚ÛŒÙ…Øª Ø¨Ø§Ù„Ø§ØªØ± Ø§Ø² Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú© ÛµÛ° Ø±ÙˆØ²Ù‡ Ø®ÙˆØ¯ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯ Ú©Ù‡ Ù†Ø´Ø§Ù†Ù‡ Ø±ÙˆÙ†Ø¯ ØµØ¹ÙˆØ¯ÛŒ Ù…ÛŒØ§Ù†â€ŒÙ…Ø¯Øª Ø§Ø³Øª."
    },

    # --- Fundamental Quality Filters ---
    "Reasonable_PE": {
        "weight": 1, 
        "description": "Ù†Ø³Ø¨Øª P/E Ø³Ù‡Ù… Ø¯Ø± Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ù…Ù†Ø·Ù‚ÛŒ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯ Ùˆ Ø³Ù‡Ù… Ø§Ø² Ù†Ø¸Ø± Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø§Ø±Ø²Ù†Ø¯Ù‡ Ø§Ø³Øª."
    },
    "Fundamental_PE_vs_Group": { # âœ… NEW
        "weight": 2, 
        "description": "Ù†Ø³Ø¨Øª P/E Ø³Ù‡Ù… Ø§Ø² P/E Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ú¯Ø±ÙˆÙ‡ Ú©ÙˆÚ†Ú©ØªØ± Ø§Ø³Øª Ú©Ù‡ Ù†Ø´Ø§Ù†Ù‡ Ø§Ø±Ø²Ù†Ø¯Ú¯ÛŒ Ù†Ø³Ø¨ÛŒ Ø§Ø³Øª."
    },
    "Reasonable_PS": {
        "weight": 1,
        "description": "Ù†Ø³Ø¨Øª P/S Ø³Ù‡Ù… Ø¯Ø± Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ù…Ù†Ø·Ù‚ÛŒ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯."
    },
    "Positive_EPS": {
        "weight": 0.5,
        "description": "Ø³ÙˆØ¯ Ø¨Ù‡ Ø§Ø²Ø§ÛŒ Ù‡Ø± Ø³Ù‡Ù… (EPS) Ø´Ø±Ú©Øª Ù…Ø«Ø¨Øª Ø§Ø³Øª."
    },

    # --- Advanced Flow/Ratio Filters --- # âœ… NEW SECTION
    "Advanced_Strong_Real_Buyer_Ratio": {
        "weight": 3,
        "description": "Ù†Ø³Ø¨Øª Ù‚Ø¯Ø±Øª Ø®Ø±ÛŒØ¯Ø§Ø± Ø­Ù‚ÛŒÙ‚ÛŒ Ø¨Ù‡ ÙØ±ÙˆØ´Ù†Ø¯Ù‡ Ø­Ù‚ÛŒÙ‚ÛŒ Ø¯Ø± Ø±ÙˆØ² Ø¢Ø®Ø± Ø¨Ø§Ù„Ø§ Ø¨ÙˆØ¯Ù‡ Ø§Ø³Øª."
    },
    "Advanced_Volume_Surge_Ratio": {
        "weight": 2,
        "description": "Ø­Ø¬Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø±ÙˆØ² Ø¢Ø®Ø± Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Û²Û° Ø±ÙˆØ²Ù‡ Ø¬Ù‡Ø´ Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³Øª."
    },
    
    # --- Candlestick Filters ---
    "Bullish_Engulfing_Detected": {
        "weight": 3,
        "description": "Ø§Ù„Ú¯ÙˆÛŒ Ú©Ù†Ø¯Ù„ÛŒ Ù¾ÙˆØ´Ø§Ù†Ù†Ø¯Ù‡ ØµØ¹ÙˆØ¯ÛŒ (Bullish Engulfing) Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª."
    },
    "Hammer_Detected": {
        "weight": 3,
        "description": "Ø§Ù„Ú¯ÙˆÛŒ Ú©Ù†Ø¯Ù„ÛŒ Ú†Ú©Ø´ (Hammer) Ú©Ù‡ ÛŒÚ© Ø§Ù„Ú¯ÙˆÛŒ Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ ØµØ¹ÙˆØ¯ÛŒ Ø§Ø³ØªØŒ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª."
    },
    "Morning_Star_Detected": {
        "weight": 4,
        "description": "Ø§Ù„Ú¯ÙˆÛŒ Ú©Ù†Ø¯Ù„ÛŒ Ø³ØªØ§Ø±Ù‡ ØµØ¨Ø­Ú¯Ø§Ù‡ÛŒ (Morning Star)ØŒ ÛŒÚ© Ø§Ù„Ú¯ÙˆÛŒ Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ Ø¨Ø³ÛŒØ§Ø± Ù…Ø¹ØªØ¨Ø±ØŒ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª."
    },
    
    # --- ML Prediction Filter ---
    "ML_Predicts_Uptrend": {
        "weight": 2,
        "description": "Ù…Ø¯Ù„ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†ØŒ Ø§Ø­ØªÙ…Ø§Ù„ Ø¨Ø§Ù„Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ø±ÙˆÙ†Ø¯ ØµØ¹ÙˆØ¯ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª."
    },

    # --- Penalties & Negative Scores (Crucial for avoiding peaks) ---
    "RSI_Is_Overbought": {
        "weight": -4,
        "description": "Ø¬Ø±ÛŒÙ…Ù‡ Ù…Ù†ÙÛŒ: RSI Ø¯Ø± Ù†Ø§Ø­ÛŒÙ‡ Ø§Ø´Ø¨Ø§Ø¹ Ø®Ø±ÛŒØ¯ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯ Ú©Ù‡ Ø±ÛŒØ³Ú© Ø§ØµÙ„Ø§Ø­ Ù‚ÛŒÙ…Øª Ø±Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯."
    },
    "Price_Too_Stretched_From_SMA50": {
        "weight": -3,
        "description": "Ø¬Ø±ÛŒÙ…Ù‡ Ù…Ù†ÙÛŒ: Ù‚ÛŒÙ…Øª ÙØ§ØµÙ„Ù‡ Ø²ÛŒØ§Ø¯ÛŒ Ø§Ø² Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú© ÛµÛ° Ø±ÙˆØ²Ù‡ Ú¯Ø±ÙØªÙ‡ Ú©Ù‡ Ø§Ø­ØªÙ…Ø§Ù„ Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø±Ø§ Ø¨Ø§Ù„Ø§ Ù…ÛŒâ€ŒØ¨Ø±Ø¯."
    },
    "Negative_Real_Money_Flow_Trend_10D": {
        "weight": -2,
        "description": "Ø¬Ø±ÛŒÙ…Ù‡ Ù…Ù†ÙÛŒ: Ø¨Ø±Ø¢ÛŒÙ†Ø¯ ÙˆØ±ÙˆØ¯ Ù¾ÙˆÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¯Ø± Û±Û° Ø±ÙˆØ² Ú¯Ø°Ø´ØªÙ‡ Ù…Ù†ÙÛŒ Ø¨ÙˆØ¯Ù‡ Ø§Ø³Øª (Ø®Ø±ÙˆØ¬ Ù¾ÙˆÙ„)."
    }
}


# Ú©Ù…Ú©â€ŒÚ©Ù†Ù†Ø¯Ù‡: Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† ÛŒÚ© Ø³Ø±ÛŒ close Ù‚Ø§Ø¨Ù„â€ŒØ§Ø¹ØªÙ…Ø§Ø¯ Ø§Ø² historical DF
def _get_close_series_from_hist_df(hist_df):
    """
    Accepts a historical dataframe and returns a numeric pandas Series of close prices.
    Tries common column names: 'close_price', 'close', 'final'
    """
    if hist_df is None or hist_df.empty:
        return pd.Series(dtype=float)

    for col in ['close_price', 'close', 'final']:
        if col in hist_df.columns:
            ser = pd.to_numeric(hist_df[col], errors='coerce').dropna()
            if not ser.empty:
                return ser
    return pd.Series(dtype=float)



# --- NEW: Market Sentiment Analysis Function ---
def _get_market_sentiment() -> str:
    """
    Determines short-term market sentiment based on the average daily change of major indices over the last 3 trading days.
    Uses data from DailyIndexData table.
    Returns: 'Bullish', 'Neutral', or 'Bearish'.
    """
    try:
        # Get last 3 distinct jdates
        last_jdates_query = db.session.query(DailyIndexData.jdate).distinct().order_by(DailyIndexData.jdate.desc()).limit(3).all()
        if not last_jdates_query:
            logger.warning("No index data available, defaulting to Neutral")
            return "Neutral"

        last_jdates = [row[0] for row in last_jdates_query]

        # Fetch data for these dates and types
        index_data = db.session.query(DailyIndexData).filter(
            DailyIndexData.jdate.in_(last_jdates),
            DailyIndexData.index_type.in_(['Total_Index', 'Equal_Weighted_Index'])
        ).all()

        if len(index_data) < 2:  # At least one per index
            logger.warning("Insufficient index data for both types, defaulting to Neutral")
            return "Neutral"

        # Group by index_type
        total_changes = [d.percent_change for d in index_data if d.index_type == 'Total_Index']
        equal_changes = [d.percent_change for d in index_data if d.index_type == 'Equal_Weighted_Index']

        if len(total_changes) < 1 or len(equal_changes) < 1:
            logger.warning("Missing data for one index type, defaulting to Neutral")
            return "Neutral"

        avg_total = np.mean(total_changes)
        avg_equal = np.mean(equal_changes)

        logger.info(f"Market Sentiment Averages over last {len(last_jdates)} days: Total {avg_total:.2f}%, Equal {avg_equal:.2f}%")

        if avg_total > 0.3 and avg_equal > 0.3:
            logger.info(f"Market Sentiment: Bullish (Total: {avg_total:.2f}%, Equal: {avg_equal:.2f}%)")
            return "Bullish"
        elif avg_total < -0.3 and avg_equal < -0.3:
            logger.info(f"Market Sentiment: Bearish (Total: {avg_total:.2f}%, Equal: {avg_equal:.2f}%)")
            return "Bearish"
        else:
            logger.info(f"Market Sentiment: Neutral (Total: {avg_total:.2f}%, Equal: {avg_equal:.2f}%)")
            return "Neutral"
    except Exception as e:
        logger.error(f"Error fetching market sentiment from DB: {e}, defaulting to Neutral")
        return "Neutral"

# --- REVISED: Filter Functions ---
def _check_market_condition_filters(hist_df, tech_df):
    """
    Checks for individual stock conditions like overbought state or consolidation.
    """
    satisfied_filters, reason_parts = [], {"market_condition": []}
    if tech_df is None or tech_df.empty or hist_df is None or len(hist_df) < 50:
        return satisfied_filters, reason_parts

    last_tech = tech_df.iloc[-1]
    close_ser = _get_close_series_from_hist_df(hist_df)
    if close_ser.empty:
        return satisfied_filters, reason_parts
    last_close = close_ser.iloc[-1]
    last_hist = hist_df.iloc[-1]

    # --- Check 1: RSI Overbought Condition (Penalize only if weak) ---
    if hasattr(last_tech, 'RSI') and last_tech.RSI is not None and last_tech.RSI > 70:
        is_negative_divergence = (
            hasattr(last_tech, 'RSI_Divergence') and last_tech.RSI_Divergence == "Negative"
        )
        historical_volume_series = hist_df.tail(10)['volume']
        average_volume = historical_volume_series.mean() if not historical_volume_series.empty else 0
        is_high_volume = last_hist['volume'] > average_volume * 1.5 if average_volume > 0 else False

        if is_negative_divergence or not is_high_volume:
            satisfied_filters.append("RSI_Is_Overbought")
            reason_parts["market_condition"].append(
                f"RSI ({last_tech.RSI:.2f}) overbought with weakness."
            )
        else:
            reason_parts["market_condition"].append(
                f"RSI ({last_tech.RSI:.2f}) overbought but supported by strong volume (no penalty)."
            )

    # --- Check 2: Price is too far from its SMA50 (Penalize) ---
    if hasattr(last_tech, 'SMA_50') and last_tech.SMA_50 is not None and last_tech.SMA_50 > 0:
        stretch_percent = ((last_close - last_tech.SMA_50) / last_tech.SMA_50) * 100
        if stretch_percent > 20:
            satisfied_filters.append("Price_Too_Stretched_From_SMA50")
            reason_parts["market_condition"].append(
                f"Price is overextended ({stretch_percent:.1f}%) from SMA50."
            )

    # --- Check 3: Consolidation Pattern (Reward) ---
    if hasattr(last_tech, 'ATR'):
        atr_series = pd.to_numeric(tech_df['ATR'].dropna())
        if len(atr_series) > 30:
            recent_atr_avg = atr_series.tail(10).mean()
            historical_atr_avg = atr_series.tail(30).mean()
            if recent_atr_avg < (historical_atr_avg * 0.7):
                satisfied_filters.append("Consolidation_Breakout_Candidate")
                reason_parts["market_condition"].append(
                    "Stock is in a low-volatility consolidation phase."
                )

    return satisfied_filters, reason_parts

# ØªØºÛŒÛŒØ± Ø¯Ø± is_data_sufficient: Ø§Ù†Ø¹Ø·Ø§Ùâ€ŒÙ¾Ø°ÛŒØ±ØªØ± Ùˆ Ù…Ù‚Ø§ÙˆÙ…â€ŒØªØ±
def is_data_sufficient(data_df, min_len):
    """
    Checks if the provided DataFrame is not empty and has at least min_len records.
    """
    if data_df is None or data_df.empty:
        return False
    return len(data_df) >= min_len

def convert_jalali_to_gregorian_timestamp(jdate_str):
    """
    Converts a Jalali date string (YYYY-MM-DD) to a pandas Timestamp (Gregorian).
    """
    if pd.notna(jdate_str) and isinstance(jdate_str, str):
        try:
            jy, jm, jd = map(int, jdate_str.split('-'))
            gregorian_date = jdatetime.date(jy, jm, jd).togregorian()
            return pd.Timestamp(gregorian_date)
        except ValueError:
            return pd.NaT
    return pd.NaT

# Helper function to safely get a value
def _get_attr_safe(rec, attr, default=None):
    val = getattr(rec, attr, default)
    if isinstance(val, (pd.Series, pd.DataFrame)):
        return val.iloc[0] if not val.empty else default
    return val

# --- REFACTORED: Technical filters are broken down into smaller functions ---

def _check_oscillator_signals(technical_rec, prev_tech_rec, close_ser):
    """Checks oscillator-based signals like RSI and Stochastic."""
    satisfied_filters, reason_parts = [], {"technical": []}
    
    # RSI Positive Divergence
    current_rsi = _get_attr_safe(technical_rec, 'RSI')
    prev_rsi = _get_attr_safe(prev_tech_rec, 'RSI')
    if current_rsi is not None and prev_rsi is not None and len(close_ser) > 1:
        if current_rsi > prev_rsi and close_ser.iloc[-1] < close_ser.iloc[-2]:
            satisfied_filters.append("RSI_Positive_Divergence")
            reason_parts["technical"].append(f"Positive divergence on RSI ({current_rsi:.2f}).")

    # Stochastic Oscillator
    current_stoch_k = _get_attr_safe(technical_rec, 'Stochastic_K')
    current_stoch_d = _get_attr_safe(technical_rec, 'Stochastic_D')
    prev_stoch_k = _get_attr_safe(prev_tech_rec, 'Stochastic_K')
    prev_stoch_d = _get_attr_safe(prev_tech_rec, 'Stochastic_D')
    if all(x is not None for x in [current_stoch_k, current_stoch_d, prev_stoch_k, prev_stoch_d]):
        if current_stoch_k > current_stoch_d and prev_stoch_k <= prev_stoch_d and current_stoch_d < 25:
            satisfied_filters.append("Stochastic_Bullish_Cross_Oversold")
            reason_parts["technical"].append("Stochastic bullish cross in oversold area.")
            
    return satisfied_filters, reason_parts

def _check_trend_signals(technical_rec, prev_tech_rec, last_close_val):
    """Checks trend-following signals like MACD, SMA, HalfTrend, and Resistance breaks."""
    satisfied_filters, reason_parts = [], {"technical": []}

    # MACD Cross
    current_macd = _get_attr_safe(technical_rec, 'MACD')
    current_macd_signal = _get_attr_safe(technical_rec, 'MACD_Signal')
    prev_macd = _get_attr_safe(prev_tech_rec, 'MACD')
    prev_macd_signal = _get_attr_safe(prev_tech_rec, 'MACD_Signal')
    if all(x is not None for x in [current_macd, current_macd_signal, prev_macd, prev_macd_signal]):
        if current_macd > current_macd_signal and prev_macd <= prev_macd_signal:
            satisfied_filters.append("MACD_Bullish_Cross_Confirmed")

    # Price vs SMA50
    sma50 = _get_attr_safe(technical_rec, 'SMA_50')
    if sma50 is not None and last_close_val > sma50:
        satisfied_filters.append("Price_Above_SMA50")
        
    # HalfTrend
    current_halftrend = _get_attr_safe(technical_rec, 'halftrend_signal')
    prev_halftrend = _get_attr_safe(prev_tech_rec, 'halftrend_signal')
    if current_halftrend == 1 and prev_halftrend != 1:
        satisfied_filters.append("HalfTrend_Buy_Signal")
        
    # Dynamic Resistance Break
    resistance_broken = _get_attr_safe(technical_rec, 'resistance_broken')
    if resistance_broken:
        satisfied_filters.append("Resistance_Broken")
        res_level = _get_attr_safe(technical_rec, 'resistance_level_50d', 'N/A')
        reason_parts["technical"].append(f"Broke a key dynamic resistance level around {res_level}.")

    return satisfied_filters, reason_parts

def _check_volatility_signals(hist_df, technical_rec, last_close_val):
    """Checks volatility-based signals like Bollinger Bands and Squeeze Momentum."""
    satisfied_filters, reason_parts = [], {"technical": []}

    # Bollinger Lower Band
    bollinger_low = _get_attr_safe(technical_rec, 'Bollinger_Low')
    if bollinger_low is not None and last_close_val < bollinger_low:
        satisfied_filters.append("Bollinger_Lower_Band_Touch")

    # Squeeze Momentum
    prev_tech_rec = hist_df.iloc[-2] if len(hist_df) > 1 else technical_rec
    current_squeeze_on = _get_attr_safe(technical_rec, 'squeeze_on')
    prev_squeeze_on = _get_attr_safe(prev_tech_rec, 'squeeze_on')
    if current_squeeze_on == False and prev_squeeze_on == True:
        satisfied_filters.append("Squeeze_Momentum_Fired_Long")
        reason_parts["technical"].append("Squeeze Momentum indicator fired long.")
        
    return satisfied_filters, reason_parts

def _check_volume_signals(hist_df, close_ser):
    """Checks for volume-based signals."""
    satisfied_filters, reason_parts = [], {"technical": []}
    
    if 'volume' in hist_df.columns and len(hist_df) >= 20 and len(close_ser) > 1:
        volume_z_score = calculate_z_score(pd.to_numeric(hist_df['volume'], errors='coerce').dropna().iloc[-20:])
        if volume_z_score is not None and volume_z_score > 1.5 and close_ser.iloc[-1] > close_ser.iloc[-2]:
            satisfied_filters.append("High_Volume_On_Up_Day")
            reason_parts["technical"].append(f"High volume (Z-Score: {volume_z_score:.2f}) on a positive day.")
            
    return satisfied_filters, reason_parts

def _check_technical_filters(hist_df, tech_df):
    """
    Checks all technical indicators by calling specialized sub-functions.
    (Coordinator function)
    """
    all_satisfied_filters, all_reason_parts = [], {"technical": []}
    if tech_df is None or tech_df.empty or len(tech_df) < 2:
        return all_satisfied_filters, all_reason_parts

    technical_rec = tech_df.iloc[-1]
    prev_tech_rec = tech_df.iloc[-2]
    
    close_ser = _get_close_series_from_hist_df(hist_df)
    last_close_val = close_ser.iloc[-1] if not close_ser.empty else None
    if last_close_val is None:
        return all_satisfied_filters, all_reason_parts

    # Call sub-functions and aggregate results
    for func in [_check_oscillator_signals, _check_trend_signals, _check_volatility_signals, _check_volume_signals]:
        # Adjust arguments as needed per function signature
        if func in [_check_oscillator_signals, _check_trend_signals]:
             satisfied, reasons = func(technical_rec, prev_tech_rec, close_ser if func == _check_oscillator_signals else last_close_val)
        elif func == _check_volatility_signals:
            # Note: _check_volatility_signals originally took hist_df, technical_rec, last_close_val,
            # but the new refactored argument list for technical filters is used here.
            # Assuming tech_df passed to _check_volatility_signals should be hist_df here for consistency.
            satisfied, reasons = func(hist_df, technical_rec, last_close_val)
        else: # _check_volume_signals
            satisfied, reasons = func(hist_df, close_ser)

        all_satisfied_filters.extend(satisfied)
        if "technical" in reasons:
            all_reason_parts["technical"].extend(reasons["technical"])

    return all_satisfied_filters, all_reason_parts

# --- END REFACTORED SECTION ---

# âœ… REPLACED: Updated _check_fundamental_filters for professional P/E checks
def _check_fundamental_filters(fundamental_rec):
    """
    Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„ (P/EØŒ P/SØŒ EPS) Ø¨Ø§ Ø±ÙˆÛŒÚ©Ø±Ø¯ Ø§Ø±Ø²Ø´â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒâ€ŒØªØ± (Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ Ú¯Ø±ÙˆÙ‡).
    """
    satisfied_filters = []
    reason_parts = {"fundamental": []}
    
    # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ø³ØªØ§Ù†Ù‡
    MAX_PE_RATIO_ABSOLUTE = 15.0  # P/E Ù…Ø·Ù„Ù‚
    MAX_PE_GROUP_RATIO = 1.25     # P/E Ù†Ø³Ø¨Øª Ø¨Ù‡ P/E Ú¯Ø±ÙˆÙ‡ (Ø­Ø¯Ø§Ú©Ø«Ø± 1.25 Ø¨Ø±Ø§Ø¨Ø±)
    MAX_PS_RATIO = 3.0            # P/S Ù…Ø·Ù„Ù‚

    if fundamental_rec:
        # 1. Ø¨Ø±Ø±Ø³ÛŒ P/E
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² getattr Ø¨Ø±Ø§ÛŒ Ø§Ù†Ø¹Ø·Ø§Ùâ€ŒÙ¾Ø°ÛŒØ±ÛŒ Ø¨ÛŒÙ† pe_ratio Ùˆ pe
        pe = getattr(fundamental_rec, 'pe_ratio', getattr(fundamental_rec, 'pe', None))
        group_pe = getattr(fundamental_rec, 'group_pe_ratio', None)
        
        # ÙÛŒÙ„ØªØ± P/E Ù…Ø·Ù„Ù‚
        if pe is not None and 0 < pe < MAX_PE_RATIO_ABSOLUTE:
            satisfied_filters.append("Reasonable_PE")
            reason_parts["fundamental"].append(f"P/E ({pe:.2f}) Ø¯Ø± Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ù…Ù†Ø·Ù‚ÛŒ Ø§Ø³Øª (Ø­Ø¯Ø§Ú©Ø«Ø± {MAX_PE_RATIO_ABSOLUTE}).")
        
        # ÙÛŒÙ„ØªØ± P/E Ù†Ø³Ø¨Øª Ø¨Ù‡ Ú¯Ø±ÙˆÙ‡
        if pe is not None and group_pe is not None and group_pe > 0:
            pe_vs_group = pe / group_pe
            if pe_vs_group < MAX_PE_GROUP_RATIO:
                satisfied_filters.append("Fundamental_PE_vs_Group")
                reason_parts["fundamental"].append(f"P/E Ø³Ù‡Ù… ({pe_vs_group:.2f}x) Ú©Ù…ØªØ± Ø§Ø² {MAX_PE_GROUP_RATIO}x P/E Ú¯Ø±ÙˆÙ‡ Ø§Ø³Øª.")

        # 2. Ø¨Ø±Ø±Ø³ÛŒ P/S
        ps = getattr(fundamental_rec, 'p_s_ratio', getattr(fundamental_rec, 'psr', None))
        if ps is not None and 0 < ps < MAX_PS_RATIO:
            satisfied_filters.append("Reasonable_PS")
            reason_parts["fundamental"].append(f"P/S ({ps:.2f}) Ø¯Ø± Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ù…Ù†Ø·Ù‚ÛŒ Ø§Ø³Øª (Ø­Ø¯Ø§Ú©Ø«Ø± {MAX_PS_RATIO}).")

        # 3. Ø¨Ø±Ø±Ø³ÛŒ EPS Ù…Ø«Ø¨Øª
        eps = getattr(fundamental_rec, 'eps', None)
        if eps is not None and eps > 0:
            satisfied_filters.append("Positive_EPS")
    
    return satisfied_filters, reason_parts

def _check_smart_money_filters(hist_df):
    """REVISED: Now also checks for heavy individual buy pressure on the last day."""
    satisfied_filters = []
    reason_parts = {"smart_money": []}
    trend_lookback = 10
    if hist_df is None or hist_df.empty or 'buy_i_volume' not in hist_df.columns:
        return satisfied_filters, reason_parts

    # Check 1: 10-Day Real Money Flow Trend
    if len(hist_df) >= trend_lookback:
        smart_money_flow_df = calculate_smart_money_flow(hist_df)
        if not smart_money_flow_df.empty and len(smart_money_flow_df) >= trend_lookback:
            trend_net_flow = smart_money_flow_df['individual_net_flow'].iloc[-trend_lookback:].sum()
            if trend_net_flow > 0:
                satisfied_filters.append("Positive_Real_Money_Flow_Trend_10D")
                reason_parts["smart_money"].append(f"Positive real money inflow over the last {trend_lookback} days.")
            elif trend_net_flow < 0:
                satisfied_filters.append("Negative_Real_Money_Flow_Trend_10D")
    
    # NEW Check 2: Heavy Individual Buy Pressure on the last day
    last_day = hist_df.iloc[-1]
    required_cols = ['buy_i_volume', 'buy_i_count', 'sell_i_volume', 'sell_i_count']
    if all(col in last_day and pd.notna(last_day[col]) for col in required_cols):
        if last_day['buy_i_count'] > 0 and last_day['sell_i_count'] > 0:
            per_capita_buy = last_day['buy_i_volume'] / last_day['buy_i_count']
            per_capita_sell = last_day['sell_i_volume'] / last_day['sell_i_count']
            
            # Check if per capita buy is 2.5x greater than sell
            if per_capita_buy > (per_capita_sell * 2.5):
                satisfied_filters.append("Heavy_Individual_Buy_Pressure")
                reason_parts["smart_money"].append(f"Significant buy pressure detected (Per capita buy: {per_capita_buy:,.0f} vs sell: {per_capita_sell:,.0f}).")

    return satisfied_filters, reason_parts


def _check_power_thrust_signal(hist_df, close_ser):
    """
    Checks for a Power Thrust Signal: a combination of high volume,
    heavy individual buy pressure, and a positive closing day.
    """
    satisfied_filters, reason_parts = [], {"power_thrust": []}
    
    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø¯Ù‡ Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„
    if hist_df is None or len(hist_df) < 20 or close_ser.empty or len(close_ser) < 2:
        return satisfied_filters, reason_parts

    last_day = hist_df.iloc[-1]
    
    # --- Ø´Ø±Ø· Û±: Ø±ÙˆØ² Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ Ù…Ø«Ø¨Øª ---
    is_up_day = close_ser.iloc[-1] > close_ser.iloc[-2]
    if not is_up_day:
        return satisfied_filters, reason_parts # Ø§Ú¯Ø± Ø±ÙˆØ² Ù…Ø«Ø¨Øª Ù†ÛŒØ³ØªØŒ Ø§Ø¯Ø§Ù…Ù‡ Ù†Ø¯Ù‡

    # --- Ø´Ø±Ø· Û²: Ø­Ø¬Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¨Ø³ÛŒØ§Ø± Ø¨Ø§Ù„Ø§ (Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Z-Score) ---
    volume_series = pd.to_numeric(hist_df['volume'], errors='coerce').dropna().tail(20)
    volume_z_score = calculate_z_score(volume_series)
    # Ø¢Ø³ØªØ§Ù†Ù‡ Ø¨Ø§Ù„Ø§ Ø¨Ø±Ø§ÛŒ Ø­Ø¬Ù…ØŒ Ù…Ø«Ù„Ø§ Z-Score Ø¨ÛŒØ´ØªØ± Ø§Ø² 1.8
    is_high_volume = volume_z_score is not None and volume_z_score > 1.8 
    
    # --- Ø´Ø±Ø· Û³: ÙØ´Ø§Ø± Ø®Ø±ÛŒØ¯ Ø³Ù†Ú¯ÛŒÙ† Ø­Ù‚ÛŒÙ‚ÛŒâ€ŒÙ‡Ø§ ---
    is_heavy_buy_pressure = False
    required_cols = ['buy_i_volume', 'buy_i_count', 'sell_i_volume', 'sell_i_count']
    if all(col in last_day and pd.notna(last_day[col]) for col in required_cols):
        if last_day['buy_i_count'] > 0 and last_day['sell_i_count'] > 0:
            per_capita_buy = last_day['buy_i_volume'] / last_day['buy_i_count']
            per_capita_sell = last_day['sell_i_volume'] / last_day['sell_i_count']
            # Ø¢Ø³ØªØ§Ù†Ù‡ Ø¨Ø§Ù„Ø§ Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø§Ù†Ù‡ Ø®Ø±ÛŒØ¯ØŒ Ù…Ø«Ù„Ø§ Û².Ûµ Ø¨Ø±Ø§Ø¨Ø± Ø³Ø±Ø§Ù†Ù‡ ÙØ±ÙˆØ´
            if per_capita_buy > (per_capita_sell * 2.5):
                is_heavy_buy_pressure = True

    # --- ØªØ±Ú©ÛŒØ¨ Ù†Ù‡Ø§ÛŒÛŒ ---
    if is_high_volume and is_heavy_buy_pressure:
        satisfied_filters.append("Power_Thrust_Signal")
        reason_parts["power_thrust"].append(
            f"Power signal detected! High Volume (Z-Score: {volume_z_score:.2f}) and Heavy Buy Pressure."
        )
            
    return satisfied_filters, reason_parts


def _check_candlestick_filters(pattern_rec):
    satisfied_filters, reason_parts = [], {"candlestick": []}
    if not pattern_rec:
        return satisfied_filters, reason_parts

    pattern_name = pattern_rec.pattern_name
    if "Bullish Engulfing" in pattern_name:
        satisfied_filters.append("Bullish_Engulfing_Detected")
        reason_parts["candlestick"].append(f"Detected: {pattern_name}")
    if "Hammer" in pattern_name:
        satisfied_filters.append("Hammer_Detected")
        reason_parts["candlestick"].append(f"Detected: {pattern_name}")
    if "Morning Star" in pattern_name:
        satisfied_filters.append("Morning_Star_Detected")
        reason_parts["candlestick"].append(f"Detected: {pattern_name}")

    return satisfied_filters, reason_parts

# âœ… NEW FUNCTION (Replaces the old _check_advanced_fundamental_filters)
def _check_money_flow_and_advanced_ratios(hist_df: pd.DataFrame, tech_df: pd.DataFrame) -> Tuple[List[str], Dict[str, str]]:
    """
    Ø¨Ø±Ø±Ø³ÛŒ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø¬Ø±ÛŒØ§Ù† Ù¾ÙˆÙ„ (Ù‚Ø¯Ø±Øª Ø®Ø±ÛŒØ¯Ø§Ø± Ø­Ù‚ÛŒÙ‚ÛŒ) Ùˆ Ù†Ø³Ø¨Øªâ€ŒÙ‡Ø§ÛŒ Ø­Ø¬Ù…ÛŒ (Volume Surge) Ú©Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.
    Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† _check_advanced_fundamental_filters Ø´Ø¯Ù‡ Ø§Ø³Øª.
    """
    satisfied_filters = []
    reasons = {"advanced_flow": []}

    if hist_df.empty or tech_df.empty:
        reasons["advanced_flow"].append("Missing historical or technical data for advanced flow analysis.")
        return satisfied_filters, reasons

    # Ø¢Ø®Ø±ÛŒÙ† Ø±Ú©ÙˆØ±Ø¯ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ…
    last_hist = hist_df.iloc[-1]
    last_tech = tech_df.iloc[-1]

    # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ø³ØªØ§Ù†Ù‡
    MIN_REAL_POWER_RATIO = 1.3  # Ø­Ø¯Ø§Ù‚Ù„ Ù†Ø³Ø¨Øª Ù‚Ø¯Ø±Øª Ø®Ø±ÛŒØ¯Ø§Ø± Ø­Ù‚ÛŒÙ‚ÛŒ Ø¨Ù‡ ÙØ±ÙˆØ´Ù†Ø¯Ù‡ Ø­Ù‚ÛŒÙ‚ÛŒ
    MIN_VOLUME_SURGE_RATIO = 1.5 # Ø­Ø¯Ø§Ù‚Ù„ Ù†Ø³Ø¨Øª Ø­Ø¬Ù… Ø±ÙˆØ² Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Û²Û° Ø±ÙˆØ²Ù‡

    # 1. Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‚Ø¯Ø±Øª Ø®Ø±ÛŒØ¯Ø§Ø± Ø­Ù‚ÛŒÙ‚ÛŒ
    required_flow_cols = ['buy_i_volume', 'buy_i_count', 'sell_i_volume', 'sell_i_count']
    real_power_ratio = np.nan

    if all(col in last_hist and pd.notna(last_hist[col]) for col in required_flow_cols):
        buy_i_count = last_hist.get('buy_i_count', 0)
        sell_i_count = last_hist.get('sell_i_count', 0)
        buy_i_volume = last_hist.get('buy_i_volume', 0)
        sell_i_volume = last_hist.get('sell_i_volume', 0)
        
        buy_power_real = buy_i_volume / buy_i_count if buy_i_count > 0 else 0
        sell_power_real = sell_i_volume / sell_i_count if sell_i_count > 0 else 0
        
        # Ø¯Ø± ØµÙˆØ±Øª ØªÙ‚Ø³ÛŒÙ… Ø¨Ø± ØµÙØ±ØŒ Ù†Ø³Ø¨Øª Ø±Ø§ Ø¨ÛŒâ€ŒÙ†Ù‡Ø§ÛŒØª Ø¯Ø± Ù†Ø¸Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ… (Ù…Ø«Ø¨Øª) ÛŒØ§ 1.0 (Ø§Ú¯Ø± Ù‡Ø± Ø¯Ùˆ ØµÙØ± Ø¨Ø§Ø´Ù†Ø¯)
        with np.errstate(divide='ignore', invalid='ignore'):
            real_power_ratio = buy_power_real / sell_power_real if sell_power_real > 0 else (np.inf if buy_power_real > 0 else 1.0)
    
    # 2. Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¬Ù‡Ø´ Ø­Ø¬Ù…
    volume_ma_20 = last_tech.get('Volume_MA_20')
    current_volume = last_hist.get('volume')
    volume_surge_ratio = np.nan
    if volume_ma_20 is not None and volume_ma_20 > 0 and current_volume is not None:
         volume_surge_ratio = current_volume / volume_ma_20

    # ğŸš¨ Ø§Ø¹Ù…Ø§Ù„ ÙÛŒÙ„ØªØ±Ù‡Ø§

    # 1. ÙÛŒÙ„ØªØ± Ù‚Ø¯Ø±Øª Ø®Ø±ÛŒØ¯Ø§Ø±
    if not pd.isna(real_power_ratio) and real_power_ratio >= MIN_REAL_POWER_RATIO:
        satisfied_filters.append("Advanced_Strong_Real_Buyer_Ratio")
        reasons["advanced_flow"].append(f"Real Power Ratio is {real_power_ratio:.2f} (Min: {MIN_REAL_POWER_RATIO}).")

    # 2. ÙÛŒÙ„ØªØ± Ø¬Ù‡Ø´ Ø­Ø¬Ù…
    if not pd.isna(volume_surge_ratio) and volume_surge_ratio >= MIN_VOLUME_SURGE_RATIO:
        satisfied_filters.append("Advanced_Volume_Surge_Ratio")
        reasons["advanced_flow"].append(f"Volume Surge is {volume_surge_ratio:.2f}x 20d MA (Min: {MIN_VOLUME_SURGE_RATIO}x).")
        
    return satisfied_filters, reasons

# --- NEW: Functions for new strategic filters --- 
def _get_leading_sectors():
    """ 
    Ø¨Ø§ Ú©ÙˆØ¦Ø±ÛŒ Ø¨Ù‡ Ø¬Ø¯ÙˆÙ„ DailySectorPerformanceØŒ Ù„ÛŒØ³Øª ØµÙ†Ø§ÛŒØ¹ Ù¾ÛŒØ´Ø±Ùˆ (Ù…Ø«Ù„Ø§Ù‹ 4 ØµÙ†Ø¹Øª Ø¨Ø±ØªØ±) Ø¯Ø± Ø¢Ø®Ø±ÛŒÙ† Ø±ÙˆØ² ØªØ­Ù„ÛŒÙ„ Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
    """
    try:
        latest_date_query = db.session.query(func.max(DailySectorPerformance.jdate)).scalar()
        if not latest_date_query:
            logger.warning("Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¯Ø± Ø¬Ø¯ÙˆÙ„ ØªØ­Ù„ÛŒÙ„ ØµÙ†Ø§ÛŒØ¹ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø§Ø² Ù„ÛŒØ³Øª Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
            return {"Ø®ÙˆØ¯Ø±Ùˆ Ùˆ Ø³Ø§Ø®Øª Ù‚Ø·Ø¹Ø§Øª"} # Fallback

        # Ø¯Ø±ÛŒØ§ÙØª 4 ØµÙ†Ø¹Øª Ø¨Ø±ØªØ± Ø¯Ø± Ø¢Ø®Ø±ÛŒÙ† Ø±ÙˆØ²
        leading_sectors_query = db.session.query(DailySectorPerformance.sector_name)\
            .filter(DailySectorPerformance.jdate == latest_date_query)\
            .order_by(DailySectorPerformance.rank.asc())\
            .limit(4).all()
        leading_sectors = {row[0] for row in leading_sectors_query}
        logger.info(f"ØµÙ†Ø§ÛŒØ¹ Ù¾ÛŒØ´Ø±Ùˆ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡ Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {leading_sectors}")
        return leading_sectors
    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª ØµÙ†Ø§ÛŒØ¹ Ù¾ÛŒØ´Ø±Ùˆ Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {e}")
        return {"Ø®ÙˆØ¯Ø±Ùˆ Ùˆ Ø³Ø§Ø®Øª Ù‚Ø·Ø¹Ø§Øª"} # Fallback in case of error

def _check_sector_strength_filter(symbol_sector, leading_sectors):
    """Checks if the symbol belongs to a leading sector."""
    satisfied_filters, reason_parts = [], {"sector_strength": []}
    if symbol_sector in leading_sectors:
        satisfied_filters.append("IsInLeadingSector")
        reason_parts["sector_strength"].append(f"Symbol is in a leading sector: {symbol_sector}.")
    return satisfied_filters, reason_parts

def _check_static_levels_filters(technical_rec, last_close_val):
    """ 
    Checks for proximity to static support or breakout of static resistance.
    This assumes 'static_support_level' and 'static_resistance_level' fields are pre-calculated and available in the TechnicalIndicatorData record.
    """
    satisfied_filters, reason_parts = [], {"static_levels": []}

    # --- Ø§ØµÙ„Ø§Ø­: Ø±ÙØ¹ Ø®Ø·Ø§ÛŒ Pandas ValueError (Ø®Ø· 526 Ø¯Ø± Ù„Ø§Ú¯ Ø´Ù…Ø§) ---
    # technical_rec Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ None ÛŒØ§ ÛŒÚ© Pandas Series Ø¨Ø§Ø´Ø¯.
    is_rec_valid = technical_rec is not None and ( 
        not isinstance(technical_rec, pd.Series) or not technical_rec.empty 
    )
    # last_close_val Ø¨Ø§ÛŒØ¯ ÛŒÚ© Ù…Ù‚Ø¯Ø§Ø± Ø¹Ø¯Ø¯ÛŒ Ø¨Ø§Ø´Ø¯ØŒ Ù¾Ø³ Ø¨Ø±Ø±Ø³ÛŒ None Ú©Ø§ÙÛŒ Ø§Ø³Øª.
    if not is_rec_valid or last_close_val is None or last_close_val <= 0:
        return satisfied_filters, reason_parts

    # 1. Check for proximity to static support
    # (Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² _get_attr_safe ØªØ¶Ù…ÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ù…Ù‚Ø¯Ø§Ø± Ø¹Ø¯Ø¯ÛŒ Ø§Ø² Series ÛŒØ§ Ø´ÛŒØ¡ ORM Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´ÙˆØ¯)
    support_level = _get_attr_safe(technical_rec, 'static_support_level')

    # Ø§Ú¯Ø± Ø³Ø·Ø­ Ø­Ù…Ø§ÛŒØª Ù…Ø¹ØªØ¨Ø± Ø¨Ø§Ø´Ø¯ Ùˆ Ù‚ÛŒÙ…Øª Ø¨Ù‡ Ø¢Ù† Ù†Ø²Ø¯ÛŒÚ© Ø¨Ø§Ø´Ø¯
    if support_level and support_level > 0:
        # ÙØ§ØµÙ„Ù‡â€ŒÛŒ Ù‚ÛŒÙ…Øª Ø§Ø² Ø³Ø·Ø­ Ø­Ù…Ø§ÛŒØª (Ù…Ø«Ø¨Øª Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ù„Ø§ÛŒ Ø­Ù…Ø§ÛŒØªØŒ Ù…Ù†ÙÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø§ÛŒÛŒÙ†)
        distance = last_close_val - support_level
        # Ù…ÛŒØ²Ø§Ù† Ù†ÙˆØ³Ø§Ù† (Ø¯Ø±ØµØ¯) Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù‚ÛŒÙ…Øª Ø­Ù…Ø§ÛŒØª
        proximity_percent = abs(distance) / support_level 
        # Ø§Ú¯Ø± Ø¯Ø± Ù…Ø­Ø¯ÙˆØ¯Ù‡ Û²Ùª Ø­ÙˆÙ„ Ø³Ø·Ø­ Ø­Ù…Ø§ÛŒØª Ø¨Ø§Ø´Ø¯ (Ø±ÛŒØ³Ú© Ø¨Ù‡ Ø±ÛŒÙˆØ§Ø±Ø¯ Ù…Ù†Ø§Ø³Ø¨)
        if proximity_percent <= 0.02 and distance >= -0.005 * support_level: # Ù†Ø²Ø¯ÛŒÚ© ÛŒØ§ Ú©Ù…ÛŒ Ø¨Ø§Ù„Ø§ØªØ± Ø§Ø² Ø³Ø·Ø­ (Ø­Ø¯Ø§Ú©Ø«Ø± 0.5% Ø²ÛŒØ± Ø³Ø·Ø­)
            satisfied_filters.append("Near_Static_Support")
            reason_parts["static_levels"].append(
                f"Price is near a major static support level at {support_level:,.0f} (Proximity: {proximity_percent*100:.1f}%)."
            )

    # 2. Check for breakout of static resistance
    resistance_level = _get_attr_safe(technical_rec, 'static_resistance_level')

    # Ø§Ú¯Ø± Ø³Ø·Ø­ Ù…Ù‚Ø§ÙˆÙ…Øª Ù…Ø¹ØªØ¨Ø± Ø¨Ø§Ø´Ø¯
    if resistance_level and resistance_level > 0:
        # Ù‚ÛŒÙ…Øª Ø¨Ø³ØªÙ‡ Ø´Ø¯Ù† Ø¨Ø§Ù„Ø§ØªØ± Ø§Ø² Ø³Ø·Ø­ Ù…Ù‚Ø§ÙˆÙ…Øª Ø¨Ø§Ø´Ø¯.
        # Ù†Ú©ØªÙ‡: Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ú†Ú© Ù‚ÙˆÛŒ Ø¨Ø§ÛŒØ¯ Ù‚ÛŒÙ…Øª Ø±ÙˆØ² Ù‚Ø¨Ù„ Ø±Ø§ Ù‡Ù… Ú†Ú© Ú©Ù†ÛŒÙ… Ú©Ù‡ Ø²ÛŒØ± Ù…Ù‚Ø§ÙˆÙ…Øª Ø¨ÙˆØ¯Ù‡ Ø¨Ø§Ø´Ø¯.
        # Ø§Ù…Ø§ Ø¨Ø§ ÙØ±Ø¶ Ø³Ø§Ø¯Ú¯ÛŒ Ùˆ Ù…ÙˆØ¬ÙˆØ¯ Ù†Ø¨ÙˆØ¯Ù† Ù‚ÛŒÙ…Øª Ø±ÙˆØ² Ù‚Ø¨Ù„: 
        if last_close_val > resistance_level and last_close_val < 1.03 * resistance_level: # Ø´Ø±Ø· Ø¯ÙˆÙ…: Ù‚ÛŒÙ…Øª Ø¨ÛŒØ´ Ø§Ø² Ø­Ø¯ Ø§Ø² Ù…Ù‚Ø§ÙˆÙ…Øª Ø¯ÙˆØ± Ù†Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ (Ø´Ú©Ø³Øª ØªØ§Ø²Ù‡ Ùˆ Ù…Ø¹ØªØ¨Ø±)
            satisfied_filters.append("Static_Resistance_Broken")
            reason_parts["static_levels"].append(
                f"Price broke a major static resistance level at {resistance_level:,.0f}."
            )

    return satisfied_filters, reason_parts
# --- END NEW STRATEGIC FILTERS --- 

def run_weekly_watchlist_selection():
    """
    Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø¨Ø±Ø§ÛŒ ÙˆØ§Ú†â€ŒÙ„ÛŒØ³Øª Ù‡ÙØªÚ¯ÛŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø­Ø¬ÛŒÙ…ØŒ
    Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ø¯ÛŒÙ†Ø§Ù…ÛŒÚ© Ùˆ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒÚ©.
    """
    from models import (
        ComprehensiveSymbolData,
        HistoricalData,
        TechnicalIndicatorData,
        FundamentalData,
        CandlestickPatternDetection,
        MLPrediction,
        WeeklyWatchlistResult,
        FinancialRatiosData
    )
    from sqlalchemy import func
    import pandas as pd
    import jdatetime
    import uuid
    from datetime import datetime, timedelta, date
    import json
    import time  # â† Ø§Ø¶Ø§ÙÙ‡: Ø¨Ø±Ø§ÛŒ timing logs

    logger.info("ğŸš€ Starting Weekly Watchlist selection process...")
    start_time = time.time()  # â† Ø§Ø¶Ø§ÙÙ‡: Total timer

    # Step 1: Determine market sentiment and leading sectors
    market_sentiment = _get_market_sentiment()
    leading_sectors = _get_leading_sectors()

    if market_sentiment == "Bullish":
        score_threshold = 7
    elif market_sentiment == "Neutral":
        score_threshold = 8
    else:
        score_threshold = 10

    logger.info(f"ğŸ“ˆ Market sentiment: {market_sentiment}, Score threshold: >= {score_threshold}")
    logger.info(f"ğŸ­ Leading sectors identified: {leading_sectors}")

    # Step 2: Bulk Data Fetching
    allowed_market_types = ['Ø¨ÙˆØ±Ø³', 'ÙØ±Ø§Ø¨ÙˆØ±Ø³', 'Ù¾Ø§ÛŒÙ‡ ÙØ±Ø§Ø¨ÙˆØ±Ø³', 'Ø¨ÙˆØ±Ø³ Ú©Ø§Ù„Ø§', 'Ø¨ÙˆØ±Ø³ Ø§Ù†Ø±Ú˜ÛŒ']

    # Fetch active symbols
    symbols_to_analyze = ComprehensiveSymbolData.query.filter(
        ComprehensiveSymbolData.market_type.in_(allowed_market_types)
    ).all()

    if not symbols_to_analyze:
        logger.warning("âš ï¸ No symbols found for analysis.")
        return False, "No active symbols available for analysis."

    symbol_ids = [s.symbol_id for s in symbols_to_analyze]
    total_symbols = len(symbol_ids)  # â† Ø§Ø¶Ø§ÙÙ‡: Counter Ø¨Ø±Ø§ÛŒ ØªØ¹Ø¯Ø§Ø¯ ÙˆØ§Ø±Ø¯Ø´Ø¯Ù‡
    logger.info(f"ğŸ§© Found {total_symbols} active symbols for analysis.")  # â† Log ØªØ¹Ø¯Ø§Ø¯ entered

    fetch_start = time.time()  # â† Ø§Ø¶Ø§ÙÙ‡: Fetch timer

    # Calculate lookback date
    today_greg = datetime.now().date()
    lookback_greg = today_greg - timedelta(days=TECHNICAL_DATA_LOOKBACK_DAYS * 2)
    lookback_jdate_str = convert_gregorian_to_jalali(lookback_greg)
    if not lookback_jdate_str:
        logger.error("âŒ Could not determine lookback date. Aborting.")
        return False, "Failed to calculate lookback date."

    # Fetch Historical Data
    hist_records = HistoricalData.query.filter(
        HistoricalData.symbol_id.in_(symbol_ids),
        HistoricalData.jdate >= lookback_jdate_str
    ).all()
    hist_df = pd.DataFrame([h.__dict__ for h in hist_records]).drop(columns=['_sa_instance_state'], errors='ignore')

    # Fetch Technical Data
    tech_records = TechnicalIndicatorData.query.filter(
        TechnicalIndicatorData.symbol_id.in_(symbol_ids),
        TechnicalIndicatorData.jdate >= lookback_jdate_str
    ).all()
    tech_df = pd.DataFrame([t.__dict__ for t in tech_records]).drop(columns=['_sa_instance_state'], errors='ignore')

    # Fetch Fundamental Data
    fundamental_records = FundamentalData.query.filter(
        FundamentalData.symbol_id.in_(symbol_ids)
    ).all()

    # Fetch Candlestick Patterns
    today_jdate = get_today_jdate_str()
    candlestick_records = CandlestickPatternDetection.query.filter(
        CandlestickPatternDetection.symbol_id.in_(symbol_ids),
        CandlestickPatternDetection.jdate == today_jdate
    ).all()

    # Fetch ML Predictions
    ml_predictions = MLPrediction.query.filter(
        MLPrediction.symbol_id.in_(symbol_ids),
        MLPrediction.jprediction_date == today_jdate,
        MLPrediction.predicted_trend == 'Uptrend'
    ).all()

    fetch_end = time.time()  # â† Ø§Ø¶Ø§ÙÙ‡: End fetch timer
    logger.info(f"ğŸ“¥ Bulk fetch time: {fetch_end - fetch_start:.2f}s (hist: {len(hist_df)}, tech: {len(tech_df)} rows)")  # â† Log fetch time & rows

    # Grouping Data
    group_start = time.time()  # â† Ø§Ø¶Ø§ÙÙ‡: Group timer
    hist_groups = {k: v.sort_values(by='jdate') for k, v in hist_df.groupby("symbol_id")} if not hist_df.empty else {}
    tech_groups = {k: v.sort_values(by='jdate') for k, v in tech_df.groupby("symbol_id")} if not tech_df.empty else {}
    fundamental_map = {rec.symbol_id: rec for rec in fundamental_records}
    candlestick_map = {rec.symbol_id: rec for rec in candlestick_records}
    ml_prediction_set = {rec.symbol_id for rec in ml_predictions}

    group_end = time.time()  # â† Ø§Ø¶Ø§ÙÙ‡
    logger.info(f"ğŸ“Š Data grouping completed: {group_end - group_start:.2f}s. Groups: hist={len(hist_groups)}, tech={len(tech_groups)}")  # â† Log group time & counts

    logger.info("ğŸ“Š Data grouping completed. Beginning scoring and selection...")

    # Step 3: Scoring
    watchlist_candidates = []
    processed_count = 0  # â† Ø§Ø¶Ø§ÙÙ‡: Counter Ø¨Ø±Ø§ÛŒ processed (full analysis)
    skipped_count = 0    # â† Ø§Ø¶Ø§ÙÙ‡: Counter Ø¨Ø±Ø§ÛŒ skipped
    loop_start = time.time()  # â† Ø§Ø¶Ø§ÙÙ‡: Loop timer
    
    for symbol in symbols_to_analyze:
        symbol_hist_df = hist_groups.get(symbol.symbol_id, pd.DataFrame()).copy()
        symbol_tech_df = tech_groups.get(symbol.symbol_id, pd.DataFrame()).copy()

        if len(symbol_hist_df) < MIN_REQUIRED_HISTORY_DAYS:
            skipped_count += 1  # â† Ø§Ø¶Ø§ÙÙ‡
            logger.debug(f"Skipped {symbol.symbol_name} ({symbol.symbol_id}): insufficient history ({len(symbol_hist_df)} < {MIN_REQUIRED_HISTORY_DAYS})")  # â† Log skip (debug, low spam)
            continue

        last_close_series = _get_close_series_from_hist_df(symbol_hist_df)
        entry_price = float(last_close_series.iloc[-1]) if not last_close_series.empty else None
        if entry_price is None or pd.isna(entry_price):
            skipped_count += 1  # â† Ø§Ø¶Ø§ÙÙ‡
            logger.warning(f"Skipping {symbol.symbol_name} ({symbol.symbol_id}) due to missing entry price.")  # Ù…ÙˆØ¬ÙˆØ¯ + symbol_id Ø¨Ø±Ø§ÛŒ debug
            continue

        all_satisfied_filters = []
        all_reason_parts = {}

        def run_check(check_func, *args):
            filters, reasons = check_func(*args)
            all_satisfied_filters.extend(filters)
            all_reason_parts.update(reasons)

        technical_rec = symbol_tech_df.iloc[-1] if not symbol_tech_df.empty else None
        fundamental_rec = fundamental_map.get(symbol.symbol_id)
        pattern_rec = candlestick_map.get(symbol.symbol_id)

        # Run Filters
        run_check(_check_sector_strength_filter, getattr(symbol, 'sector_name', ''), leading_sectors)
        run_check(_check_technical_filters, symbol_hist_df, symbol_tech_df)
        run_check(_check_market_condition_filters, symbol_hist_df, symbol_tech_df)
        run_check(_check_static_levels_filters, technical_rec, entry_price)
        run_check(_check_fundamental_filters, fundamental_rec)
        run_check(_check_money_flow_and_advanced_ratios, symbol_hist_df, symbol_tech_df)
        run_check(_check_smart_money_filters, symbol_hist_df)
        run_check(_check_power_thrust_signal, symbol_hist_df, last_close_series)
        run_check(_check_candlestick_filters, pattern_rec)

        # ML Filter
        if symbol.symbol_id in ml_prediction_set:
            all_satisfied_filters.append("ML_Predicts_Uptrend")
            all_reason_parts.setdefault("ml_signal", []).append("ML model predicts a high-probability uptrend.")

        # Calculate Weighted Score
        score = sum(FILTER_WEIGHTS.get(f, {}).get('weight', 0) for f in all_satisfied_filters)

        if score >= score_threshold:
            watchlist_candidates.append({
                "symbol_id": symbol.symbol_id,
                "symbol_name": symbol.symbol_name,
                "entry_price": entry_price,
                "entry_date": date.today(),
                "jentry_date": today_jdate,
                "outlook": "Bullish",
                "reason_json": json.dumps(all_reason_parts, ensure_ascii=False),
                "satisfied_filters": json.dumps(list(set(all_satisfied_filters)), ensure_ascii=False),
                "score": score
            })

        processed_count += 1  # â† Ø§Ø¶Ø§ÙÙ‡: Ù‡Ø± symbol Ú©Ù‡ Ø¨Ù‡ filters Ø±Ø³ÛŒØ¯ (Ø­ØªÛŒ Ø§Ú¯Ø± score < threshold) â€“ full analysis count
        
        # Log progress every 100 symbols
        if processed_count % 100 == 0:
            avg_loop_time = (time.time() - loop_start) / processed_count
            logger.info(f"Progress: {processed_count}/{total_symbols} symbols analyzed (avg {avg_loop_time:.3f}s per symbol)")

    loop_end = time.time()  # â† Ø§Ø¶Ø§ÙÙ‡
    logger.info(f"ğŸ”„ Loop completed: Processed {processed_count}/{total_symbols} symbols (skipped {skipped_count}). Loop time: {loop_end - loop_start:.2f}s")  # â† Log counts & time

    logger.info(f"âœ… {len(watchlist_candidates)} symbols passed the threshold ({score_threshold}). Saving top 8...")

    # Step 4: Save results
    watchlist_candidates.sort(key=lambda x: x['score'], reverse=True)
    final_watchlist = watchlist_candidates[:8]

    saved_count = 0
    for candidate in final_watchlist:
        existing_result = WeeklyWatchlistResult.query.filter_by(
            symbol_id=candidate['symbol_id'], jentry_date=candidate['jentry_date']
        ).first()

        if existing_result:
            existing_result.entry_price = candidate['entry_price']
            existing_result.outlook = candidate['outlook']
            existing_result.reason = candidate['satisfied_filters']
            existing_result.probability_percent = min(100, candidate['score'] * 4)
            existing_result.created_at = datetime.now()
        else:
            existing_result = WeeklyWatchlistResult(
                signal_unique_id=str(uuid.uuid4()),
                symbol_id=candidate['symbol_id'],
                symbol_name=candidate['symbol_name'],
                entry_price=candidate['entry_price'],
                entry_date=candidate['entry_date'],
                jentry_date=candidate['jentry_date'],
                outlook=candidate['outlook'],
                reason=candidate['satisfied_filters'],
                probability_percent=min(100, candidate['score'] * 4),
                status='active',
            )
        db.session.add(existing_result)
        saved_count += 1

    try:
        db.session.commit()
        message = f"Weekly Watchlist selection completed. Saved {saved_count} symbols."
        logger.info(message)
        total_time = time.time() - start_time  # â† Ø§Ø¶Ø§ÙÙ‡
        logger.info(f"â±ï¸ Full process time: {total_time:.2f}s")  # â† Log total time
        return True, message
    except Exception as e:
        db.session.rollback()
        logger.error(f"âŒ Database commit failed: {e}", exc_info=True)
        return False, "Database commit failed."



def get_weekly_watchlist_results(jdate_str: Optional[str] = None):
    """
    Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø¢Ø®Ø±ÛŒÙ† Ù†ØªØ§ÛŒØ¬ ÙˆØ§Ú†â€ŒÙ„ÛŒØ³Øª Ù‡ÙØªÚ¯ÛŒ (ÛŒØ§ ØªØ§Ø±ÛŒØ® Ù…Ø´Ø®Øµâ€ŒØ´Ø¯Ù‡ Ø¯Ø± ØµÙˆØ±Øª Ø§Ø±Ø³Ø§Ù„).
    Ø®Ø±ÙˆØ¬ÛŒ Ø´Ø§Ù…Ù„ Ø¬Ø²Ø¦ÛŒØ§Øª Ù†Ù…Ø§Ø¯Ù‡Ø§ Ùˆ Ø¢Ø®Ø±ÛŒÙ† ØªØ§Ø±ÛŒØ® Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø§Ø³Øª.
    """

    logger.info("ğŸ“Š Retrieving latest weekly watchlist results...")

    # --- Step 1: ØªØ¹ÛŒÛŒÙ† Ø¢Ø®Ø±ÛŒÙ† ØªØ§Ø±ÛŒØ® ---
    if not jdate_str:
        latest_record = WeeklyWatchlistResult.query.order_by(WeeklyWatchlistResult.jentry_date.desc()).first()
        if not latest_record or not latest_record.jentry_date:
            logger.warning("âš ï¸ No weekly watchlist results found.")
            return {"top_watchlist_stocks": [], "last_updated": "Ù†Ø§Ù…Ø´Ø®Øµ"}
        jdate_str = latest_record.jentry_date

    logger.info(f"ğŸ—“ Latest Weekly Watchlist results date: {jdate_str}")

    # --- Step 2: ÙˆØ§Ú©Ø´ÛŒ Ù†ØªØ§ÛŒØ¬ ---
    results = WeeklyWatchlistResult.query.filter_by(jentry_date=jdate_str)\
                                         .order_by(WeeklyWatchlistResult.created_at.desc()).all()

    if not results:
        logger.warning("âš ï¸ No results found for this date.")
        return {"top_watchlist_stocks": [], "last_updated": jdate_str}

    # --- Step 3: ÙˆØ§Ú©Ø´ÛŒ Ù†Ø§Ù… Ø´Ø±Ú©Øªâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ ÙˆØ§Ú†â€ŒÙ„ÛŒØ³Øª ---
    symbol_ids_in_watchlist = [r.symbol_id for r in results]
    company_name_map = {}

    if symbol_ids_in_watchlist:
        company_name_records = ComprehensiveSymbolData.query.filter(
            ComprehensiveSymbolData.symbol_id.in_(symbol_ids_in_watchlist)
        ).with_entities(
            ComprehensiveSymbolData.symbol_id,
            ComprehensiveSymbolData.company_name
        ).all()

        company_name_map = {sid: cname for sid, cname in company_name_records}

    # --- Step 4: Ø³Ø§Ø®Øª Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù‡Ø§ÛŒÛŒ ---
    output_stocks = []
    for r in results:
        output_stocks.append({
            "signal_unique_id": r.signal_unique_id,
            "symbol_id": r.symbol_id,
            "symbol_name": r.symbol_name,
            "company_name": company_name_map.get(r.symbol_id, r.symbol_name),
            "outlook": r.outlook,
            "reason": r.reason,
            "entry_price": r.entry_price,
            "jentry_date": r.jentry_date,
            "exit_price": r.exit_price,
            "jexit_date": r.jexit_date,
            "profit_loss_percentage": r.profit_loss_percentage,
            "status": r.status,
            "probability_percent": getattr(r, "probability_percent", None),
            "score": getattr(r, "score", None)
        })

    # --- Step 5: Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù…ØªÛŒØ§Ø² ---
    output_stocks.sort(key=lambda x: (x.get("score") or 0), reverse=True)

    logger.info(f"âœ… Retrieved and enriched {len(output_stocks)} weekly watchlist results for {jdate_str}.")

    # --- Step 6: Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ ---
    return {
        "top_watchlist_stocks": output_stocks,
        "last_updated": jdate_str
    }
