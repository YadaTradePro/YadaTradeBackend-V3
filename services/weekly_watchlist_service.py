# -*- coding: utf-8 -*-
# services/weekly_watchlist_service.py

# --- ğŸ’¡ G-CleanUp: ØªÙ…Ø§Ù… Ø§ÛŒÙ…Ù¾ÙˆØ±Øªâ€ŒÙ‡Ø§ Ø¨Ù‡ Ø³Ø·Ø­ Ø¨Ø§Ù„Ø§ Ù…Ù†ØªÙ‚Ù„ Ø´Ø¯Ù†Ø¯ ---
from extensions import db
from models import (
    HistoricalData, ComprehensiveSymbolData, TechnicalIndicatorData, 
    WeeklyWatchlistResult, SignalsPerformance, AggregatedPerformance, 
    GoldenKeyResult, MLPrediction, DailyIndexData, DailySectorPerformance
)
from flask import current_app
import pandas as pd
from datetime import datetime, timedelta, date
import jdatetime
import uuid
from sqlalchemy import func, text
import logging
import json
import numpy as np
from types import SimpleNamespace
from typing import List, Dict, Tuple, Optional
import time
from sqlalchemy import desc

# ğŸ’¡ G-Performance: Ø§ÛŒÙ…Ù¾ÙˆØ±Øª Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆØ§Ø²ÛŒ
from joblib import Parallel, delayed

# ğŸ’¡ G-Sentiment: Ø§ÛŒÙ…Ù¾ÙˆØ±Øª Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø³Ù†ØªÛŒÙ…Ù†Øª
from services.index_data_fetcher import get_market_indices

# ğŸ’¡ G-CleanUp: 
Ø§ÛŒÙ…Ù¾ÙˆØ±Øª Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù„Ø§Ø²Ù…
from services.technical_analysis_utils import (
    get_today_jdate_str, normalize_value, calculate_rsi, 
    calculate_macd, calculate_sma, calculate_bollinger_bands, 
    calculate_volume_ma, calculate_atr, calculate_smart_money_flow, 
    convert_gregorian_to_jalali, calculate_z_score
)

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯ÛŒÙ†Ú¯ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù…Ø§Ú˜ÙˆÙ„
logger = logging.getLogger(__name__)

# IMPROVEMENT: Lookback period and minimum history days 
# adjusted for better indicator quality
TECHNICAL_DATA_LOOKBACK_DAYS = 120
MIN_REQUIRED_HISTORY_DAYS = 50

## ğŸ’¡ G-All: Ø¨Ø§Ø²Ø¨ÛŒÙ†ÛŒ Ú©Ø§Ù…Ù„ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ ÙÛŒÙ„ØªØ±Ù‡Ø§
# REVISED: New filter weights dictionary with descriptions for clarity
FILTER_WEIGHTS = {
    # --- High-Impact Leading & Breakout Signals ---
    "Power_Thrust_Signal": {
        "weight": 5,
     "description": "Ø³ÛŒÚ¯Ù†Ø§Ù„ Ù‚Ø¯Ø±Øª: ØªØ±Ú©ÛŒØ¨ÛŒ Ø§Ø² Ø­Ø¬Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù†ÙØ¬Ø§Ø±ÛŒØŒ ÙˆØ±ÙˆØ¯ Ù¾ÙˆÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø³Ù†Ú¯ÛŒÙ† Ø¯Ø± ÛŒÚ© Ø±ÙˆØ² Ù…Ø«Ø¨Øª (Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ† ØµÙ Ø®Ø±ÛŒØ¯/ÙØ±ÙˆØ´)."
},
    "RSI_Positive_Divergence": {
        "weight": 5,
        "description": "ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ù…Ø«Ø¨Øª Ø¯Ø± RSIØŒ ÛŒÚ© Ø³ÛŒÚ¯Ù†Ø§Ù„ Ù¾ÛŒØ´Ø±Ùˆ Ù‚ÙˆÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø­ØªÙ…Ø§Ù„ Ø¨Ø§Ø²Ú¯Ø´Øª Ø±ÙˆÙ†Ø¯ Ù†Ø²ÙˆÙ„ÛŒ Ø¨Ù‡ ØµØ¹ÙˆØ¯ÛŒ."
},
    "Resistance_Broken": {
        "weight": 5,
        "description": "Ø´Ú©Ø³Øª ÛŒÚ© Ø³Ø·Ø­ Ù…Ù‚Ø§ÙˆÙ…Øª Ú©Ù„ÛŒØ¯ÛŒØŒ Ù†Ø´Ø§Ù†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ù‚Ø¯Ø±Øª Ø®Ø±ÛŒØ¯Ø§Ø±Ø§Ù† Ùˆ Ù¾ØªØ§Ù†Ø³ÛŒÙ„ Ø´Ø±ÙˆØ¹ ÛŒÚ© Ø­Ø±Ú©Øª ØµØ¹ÙˆØ¯ÛŒ Ø¬Ø¯ÛŒØ¯."
},
    "Static_Resistance_Broken": {
        "weight": 5,
        "description": "Ø´Ú©Ø³Øª ÛŒÚ© Ù…Ù‚Ø§ÙˆÙ…Øª Ù…Ù‡Ù… Ø§Ø³ØªØ§ØªÛŒÚ© (Ú©Ù„Ø§Ø³ÛŒÚ©)ØŒ Ø³ÛŒÚ¯Ù†Ø§Ù„ÛŒ Ø¨Ø³ÛŒØ§Ø± Ù…Ø¹ØªØ¨Ø± Ø¨Ø±Ø§ÛŒ Ø§Ø¯Ø§Ù…Ù‡ Ø±Ø´Ø¯."
},
    "Squeeze_Momentum_Fired_Long": {
        "weight": 4,
        "description": "Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± Squeeze Momentum Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø®Ø±ÛŒØ¯ (Ø®Ø±ÙˆØ¬ Ø§Ø² ÙØ´Ø±Ø¯Ú¯ÛŒ) ØµØ§Ø¯Ø± Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª Ú©Ù‡ Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡ Ø§Ø­ØªÙ…Ø§Ù„ ÛŒÚ© Ø­Ø±Ú©Øª Ø§Ù†ÙØ¬Ø§Ø±ÛŒ Ø§Ø³Øª."
},
    "Stochastic_Bullish_Cross_Oversold": {
        "weight": 4,
        "description": "ØªÙ‚Ø§Ø·Ø¹ ØµØ¹ÙˆØ¯ÛŒ Ø¯Ø± Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± Ø§Ø³ØªÙˆÚ©Ø§Ø³ØªÛŒÚ© Ø¯Ø± Ù†Ø§Ø­ÛŒÙ‡ Ø§Ø´Ø¨Ø§Ø¹ ÙØ±ÙˆØ´ØŒ ÛŒÚ© Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø®Ø±ÛŒØ¯ Ú©Ù„Ø§Ø³ÛŒÚ© Ùˆ Ù…Ø¹ØªØ¨Ø±."
},
    "Consolidation_Breakout_Candidate": {
        "weight": 3,
        "description": "Ø³Ù‡Ù… Ø¯Ø± ÙØ§Ø² ØªØ±Ø§Ú©Ù… Ùˆ Ù†ÙˆØ³Ø§Ù† Ú©Ù… Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡ ÛŒÚ© Ø­Ø±Ú©Øª Ù‚ÙˆÛŒ (Ø´Ú©Ø³Øª) Ø§Ø³Øª."
},
    "Near_Static_Support": {
        "weight": 3,
        "description": "Ù‚ÛŒÙ…Øª Ø¯Ø± Ù†Ø²Ø¯ÛŒÚ©ÛŒ ÛŒÚ© Ø³Ø·Ø­ Ø­Ù…Ø§ÛŒØªÛŒ Ø§Ø³ØªØ§ØªÛŒÚ© Ù…Ø¹ØªØ¨Ø± Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯ Ú©Ù‡ Ø±ÛŒØ³Ú© Ø¨Ù‡ Ø±ÛŒÙˆØ§Ø±Ø¯ Ù…Ù†Ø§Ø³Ø¨ÛŒ Ø¨Ø±Ø§ÛŒ ÙˆØ±ÙˆØ¯ ÙØ±Ø§Ù‡Ù… Ù…ÛŒâ€ŒÚ©Ù†Ø¯."
},
    "Bollinger_Lower_Band_Touch": {
        "weight": 1,
        "description": "Ù‚ÛŒÙ…Øª Ø¨Ù‡ Ø¨Ø§Ù†Ø¯ Ù¾Ø§ÛŒÛŒÙ† Ø¨ÙˆÙ„ÛŒÙ†Ú¯Ø± Ø¨Ø±Ø®ÙˆØ±Ø¯ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª Ú©Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ù†Ø´Ø§Ù†Ù‡ Ø¨Ø§Ø²Ú¯Ø´Øª Ú©ÙˆØªØ§Ù‡â€ŒÙ…Ø¯Øª Ù‚ÛŒÙ…Øª Ø¨Ø§Ø´Ø¯."
},

    # --- Trend Confirmation & Strength Signals ---
    "IsInLeadingSector": {
        "weight": 4,
        "description": "Ø³Ù‡Ù… Ù…ØªØ¹Ù„Ù‚ Ø¨Ù‡ ÛŒÚ©ÛŒ Ø§Ø² ØµÙ†Ø§ÛŒØ¹ Ù¾ÛŒØ´Ø±Ùˆ Ùˆ Ù…ÙˆØ±Ø¯ ØªÙˆØ¬Ù‡ Ø¨Ø§Ø²Ø§Ø± Ø§Ø³Øª Ú©Ù‡ Ø´Ø§Ù†Ø³ Ù…ÙˆÙÙ‚ÛŒØª Ø±Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯."
},
    "Strong_Uptrend_Confirmed": {
        "weight": 3,
        "description": "ØªØ§ÛŒÛŒØ¯ Ø±ÙˆÙ†Ø¯ ØµØ¹ÙˆØ¯ÛŒ Ù‚ÙˆÛŒ: SMA_20 > SMA_50 Ùˆ Ù‚ÛŒÙ…Øª Ø¨Ø§Ù„Ø§ØªØ± Ø§Ø² SMA_20 Ø§Ø³Øª."
},
    "Buy_The_Dip_SMA50": {
        "weight": 3,
        "description": "Ù¾Ø§Ø¯Ø§Ø´ Ù¾ÙˆÙ„Ø¨Ú©: Ù‚ÛŒÙ…Øª Ø¯Ø± ÛŒÚ© Ø±ÙˆÙ†Ø¯ ØµØ¹ÙˆØ¯ÛŒØŒ Ø¨Ù‡ SMA_50 Ù¾ÙˆÙ„Ø¨Ú© Ø²Ø¯Ù‡ (Ø¯Ø± Ù…Ø­Ø¯ÙˆØ¯Ù‡ 0% ØªØ§ 3% Ø¨Ø§Ù„Ø§ÛŒ Ø¢Ù†)."
},
    "Positive_Real_Money_Flow_Trend_10D": {
        "weight": 3,
        "description": "Ø¨Ø±Ø¢ÛŒÙ†Ø¯ ÙˆØ±ÙˆØ¯ Ù¾ÙˆÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯ (Ø­Ù‚ÛŒÙ‚ÛŒ) Ø¯Ø± Û±Û° Ø±ÙˆØ² Ú¯Ø°Ø´ØªÙ‡ Ù…Ø«Ø¨Øª Ø¨ÙˆØ¯Ù‡ Ø§Ø³Øª."
},
    "Heavy_Individual_Buy_Pressure": {
        "weight": 3,
        "description": "Ø³Ø±Ø§Ù†Ù‡ Ø®Ø±ÛŒØ¯ Ø­Ù‚ÛŒÙ‚ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± Ø±ÙˆØ² Ø¢Ø®Ø± Ø¨Ù‡ Ø·ÙˆØ± Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ÛŒ Ø¨Ø§Ù„Ø§ØªØ± Ø§Ø² Ø³Ø±Ø§Ù†Ù‡ ÙØ±ÙˆØ´ Ø¨ÙˆØ¯Ù‡ Ø§Ø³Øª (ÙˆØ±ÙˆØ¯ Ù¾ÙˆÙ„ Ø³Ù†Ú¯ÛŒÙ†)."
},
    "MACD_Bullish_Cross_Confirmed": {
        "weight": 2,
        "description": "Ø®Ø· MACD Ø®Ø· Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù‡ Ø³Ù…Øª Ø¨Ø§Ù„Ø§ Ù‚Ø·Ø¹ Ú©Ø±Ø¯Ù‡ Ú©Ù‡ ØªØ§ÛŒÛŒØ¯ÛŒ Ø¨Ø± Ø´Ø±ÙˆØ¹ Ø±ÙˆÙ†Ø¯ ØµØ¹ÙˆØ¯ÛŒ Ø§Ø³Øª."
},
    "HalfTrend_Buy_Signal": {
        "weight": 2,
        "description": "Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± HalfTrend Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø®Ø±ÛŒØ¯ ØµØ§Ø¯Ø± Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª."
},
    "High_Volume_On_Up_Day": {
        "weight": 2,
        "description": "Ø­Ø¬Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¯Ø± ÛŒÚ© Ø±ÙˆØ² Ù…Ø«Ø¨Øª Ø¨Ù‡ Ø·ÙˆØ± Ù…Ø¹Ù†Ø§Ø¯Ø§Ø±ÛŒ Ø§ÙØ²Ø§ÛŒØ´ ÛŒØ§ÙØªÙ‡ Ú©Ù‡ Ù†Ø´Ø§Ù† Ø§Ø² Ø­Ù…Ø§ÛŒØª Ù‚ÙˆÛŒ Ø§Ø² Ø±Ø´Ø¯ Ù‚ÛŒÙ…Øª Ø¯Ø§Ø±Ø¯ (Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ† ØµÙ)."
},
    "Volume_MA_Is_Rising": { # ğŸ’¡ G-Volume: Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡
        "weight": 2,
        "description": "Ø±ÙˆÙ†Ø¯ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø­Ø¬Ù…: Ø´ÛŒØ¨ (Slope) Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø­Ø¬Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª (20 Ø±ÙˆØ²Ù‡) ØµØ¹ÙˆØ¯ÛŒ Ø§Ø³Øª."
},

    # --- ğŸ’¡ G-Fundamental: Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† ÙÛŒÙ„ØªØ± Ø³Ø§Ø¯Ù‡ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„ ---
    "Reasonable_PE": {
        "weight": 1, 
        "description": "Ù†Ø³Ø¨Øª P/E Ø³Ù‡Ù… Ú©Ù…ØªØ± Ø§Ø² 15 Ø§Ø³Øª Ùˆ Ø³Ù‡Ù… Ø­Ø¨Ø§Ø¨ Ù‚ÛŒÙ…ØªÛŒ Ù†Ø¯Ø§Ø±Ø¯."
},
    
    # --- ML Prediction Filter ---
    "ML_Predicts_Uptrend": {
        "weight": 2,
        "description": "Ù…Ø¯Ù„ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†ØŒ Ø§Ø­ØªÙ…Ø§Ù„ Ø¨Ø§Ù„Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ø±ÙˆÙ†Ø¯ ØµØ¹ÙˆØ¯ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª."
},

    # --- Penalties & Negative Scores (Crucial for avoiding peaks) ---
    "Price_Below_SMA200": { # ğŸ’¡ G-Trend: ÙÛŒÙ„ØªØ± Ø¬Ø¯ÛŒØ¯ Ø±ÙˆÙ†Ø¯ Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª
        "weight": -5,
        "description": "Ø¬Ø±ÛŒÙ…Ù‡ Ø±ÙˆÙ†Ø¯ Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª: Ù‚ÛŒÙ…Øª Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ± Ø§Ø² Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú© Û²Û°Û° Ø±ÙˆØ²Ù‡ Ø§Ø³Øª."
},
    "Strong_Downtrend_Confirmed": {
        "weight": -4,
        "description": "Ø¬Ø±ÛŒÙ…Ù‡ Ø±ÙˆÙ†Ø¯ Ù†Ø²ÙˆÙ„ÛŒ: SMA_20 Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ± Ø§Ø² SMA_50 Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯."
},
    "MACD_Negative_Divergence": {
        "weight": -4,
        "description": "Ø¬Ø±ÛŒÙ…Ù‡ ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ù…Ù†ÙÛŒ: Ù‚ÛŒÙ…Øª Ø³Ù‚Ù Ø¬Ø¯ÛŒØ¯ Ø²Ø¯Ù‡ Ø§Ù…Ø§ MACD Ø³Ù‚Ù Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ±ÛŒ Ø«Ø¨Øª Ú©Ø±Ø¯Ù‡ (Ù†Ø´Ø§Ù†Ù‡ Ø¶Ø¹Ù Ø´Ø¯ÛŒØ¯ Ø±ÙˆÙ†Ø¯)."
},
    "RSI_Is_Overbought": {
        "weight": -4,
        "description": "Ø¬Ø±ÛŒÙ…Ù‡ Ø§Ø´Ø¨Ø§Ø¹ Ø®Ø±ÛŒØ¯: RSI Ø¯Ø± Ù†Ø§Ø­ÛŒÙ‡ Ø§Ø´Ø¨Ø§Ø¹ Ø®Ø±ÛŒØ¯ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯ Ú©Ù‡ Ø±ÛŒØ³Ú© Ø§ØµÙ„Ø§Ø­ Ù‚ÛŒÙ…Øª Ø±Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯."
},
    "Price_Too_Stretched_From_SMA50": {
        "weight": -3,
        "description": "Ø¬Ø±ÛŒÙ…Ù‡ ÙØ§ØµÙ„Ù‡ Ø²ÛŒØ§Ø¯: Ù‚ÛŒÙ…Øª ÙØ§ØµÙ„Ù‡ Ø²ÛŒØ§Ø¯ÛŒ Ø§Ø² Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú© ÛµÛ° Ø±ÙˆØ²Ù‡ Ú¯Ø±ÙØªÙ‡ Ú©Ù‡ Ø§Ø­ØªÙ…Ø§Ù„ Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø±Ø§ Ø¨Ø§Ù„Ø§ Ù…ÛŒâ€ŒØ¨Ø±Ø¯."
},
    "Negative_Real_Money_Flow_Trend_10D": {
        "weight": -2,
        "description": "Ø¬Ø±ÛŒÙ…Ù‡ Ø®Ø±ÙˆØ¬ Ù¾ÙˆÙ„: Ø¨Ø±Ø¢ÛŒÙ†Ø¯ ÙˆØ±ÙˆØ¯ Ù¾ÙˆÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¯Ø± Û±Û° Ø±ÙˆØ² Ú¯Ø°Ø´ØªÙ‡ Ù…Ù†ÙÛŒ Ø¨ÙˆØ¯Ù‡ Ø§Ø³Øª (Ø®Ø±ÙˆØ¬ Ù¾ÙˆÙ„)."
},
    "Signal_Against_Market_Trend": {
        "weight": -2,
        "description": "Ø¬Ø±ÛŒÙ…Ù‡ Ø®Ù„Ø§Ù Ø¬Ù‡Øª Ø¨Ø§Ø²Ø§Ø±: Ø³ÛŒÚ¯Ù†Ø§Ù„ ØµØ¹ÙˆØ¯ÛŒ (Ù…Ø«Ù„ MACD Cross) Ø¯Ø± ÛŒÚ© Ø¨Ø§Ø²Ø§Ø± Ø®Ø±Ø³ÛŒ ØµØ§Ø¯Ø± Ø´Ø¯Ù‡ Ø§Ø³Øª."
}
}


# Ú©Ù…Ú©â€ŒÚ©Ù†Ù†Ø¯Ù‡: Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† ÛŒÚ© Ø³Ø±ÛŒ close Ù‚Ø§Ø¨Ù„â€ŒØ§Ø¹ØªÙ…Ø§Ø¯ Ø§Ø² historical DF
def _get_close_series_from_hist_df(hist_df):
    """
    Accepts a historical dataframe and returns a numeric pandas Series of close prices.
    Tries common column names: 'close_price', 'close', 'final'
    """
    if hist_df is None or hist_df.empty:
        return pd.Series(dtype=float)

    for col in ['close_price', 'close', 'final']:
        if col in hist_df.columns:
            ser = pd.to_numeric(hist_df[col], errors='coerce').dropna()
        if not ser.empty:
                return ser
    return pd.Series(dtype=float)



# --- NEW: Market Sentiment Analysis Function ---
def _get_market_sentiment() -> str:
    """
    Determines short-term market sentiment based on the average daily change of major indices over the last 3 trading days.
    Uses data from DailyIndexData table.
    Returns: 'Bullish', 'Neutral', or 'Bearish'.
    """
    try:
    # Get last 3 distinct jdates
        last_jdates_query = db.session.query(DailyIndexData.jdate).distinct().order_by(DailyIndexData.jdate.desc()).limit(3).all()
        if not last_jdates_query:
            logger.warning("No index data available, defaulting to Neutral")
            return "Neutral"

        last_jdates = [row[0] for row in last_jdates_query]

        # ğŸ’¡ G-Sentiment: Ø§ÙØ²ÙˆØ¯Ù† 'TEDPIX' Ø¨Ù‡ Ù„ÛŒØ³Øª Ø¨Ø±Ø±Ø³ÛŒ
        index_types_to_check = ['Total_Index', 'Equal_Weighted_Index', 'TEDPIX']
      index_data = db.session.query(DailyIndexData).filter(
            DailyIndexData.jdate.in_(last_jdates),
            DailyIndexData.index_type.in_(index_types_to_check)
        ).all()

        if len(index_data) < 2:  # At least one per index
            logger.warning(f"Insufficient index data for types {index_types_to_check}, defaulting to Neutral")
            return "Neutral"

        # 
Group by index_type
        # ğŸ’¡ G-Sentiment: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² 'Total_Index' ÛŒØ§ 'TEDPIX' Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø´Ø§Ø®Øµ Ú©Ù„
        total_changes = [d.percent_change for d in index_data if d.index_type == 'Total_Index' or d.index_type == 'TEDPIX']
        equal_changes = [d.percent_change for d in index_data if d.index_type == 'Equal_Weighted_Index']

        if len(total_changes) < 1 or len(equal_changes) < 1:
            logger.warning("Missing data for one index type (Total or Equal), defaulting to Neutral")
           return "Neutral"

        avg_total = np.mean(total_changes)
        avg_equal = np.mean(equal_changes)

        logger.info(f"Market Sentiment Averages over last {len(last_jdates)} days: Total {avg_total:.2f}%, Equal {avg_equal:.2f}%")

        if avg_total > 0.3 and avg_equal > 0.3:
            logger.info(f"Market Sentiment: Bullish (Total: {avg_total:.2f}%, Equal: {avg_equal:.2f}%)")
            return "Bullish"
    elif avg_total < -0.3 and avg_equal < -0.3:
            logger.info(f"Market Sentiment: Bearish (Total: {avg_total:.2f}%, Equal: {avg_equal:.2f}%)")
            # ğŸ’¡ FIX: Indent Error (Ø®Ø· Û±Û¶Û¸)
            return "Bearish"
        else:
            logger.info(f"Market Sentiment: Neutral (Total: {avg_total:.2f}%, Equal: {avg_equal:.2f}%)")
            return "Neutral"
   except Exception as e:
        logger.error(f"Error fetching market sentiment from DB: {e}, defaulting to Neutral")
        return "Neutral"


# --- REVISED: Filter Functions ---
def _check_market_condition_filters(hist_df, tech_df):
    """
    Checks for individual stock conditions like overbought state 
or consolidation.
"""
    satisfied_filters, reason_parts = [], {"market_condition": []}
    if not is_data_sufficient(tech_df, 1) or not is_data_sufficient(hist_df, MIN_REQUIRED_HISTORY_DAYS):
        return satisfied_filters, reason_parts

    last_tech = tech_df.iloc[-1]
    close_ser = _get_close_series_from_hist_df(hist_df)
    if close_ser.empty:
        return satisfied_filters, reason_parts
    last_close = close_ser.iloc[-1]
    last_hist = hist_df.iloc[-1]

    # --- Check 1: RSI Overbought Condition (Penalize only if weak) ---
    if hasattr(last_tech, 'RSI') and last_tech.RSI is not None and last_tech.RSI > 70:
       is_negative_divergence = (
            hasattr(last_tech, 'RSI_Divergence') and last_tech.RSI_Divergence == "Negative"
        )
        historical_volume_series = hist_df.tail(10)['volume']
        average_volume = historical_volume_series.mean() if not historical_volume_series.empty else 0
        is_high_volume = last_hist['volume'] > average_volume * 1.5 if average_volume > 0 else False

        if is_negative_divergence or not is_high_volume:
           satisfied_filters.append("RSI_Is_Overbought")
            reason_parts["market_condition"].append(
                f"RSI ({last_tech.RSI:.2f}) overbought with weakness."
            )
        else:
            reason_parts["market_condition"].append(
                f"RSI ({last_tech.RSI:.2f}) overbought but supported by strong volume (no penalty)."
    )

    # --- Check 2 & 3: Price Stretch (Penalize) OR Buy The Dip (Reward) ---
    sma50 = _get_attr_safe(last_tech, 'SMA_50')
    sma20 = _get_attr_safe(last_tech, 'SMA_20')
    
    if sma50 is not None and sma50 > 0 and sma20 is not None:
        stretch_percent = ((last_close - sma50) / sma50) * 100
        is_uptrend = sma20 > sma50

        # ğŸ’¡ G-2: Ø¬Ø±ÛŒÙ…Ù‡ ÙØ§ØµÙ„Ù‡ Ø²ÛŒØ§Ø¯ (Ø´Ø±Ø· 
        if stretch_percent > 20:
            satisfied_filters.append("Price_Too_Stretched_From_SMA50")
            reason_parts["market_condition"].append(
                f"Price is overextended ({stretch_percent:.1f}%) from SMA50."
            )
        
        
        # ğŸ’¡ G-2: Ù¾Ø§Ø¯Ø§Ø´ Ù¾ÙˆÙ„Ø¨Ú© (Ø´Ø±Ø· Ø¬Ø¯ÛŒØ¯)
      elif is_uptrend and 0 <= stretch_percent <= 3:
            satisfied_filters.append("Buy_The_Dip_SMA50")
            reason_parts["market_condition"].append(
                f"Pullback signal: Price is near SMA50 ({stretch_percent:.1f}%) in an uptrend."
            )

    # --- Check 4: Consolidation Pattern (Reward) ---
    if hasattr(last_tech, 'ATR'):
        atr_series = pd.to_numeric(tech_df['ATR'].dropna())

        if len(atr_series) > 30:
            recent_atr_avg = atr_series.tail(10).mean()
            historical_atr_avg = atr_series.tail(30).mean()
            if recent_atr_avg < (historical_atr_avg * 0.7):
                satisfied_filters.append("Consolidation_Breakout_Candidate")
                reason_parts["market_condition"].append(
                "Stock is in a low-volatility consolidation phase."
                )

    return satisfied_filters, reason_parts

# ØªØºÛŒÛŒØ± Ø¯Ø± is_data_sufficient: Ø§Ù†Ø¹Ø·Ø§Ùâ€ŒÙ¾Ø°ÛŒØ±ØªØ± Ùˆ Ù…Ù‚Ø§ÙˆÙ…â€ŒØªØ±
def is_data_sufficient(data_df, min_len):
    """
    Checks if the provided DataFrame is not empty and has at least min_len records.
"""
    if data_df is None or data_df.empty:
        return False
    return len(data_df) >= min_len

def convert_jalali_to_gregorian_timestamp(jdate_str):
    """
    Converts a Jalali date string (YYYY-MM-DD) to a pandas Timestamp (Gregorian).
"""
    if pd.notna(jdate_str) and isinstance(jdate_str, str):
        try:
            jy, jm, jd = map(int, jdate_str.split('-'))
            gregorian_date = jdatetime.date(jy, jm, jd).togregorian()
            return pd.Timestamp(gregorian_date)
        except ValueError:
            return pd.NaT
    return pd.NaT

# Helper function to safely get a value
def _get_attr_safe(rec, attr, default=None):
   val = getattr(rec, attr, default)
    if isinstance(val, (pd.Series, pd.DataFrame)):
        # ğŸ’¡ FIX: (Bug) Ø¨Ø§ÛŒØ¯ Ø¢Ø®Ø±ÛŒÙ† Ù…Ù‚Ø¯Ø§Ø± (iloc[-1]) Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù‡ Ø´ÙˆØ¯ØŒ Ù†Ù‡ Ø§ÙˆÙ„ÛŒÙ† (iloc[0])
        return val.iloc[-1] if not val.empty else default
    return val

# --- REFACTORED: Technical filters are broken down into smaller functions ---

# ğŸ’¡ G-Fix: ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ ØµØ­ÛŒØ­ ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ
def _find_divergence(price_series: pd.Series, indicator_series: pd.Series, lookback: int, check_type: str) -> bool:
    """
    ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ ÛŒØ§ÙØªÙ† ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ø³Ø§Ø¯Ù‡ (Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ù…Ù†Ø·Ù‚ Ø§Ø´ØªØ¨Ø§Ù‡ Ù‚Ø¨Ù„ÛŒ)
    check_type: 'positive_rsi' (RD+) ÛŒØ§ 'negative' (RD-)
    """
    if len(price_series) < lookback or len(indicator_series) < lookback:
        return False
        
    price_series = pd.to_numeric(price_series.tail(lookback), errors='coerce').dropna()
    indicator_series = pd.to_numeric(indicator_series.tail(lookback), errors='coerce').dropna()
    
    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ù‡Ù…â€ŒØ±Ø§Ø³ØªØ§ÛŒÛŒ Ø§ÛŒÙ†Ø¯Ú©Ø³â€ŒÙ‡Ø§ Ù¾Ø³ Ø§Ø² Ø­Ø°Ù NA
    common_index = price_series.index.intersection(indicator_series.index)
    if len(common_index) < lookback:
         return False # Ø¯Ø§Ø¯Ù‡ Ù…Ø´ØªØ±Ú© Ú©Ø§ÙÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯
         
    price_series = price_series.loc[common_index]
    indicator_series = indicator_series.loc[common_index]

    if price_series.empty or indicator_series.empty or len(price_series) < 2:
        return False

    last_price = price_series.iloc[-1]
    last_indicator = indicator_series.iloc[-1]
    
    lookback_prices = price_series.iloc[:-1]
    lookback_indicators = indicator_series.iloc[:-1]

    if lookback_prices.empty:
        return False

    try:
        if check_type == 'positive_rsi':
            # RD+: Ù‚ÛŒÙ…Øª Ú©Ù Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ± Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯ØŒ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± Ú©Ù Ø¨Ø§Ù„Ø§ØªØ± Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯.
            min_price_idx = lookback_prices.idxmin()
            min_price_val = lookback_prices.loc[min_price_idx]
            indicator_at_min_price = lookback_indicators.loc[min_price_idx]

            if pd.isna(min_price_val) or pd.isna(indicator_at_min_price) or pd.isna(last_price) or pd.isna(last_indicator):
                return False

            # Û±. Ù‚ÛŒÙ…Øª Ú©Ù Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ± Ø²Ø¯Ù‡
            price_makes_lower_low = last_price < min_price_val
            # Û². Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± Ú©Ù Ø¨Ø§Ù„Ø§ØªØ± Ø²Ø¯Ù‡
            indicator_makes_higher_low = last_indicator > indicator_at_min_price
            # Û³. Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø¯Ø± Ù†Ø§Ø­ÛŒÙ‡ Ú©Ù (Ù…Ø«Ù„Ø§Ù‹ RSI < 50) Ù…Ø¹ØªØ¨Ø±ØªØ± Ø§Ø³Øª
            is_at_bottom = last_indicator < 50
            
            return price_makes_lower_low and indicator_makes_higher_low and is_at_bottom

        elif check_type == 'negative':
            # RD-: Ù‚ÛŒÙ…Øª Ø³Ù‚Ù Ø¨Ø§Ù„Ø§ØªØ± Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯ØŒ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± Ø³Ù‚Ù Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ± Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯.
            max_price_idx = lookback_prices.idxmax()
            max_price_val = lookback_prices.loc[max_price_idx]
            indicator_at_max_price = lookback_indicators.loc[max_price_idx]

            if pd.isna(max_price_val) or pd.isna(indicator_at_max_price) or pd.isna(last_price) or pd.isna(last_indicator):
                return False

            # Û±. Ù‚ÛŒÙ…Øª Ø³Ù‚Ù Ø¨Ø§Ù„Ø§ØªØ± Ø²Ø¯Ù‡
            price_makes_higher_high = last_price > max_price_val
            # Û². Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± Ø³Ù‚Ù Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ± Ø²Ø¯Ù‡
            indicator_makes_lower_high = last_indicator < indicator_at_max_price
            
            return price_makes_higher_high and indicator_makes_lower_high

    except Exception as e:
        # e.g., if series is empty or idxmin fails
        logger.warning(f"Divergence check failed: {e}")
        return False
        
    return False


def _check_oscillator_signals(tech_df: pd.DataFrame, close_ser: pd.Series, technical_rec, prev_tech_rec):
    """
    Checks oscillator-based signals like RSI and Stochastic.
    ğŸ’¡ G-Fix: Ø´Ø§Ù…Ù„ Ù…Ø­Ø§Ø³Ø¨Ù‡ ØµØ­ÛŒØ­ ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ø¨Ø§ Ù†Ú¯Ø§Ù‡ Ø¨Ù‡ Ú¯Ø°Ø´ØªÙ‡ (Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ù…Ù†Ø·Ù‚ Ø§Ø´ØªØ¨Ø§Ù‡ Û² Ø±ÙˆØ²Ù‡).
    """

    satisfied_filters, reason_parts = [], {"technical": []}
    
    # ØªØ¹Ø±ÛŒÙ Ø¯ÙˆØ±Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ
    DIVERGENCE_LOOKBACK = 20 

    # --- RSI Positive Divergence ---
    rsi_series = tech_df['RSI'] if 'RSI' in tech_df.columns else pd.Series(dtype=float)
    
    if _find_divergence(close_ser, rsi_series, DIVERGENCE_LOOKBACK, 'positive_rsi'):
        current_rsi = _get_attr_safe(technical_rec, 'RSI')
        satisfied_filters.append("RSI_Positive_Divergence")
        reason_parts["technical"].append(f"Positive divergence on RSI ({current_rsi:.2f}).")

    
# --- ğŸ’¡ G-4: MACD Negative Divergence (Penalty) ---
    macd_series = tech_df['MACD'] if 'MACD' in tech_df.columns else pd.Series(dtype=float)
        
    if _find_divergence(close_ser, macd_series, DIVERGENCE_LOOKBACK, 'negative'):
        current_macd = _get_attr_safe(technical_rec, 'MACD')
        satisfied_filters.append("MACD_Negative_Divergence")
        reason_parts["technical"].append(f"Negative divergence on MACD (Price up, MACD down).")

    # --- Stochastic Oscillator ---
    current_stoch_k = _get_attr_safe(technical_rec, 
'Stochastic_K')
    current_stoch_d = _get_attr_safe(technical_rec, 'Stochastic_D')
    prev_stoch_k = _get_attr_safe(prev_tech_rec, 'Stochastic_K')
    prev_stoch_d = _get_attr_safe(prev_tech_rec, 'Stochastic_D')
    if all(x is not None for x in [current_stoch_k, current_stoch_d, prev_stoch_k, prev_stoch_d]):
        if current_stoch_k > current_stoch_d and prev_stoch_k <= prev_stoch_d and current_stoch_d < 25:
            satisfied_filters.append("Stochastic_Bullish_Cross_Oversold")
            reason_parts["technical"].append("Stochastic bullish cross in oversold area.")
            
return satisfied_filters, reason_parts

def _check_trend_signals(technical_rec, prev_tech_rec, last_close_val, market_sentiment: str):
    """
    Checks trend-following signals.
"""
    satisfied_filters, reason_parts = [], {"technical": []}
    
    # --- ğŸ’¡ G-1: Trend State Definition (using SMA) ---
    sma20 = _get_attr_safe(technical_rec, 'SMA_20')
    sma50 = _get_attr_safe(technical_rec, 'SMA_50')
    is_uptrend = False

    if sma20 is not None and sma50 is not None:
        if sma20 > sma50:
            is_uptrend = True
            if last_close_val > sma20:
               satisfied_filters.append("Strong_Uptrend_Confirmed")
                reason_parts["technical"].append(f"Strong Uptrend (SMA20 > SMA50, Close > SMA20).")
        else:
            satisfied_filters.append("Strong_Downtrend_Confirmed") # Penalty
            reason_parts["technical"].append(f"Downtrend Confirmed (SMA20 < SMA50).")

    # --- ğŸ’¡ G-Trend: Long-Term Trend Penalty (SMA200) ---
    sma200 = _get_attr_safe(technical_rec, 'SMA_200') # ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… SMA_200 
Ù…Ø­Ø§Ø³Ø¨Ù‡ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡
    if sma200 is not None and last_close_val < sma200:
        satisfied_filters.append("Price_Below_SMA200")
        reason_parts["technical"].append(f"Penalty: Price is below long-term SMA200.")

    # --- MACD Cross ---
    current_macd = _get_attr_safe(technical_rec, 'MACD')
    current_macd_signal = _get_attr_safe(technical_rec, 'MACD_Signal')
    prev_macd = _get_attr_safe(prev_tech_rec, 'MACD')
    prev_macd_signal = _get_attr_safe(prev_tech_rec, 'MACD_Signal')
    macd_crossed = False
    if all(x is not None for x in [current_macd, current_macd_signal, prev_macd, prev_macd_signal]):
    if current_macd > current_macd_signal and prev_macd <= prev_macd_signal:
            satisfied_filters.append("MACD_Bullish_Cross_Confirmed")
            # ğŸ’¡ FIX: Indent Error (Ø®Ø· Û´Û²Û°)
            macd_crossed = True
        
    # --- HalfTrend ---
    current_halftrend = _get_attr_safe(technical_rec, 'halftrend_signal')
    prev_halftrend = _get_attr_safe(prev_tech_rec, 'halftrend_signal')
    halftrend_buy = False
    if current_halftrend == 1 and prev_halftrend != 1:
      satisfied_filters.append("HalfTrend_Buy_Signal")
        halftrend_buy = True
        
    # --- Dynamic Resistance Break (from model) ---
    resistance_broken = _get_attr_safe(technical_rec, 'resistance_broken')
    if resistance_broken:
        satisfied_filters.append("Resistance_Broken")
        res_level = _get_attr_safe(technical_rec, 'resistance_level_50d', 'N/A')
        reason_parts["technical"].append(f"Broke a key dynamic resistance level around {res_level}.")

    # --- ğŸ’¡ G-3: Market Sentiment Penalty (Expanded) ---
    if market_sentiment 
== "Bearish" and (macd_crossed or halftrend_buy or resistance_broken):
        satisfied_filters.append("Signal_Against_Market_Trend")
        reason_parts["technical"].append(f"Penalty: Bullish signal detected in a Bearish market.")

    return satisfied_filters, reason_parts

def _check_volatility_signals(hist_df, technical_rec, last_close_val):
    """Checks volatility-based signals like Bollinger Bands and Squeeze Momentum."""
    satisfied_filters, reason_parts = [], {"technical": []}

    # Bollinger Lower Band
    bollinger_low = _get_attr_safe(technical_rec, 'Bollinger_Low')
    if bollinger_low is not None and last_close_val < bollinger_low:
        satisfied_filters.append("Bollinger_Lower_Band_Touch")

    # Squeeze 
Momentum
    prev_tech_rec = hist_df.iloc[-2] if len(hist_df) > 1 else technical_rec
    current_squeeze_on = _get_attr_safe(technical_rec, 'squeeze_on')
    prev_squeeze_on = _get_attr_safe(prev_tech_rec, 'squeeze_on')
    if current_squeeze_on == False and prev_squeeze_on == True:
        satisfied_filters.append("Squeeze_Momentum_Fired_Long")
        reason_parts["technical"].append("Squeeze Momentum indicator fired long.")
        
    return satisfied_filters, reason_parts

def _check_volume_signals(hist_df, tech_df, technical_rec, close_ser):
    """
    Checks for volume-based signals including Z-Score (last day spike) 
    and Volume MA trend 
(sustained interest).
    """
    satisfied_filters, reason_parts = [], {"technical": []}
    
    # ğŸ’¡ G-Volume: Ú†Ú© Ú©Ø±Ø¯Ù† Ù‚ÙÙ„ ØµÙ (Locked Market)
    is_locked_market = False
    if not hist_df.empty:
        last_hist = hist_df.iloc[-1]
        if last_hist['high'] == last_hist['low']:
            is_locked_market = True
            reason_parts["technical"].append(f"Note: Market was locked (High == Low). Volume spikes ignored.")
    
   # 1. High Volume On Up Day (Z-Score)
    if 'volume' in hist_df.columns and len(hist_df) >= 20 and len(close_ser) > 1:
        volume_z_score = calculate_z_score(pd.to_numeric(hist_df['volume'], errors='coerce').dropna().iloc[-20:])
        # ğŸ’¡ G-Volume: Ø³ÛŒÚ¯Ù†Ø§Ù„ ÙÙ‚Ø· Ø¯Ø± ØµÙˆØ±ØªÛŒ ÙØ¹Ø§Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ ØµÙ Ù†Ø¨Ø§Ø´Ø¯
        if volume_z_score is not None and volume_z_score > 1.5 and close_ser.iloc[-1] > close_ser.iloc[-2] and not is_locked_market:
            satisfied_filters.append("High_Volume_On_Up_Day")
           reason_parts["technical"].append(f"High volume (Z-Score: {volume_z_score:.2f}) on a positive day.")

    # 2. ğŸ’¡ G-Volume: Volume MA Is Rising (Using Slope)
    if is_data_sufficient(tech_df, 10): # Ø­Ø¯Ø§Ù‚Ù„ Û±Û° Ø±ÙˆØ² Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´ÛŒØ¨
        try:
            # Û±Û° Ø¯ÛŒØªØ§ÛŒ Ø¢Ø®Ø± Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø­Ø¬Ù… Û²Û° Ø±ÙˆØ²Ù‡
            vol_ma_series = pd.to_numeric(tech_df['Volume_MA_20'].dropna().tail(10))
            if len(vol_ma_series) >= 5: # Ø­Ø¯Ø§Ù‚Ù„ Ûµ Ù†Ù‚Ø·Ù‡ Ø¨Ø±Ø§ÛŒ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ†
              x = np.arange(len(vol_ma_series))
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´ÛŒØ¨ Ø®Ø· Ø±Ú¯Ø±Ø³ÛŒÙˆÙ†
                slope = np.polyfit(x, vol_ma_series, 1)[0] 
                
                # Ø§Ú¯Ø± Ø´ÛŒØ¨ Ù…Ø«Ø¨Øª Ø¨Ø§Ø´Ø¯ (Ø±ÙˆÙ†Ø¯ ØµØ¹ÙˆØ¯ÛŒ)
           if slope > 0:
                    satisfied_filters.append("Volume_MA_Is_Rising")
                    reason_parts["technical"].append(f"Volume MA (20d) slope is positive ({slope:,.0f}).")
        except Exception as e:
            logger.warning(f"Could not calculate volume MA slope: {e}")
           pass 
            
    return satisfied_filters, reason_parts

def _check_technical_filters(hist_df, tech_df, market_sentiment: str):
    """
    Checks all technical indicators by calling specialized sub-functions.
(Coordinator function)
    """
    all_satisfied_filters, all_reason_parts = [], {"technical": []}
    if not is_data_sufficient(tech_df, 2) or not is_data_sufficient(hist_df, MIN_REQUIRED_HISTORY_DAYS):
        return all_satisfied_filters, all_reason_parts

    technical_rec = tech_df.iloc[-1]
    prev_tech_rec = tech_df.iloc[-2]
    
    close_ser = _get_close_series_from_hist_df(hist_df)
    last_close_val = close_ser.iloc[-1] if not close_ser.empty else None
    if last_close_val is None:
        return all_satisfied_filters, all_reason_parts

    # ğŸ’¡ FIX: Indent Error (Ø®Ø· Û³ÛµÛ°)
    # 
Call sub-functions and aggregate results
    for func in [_check_oscillator_signals, _check_trend_signals, _check_volatility_signals, _check_volume_signals]:
        # Adjust arguments as needed per function signature
        
        # ğŸ’¡ G-Fix: Ø§ØµÙ„Ø§Ø­ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ ØªØ§Ø¨Ø¹ _check_oscillator_signals Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ tech_df Ú©Ø§Ù…Ù„
        if func == _check_oscillator_signals:
             satisfied, reasons = func(tech_df, close_ser, technical_rec, prev_tech_rec)
        
        elif func == _check_trend_signals:
             satisfied, reasons = func(technical_rec, prev_tech_rec, last_close_val, market_sentiment)
         
        elif func == _check_volatility_signals:
            satisfied, reasons = func(hist_df, technical_rec, last_close_val)
        
        elif func == _check_volume_signals:
            satisfied, reasons = func(hist_df, tech_df, technical_rec, close_ser)
        else:
            continue

   all_satisfied_filters.extend(satisfied)
        if "technical" in reasons:
            all_reason_parts.setdefault("technical", []).extend(reasons["technical"])

    return all_satisfied_filters, all_reason_parts

# --- END REFACTORED SECTION ---

# ğŸ’¡ G-Fundamental: ØªØ§Ø¨Ø¹ Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù‡ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„ØªØ± Ø³Ø§Ø¯Ù‡ P/E
def _check_simple_fundamental_filters(symbol_data_rec):
    """
    Ø¨Ø±Ø±Ø³ÛŒ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø³Ø§Ø¯Ù‡ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„ (P/E) Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø³Ù‡Ø§Ù… Ø­Ø¨Ø§Ø¨ÛŒ.
"""
    satisfied_filters, reason_parts = [], {"fundamental": []}
    if symbol_data_rec:
        # pe_ratio Ø§Ø² ComprehensiveSymbolData Ø®ÙˆØ§Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
        pe = getattr(symbol_data_rec, 'pe_ratio', None)
        if pe is not None and 0 < pe < 15:
            satisfied_filters.append("Reasonable_PE")
            reason_parts["fundamental"].append(f"P/E ({pe:.2f}) is reasonable (< 15).")
    return satisfied_filters, reason_parts

def _check_smart_money_filters(hist_df):
    """REVISED: Now also 
checks for heavy individual buy pressure on the last day."""
    satisfied_filters = []
    reason_parts = {"smart_money": []}
    trend_lookback = 10
    if hist_df is None or hist_df.empty or 'buy_i_volume' not in hist_df.columns:
        return satisfied_filters, reason_parts

    # Check 1: 10-Day Real Money Flow Trend
    if len(hist_df) >= trend_lookback:
        smart_money_flow_df = calculate_smart_money_flow(hist_df)
        if not smart_money_flow_df.empty and len(smart_money_flow_df) >= trend_lookback:
          trend_net_flow = smart_money_flow_df['individual_net_flow'].iloc[-trend_lookback:].sum()
            if trend_net_flow > 0:
                satisfied_filters.append("Positive_Real_Money_Flow_Trend_10D")
                reason_parts["smart_money"].append(f"Positive real money inflow over the last {trend_lookback} days.")
            elif trend_net_flow < 0:
                satisfied_filters.append("Negative_Real_Money_Flow_Trend_10D")
    
   # NEW Check 2: Heavy Individual Buy Pressure on the last day
    last_day = hist_df.iloc[-1]
    required_cols = ['buy_i_volume', 'buy_count_i', 'sell_i_volume', 'sell_count_i']
    if all(col in last_day and pd.notna(last_day[col]) for col in required_cols):
        if last_day['buy_count_i'] > 0 and last_day['sell_count_i'] > 0:
            per_capita_buy = last_day['buy_i_volume'] / last_day['buy_count_i']
            per_capita_sell = last_day['sell_i_volume'] / last_day['sell_count_i']
            

            # Check if per capita buy is 2.5x greater than sell
            if per_capita_buy > (per_capita_sell * 2.5):
                satisfied_filters.append("Heavy_Individual_Buy_Pressure")
                reason_parts["smart_money"].append(f"Significant buy pressure detected (Per capita buy: {per_capita_buy:,.0f} vs sell: {per_capita_sell:,.0f}).")

    return satisfied_filters, reason_parts


def _check_power_thrust_signal(hist_df, close_ser):
    """
    Checks for a 
Power Thrust Signal: a combination of high volume,
    heavy individual buy pressure, and a positive closing day.
"""
    satisfied_filters, reason_parts = [], {"power_thrust": []}
    
    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø¯Ù‡ Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„
    if hist_df is None or len(hist_df) < 20 or close_ser.empty or len(close_ser) < 2:
        return satisfied_filters, reason_parts

    last_day = hist_df.iloc[-1]
    
    # --- ğŸ’¡ G-Volume: Ú†Ú© Ú©Ø±Ø¯Ù† Ù‚ÙÙ„ ØµÙ ---
    is_locked_market = last_day['high'] == last_day['low']
    if is_locked_market:
        return satisfied_filters, reason_parts # Ø³ÛŒÚ¯Ù†Ø§Ù„ Ù‚Ø¯Ø±Øª 
Ø¯Ø± ØµÙ Ø®Ø±ÛŒØ¯/ÙØ±ÙˆØ´ØŒ Ù…Ø¹ØªØ¨Ø± Ù†ÛŒØ³Øª

    # --- Ø´Ø±Ø· Û±: Ø±ÙˆØ² Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ Ù…Ø«Ø¨Øª ---
    is_up_day = close_ser.iloc[-1] > close_ser.iloc[-2]
    if not is_up_day:
        return satisfied_filters, reason_parts # Ø§Ú¯Ø± Ø±ÙˆØ² 
Ù…Ø«Ø¨Øª Ù†ÛŒØ³ØªØŒ Ø§Ø¯Ø§Ù…Ù‡ Ù†Ø¯Ù‡

    # --- Ø´Ø±Ø· Û²: Ø­Ø¬Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¨Ø³ÛŒØ§Ø± Ø¨Ø§Ù„Ø§ (Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Z-Score) ---
    volume_series = pd.to_numeric(hist_df['volume'], errors='coerce').dropna().tail(20)
    volume_z_score = calculate_z_score(volume_series)
    # Ø¢Ø³ØªØ§Ù†Ù‡ Ø¨Ø§Ù„Ø§ Ø¨Ø±Ø§ÛŒ Ø­Ø¬Ù…ØŒ Ù…Ø«Ù„Ø§ Z-Score Ø¨ÛŒØ´ØªØ± Ø§Ø² 1.8
    is_high_volume = volume_z_score is not None and volume_z_score > 
1.8 
    
    # --- Ø´Ø±Ø· Û³: ÙØ´Ø§Ø± Ø®Ø±ÛŒØ¯ Ø³Ù†Ú¯ÛŒÙ† Ø­Ù‚ÛŒÙ‚ÛŒâ€ŒÙ‡Ø§ ---
    is_heavy_buy_pressure = False
    required_cols = ['buy_i_volume', 'buy_count_i', 'sell_i_volume', 'sell_count_i']
    if all(col in last_day and pd.notna(last_day[col]) for col in required_cols):
        if last_day['buy_count_i'] > 0 and last_day['sell_count_i'] > 0:
            per_capita_buy = last_day['buy_i_volume'] / last_day['buy_count_i']
            per_capita_sell = last_day['sell_i_volume'] / last_day['sell_count_i']
        # Ø¢Ø³ØªØ§Ù†Ù‡ Ø¨Ø§Ù„Ø§ Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø§Ù†Ù‡ Ø®Ø±ÛŒØ¯ØŒ Ù…Ø«Ù„Ø§ Û².Ûµ Ø¨Ø±Ø§Ø¨Ø± Ø³Ø±Ø§Ù†Ù‡ ÙØ±ÙˆØ´
            if per_capita_buy > (per_capita_sell * 2.5):
                is_heavy_buy_pressure = True

    # --- ØªØ±Ú©ÛŒØ¨ Ù†Ù‡Ø§ÛŒÛŒ ---
    if is_high_volume and is_heavy_buy_pressure:
        satisfied_filters.append("Power_Thrust_Signal")
        reason_parts["power_thrust"].append(
            f"Power signal detected! 
High Volume (Z-Score: {volume_z_score:.2f}) and Heavy Buy Pressure."
        )
            
    return satisfied_filters, reason_parts


# --- NEW: Functions for new strategic filters --- 
def _get_leading_sectors():
    """ 
    Ø¨Ø§ Ú©ÙˆØ¦Ø±ÛŒ Ø¨Ù‡ Ø¬Ø¯ÙˆÙ„ DailySectorPerformanceØŒ Ù„ÛŒØ³Øª ØµÙ†Ø§ÛŒØ¹ Ù¾ÛŒØ´Ø±Ùˆ (Ù…Ø«Ù„Ø§Ù‹ 4 
ØµÙ†Ø¹Øª Ø¨Ø±ØªØ±) Ø¯Ø± Ø¢Ø®Ø±ÛŒÙ† Ø±ÙˆØ² ØªØ­Ù„ÛŒÙ„ Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
"""
    try:
        latest_date_query = db.session.query(func.max(DailySectorPerformance.jdate)).scalar()
        if not latest_date_query:
            logger.warning("Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¯Ø± Ø¬Ø¯ÙˆÙ„ ØªØ­Ù„ÛŒÙ„ ØµÙ†Ø§ÛŒØ¹ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø§Ø² Ù„ÛŒØ³Øª Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
            return {"Ø®ÙˆØ¯Ø±Ùˆ Ùˆ Ø³Ø§Ø®Øª Ù‚Ø·Ø¹Ø§Øª"} # Fallback

        # Ø¯Ø±ÛŒØ§ÙØª 4 ØµÙ†Ø¹Øª Ø¨Ø±ØªØ± Ø¯Ø± Ø¢Ø®Ø±ÛŒÙ† Ø±ÙˆØ²
        leading_sectors_query = db.session.query(DailySectorPerformance.sector_name)\
           .filter(DailySectorPerformance.jdate == latest_date_query)\
            .order_by(DailySectorPerformance.rank.asc())\
            .limit(4).all()
        leading_sectors = {row[0] for row in leading_sectors_query}
        logger.info(f"ØµÙ†Ø§ÛŒØ¹ Ù¾ÛŒØ´Ø±Ùˆ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡ Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {leading_sectors}")
        return leading_sectors
    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª ØµÙ†Ø§ÛŒØ¹ Ù¾ÛŒØ´Ø±Ùˆ Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {e}")
        return {"Ø®ÙˆØ¯Ø±Ùˆ Ùˆ Ø³Ø§Ø®Øª 
Ù‚Ø·Ø¹Ø§Øª"} # Fallback in case of error

def _check_sector_strength_filter(symbol_sector, leading_sectors):
    """Checks if the symbol belongs to a leading sector."""
    satisfied_filters, reason_parts = [], {"sector_strength": []}
    if symbol_sector in leading_sectors:
        satisfied_filters.append("IsInLeadingSector")
        reason_parts["sector_strength"].append(f"Symbol is in a leading sector: {symbol_sector}.")
    return satisfied_filters, reason_parts

def _check_static_levels_filters(technical_rec, last_close_val):
    """ 
    Checks for proximity to static support or breakout of static resistance.
This assumes 'static_support_level' and 'static_resistance_level' fields are pre-calculated and available in the TechnicalIndicatorData record.
"""
    satisfied_filters, reason_parts = [], {"static_levels": []}

    is_rec_valid = technical_rec is not None and ( 
        not isinstance(technical_rec, pd.Series) or not technical_rec.empty 
    )
    if not is_rec_valid or last_close_val is None or last_close_val <= 0:
        return satisfied_filters, reason_parts

    # 1. Check for proximity to static support
    support_level = _get_attr_safe(technical_rec, 'static_support_level')

    if support_level and support_level > 0:
        distance = 
last_close_val - support_level
        proximity_percent = abs(distance) / support_level 
        if proximity_percent <= 0.02 and distance >= -0.005 * support_level: # Ù†Ø²Ø¯ÛŒÚ© ÛŒØ§ Ú©Ù…ÛŒ Ø¨Ø§Ù„Ø§ØªØ± Ø§Ø² Ø³Ø·Ø­ (Ø­Ø¯Ø§Ú©Ø«Ø± 0.5% Ø²ÛŒØ± Ø³Ø·Ø­)
            satisfied_filters.append("Near_Static_Support")
            reason_parts["static_levels"].append(
                f"Price is near a major static support level at {support_level:,.0f} (Proximity: {proximity_percent*100:.1f}%)."
      )

    # 2. Check for breakout of static resistance
    resistance_level = _get_attr_safe(technical_rec, 'resistance_level_50d')

    if resistance_level and resistance_level > 0:
        if last_close_val > resistance_level and last_close_val < 1.03 * resistance_level: # Ø´Ø±Ø· Ø¯ÙˆÙ…: Ù‚ÛŒÙ…Øª Ø¨ÛŒØ´ Ø§Ø² Ø­Ø¯ Ø§Ø² Ù…Ù‚Ø§ÙˆÙ…Øª Ø¯ÙˆØ± Ù†Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ (Ø´Ú©Ø³Øª ØªØ§Ø²Ù‡ Ùˆ Ù…Ø¹ØªØ¨Ø±)
            satisfied_filters.append("Static_Resistance_Broken")
            reason_parts["static_levels"].append(
          f"Price broke a major static resistance level at {resistance_level:,.0f}."
            )

    return satisfied_filters, reason_parts
# --- END NEW STRATEGIC FILTERS --- 


# ğŸ’¡ G-Performance: ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ù…ÙˆØ§Ø²ÛŒ
def _analyze_single_symbol(
    symbol: ComprehensiveSymbolData,
    hist_groups: Dict[str, pd.DataFrame],
    tech_groups: Dict[str, pd.DataFrame],
    ml_prediction_set: set,
    leading_sectors: set,
    market_sentiment: str,
    score_threshold: int,
    today_jdate: str
) -> Optional[Dict]:
    """
    Ù‡Ø³ØªÙ‡ 
Ù…Ù†Ø·Ù‚ ØªØ­Ù„ÛŒÙ„ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ù†Ù…Ø§Ø¯ ÙˆØ§Ø­Ø¯.
    Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ù…ÙˆØ§Ø²ÛŒ (joblib) Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª.
"""
    
    # ğŸ’¡ G-Performance: Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø§ÛŒØ¯ Ø¯Ø± Ø¯Ø§Ø®Ù„ ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ ØªØ¹Ø±ÛŒÙ Ø´ÙˆØ¯
    # ÛŒØ§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ© ØªØ§Ø¨Ø¹ Ù…Ø³ØªÙ‚Ù„ Ø¯Ø± Ø³Ø·Ø­ Ø¨Ø§Ù„Ø§ (Ú©Ù‡ Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡)
    def run_check(check_func, *args):
        filters, reasons = check_func(*args)
        all_satisfied_filters.extend(filters)
        all_reason_parts.update(reasons)

    try:
        symbol_hist_df = hist_groups.get(symbol.symbol_id, pd.DataFrame()).copy()
        symbol_tech_df = tech_groups.get(symbol.symbol_id, pd.DataFrame()).copy()

    if len(symbol_hist_df) < MIN_REQUIRED_HISTORY_DAYS:
            logger.debug(f"Skipped {symbol.symbol_name} ({symbol.symbol_id}): insufficient history ({len(symbol_hist_df)} < {MIN_REQUIRED_HISTORY_DAYS})")
            return None

        last_close_series = _get_close_series_from_hist_df(symbol_hist_df)
        entry_price = float(last_close_series.iloc[-1]) if not last_close_series.empty else None
        if entry_price is None or pd.isna(entry_price):
            logger.warning(f"Skipping {symbol.symbol_name} ({symbol.symbol_id}) due to missing entry price.")
         return None

        all_satisfied_filters = []
        all_reason_parts = {}

        technical_rec = symbol_tech_df.iloc[-1] if not symbol_tech_df.empty else None
        
        # ğŸ’¡ G-Fundamental: Ø§Ø² Ø®ÙˆØ¯ Ø¢Ø¨Ø¬Ú©Øª symbol Ø¨Ø±Ø§ÛŒ P/E Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        symbol_data_rec = symbol 

        # --- Run All Filters ---
    run_check(_check_sector_strength_filter, getattr(symbol, 'sector_name', ''), leading_sectors)
        run_check(_check_technical_filters, symbol_hist_df, symbol_tech_df, market_sentiment)
        run_check(_check_market_condition_filters, symbol_hist_df, symbol_tech_df)
        run_check(_check_static_levels_filters, technical_rec, entry_price)
        run_check(_check_simple_fundamental_filters, symbol_data_rec) # ğŸ’¡ G-Fundamental
        run_check(_check_smart_money_filters, symbol_hist_df)
        run_check(_check_power_thrust_signal, symbol_hist_df, last_close_series)

        # ML Filter
        if symbol.symbol_id in ml_prediction_set:
           all_satisfied_filters.append("ML_Predicts_Uptrend")
            all_reason_parts.setdefault("ml_signal", []).append("ML model predicts a high-probability uptrend.")

        # Calculate Weighted Score
        score = sum(FILTER_WEIGHTS.get(f, {}).get('weight', 0) for f in all_satisfied_filters)

        if score >= score_threshold:
            return {
                "symbol_id": symbol.symbol_id,
           "symbol_name": symbol.symbol_name,
                "entry_price": entry_price,
                "entry_date": date.today(),
                "jentry_date": today_jdate,
                "outlook": "Bullish",
                "reason_json": json.dumps(all_reason_parts, ensure_ascii=False),
          "satisfied_filters": json.dumps(list(set(all_satisfied_filters)), ensure_ascii=False),
                "score": score
            }
        
        return None # Score below threshold
    
    except Exception as e:
        logger.error(f"Error processing symbol {symbol.symbol_id}: {e}", exc_info=True)
        return None


def run_weekly_watchlist_selection():
    """
  Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø¨Ø±Ø§ÛŒ ÙˆØ§Ú†â€ŒÙ„ÛŒØ³Øª Ù‡ÙØªÚ¯ÛŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø­Ø¬ÛŒÙ…ØŒ
    Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ø¯ÛŒÙ†Ø§Ù…ÛŒÚ© Ùˆ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒÚ©.
"""
    # ğŸ’¡ G-CleanUp: Ø§ÛŒÙ…Ù¾ÙˆØ±Øªâ€ŒÙ‡Ø§ Ù‚Ø¨Ù„Ø§Ù‹ Ø¨Ù‡ Ø¨Ø§Ù„Ø§ Ù…Ù†ØªÙ‚Ù„ Ø´Ø¯Ù†Ø¯
    
    logger.info("ğŸš€ Starting Weekly Watchlist selection process...")
    start_time = time.time()

    # Step 1: Determine market sentiment and leading sectors
    market_sentiment = _get_market_sentiment()
    leading_sectors = _get_leading_sectors()

    if market_sentiment == "Bullish":
        score_threshold = 7
    elif market_sentiment == "Neutral":
        score_threshold = 8
    else:
    score_threshold = 10

    logger.info(f"ğŸ“ˆ Market sentiment: {market_sentiment}, Score threshold: >= {score_threshold}")
    logger.info(f"ğŸ­ Leading sectors identified: {leading_sectors}")

    # Step 2: Bulk Data Fetching
    allowed_market_types = ['Ø¨ÙˆØ±Ø³', 'ÙØ±Ø§Ø¨ÙˆØ±Ø³', 'Ù¾Ø§ÛŒÙ‡ ÙØ±Ø§Ø¨ÙˆØ±Ø³', 'Ø¨ÙˆØ±Ø³ Ú©Ø§Ù„Ø§', 'Ø¨ÙˆØ±Ø³ Ø§Ù†Ø±Ú˜ÛŒ']

    # Fetch active symbols
    symbols_to_analyze = ComprehensiveSymbolData.query.filter(
        ComprehensiveSymbolData.market_type.in_(allowed_market_types)
    ).all()

    if not symbols_to_analyze:
        logger.warning("âš ï¸ No symbols found for analysis.")
        return False, "No active 
symbols available for analysis."
    
    # ğŸ’¡ FIX: Indent Error (Ø®Ø· ÛµÛµÛ°)
    symbol_ids = [s.symbol_id for s in symbols_to_analyze]
    total_symbols = len(symbol_ids)
    logger.info(f"ğŸ§© Found {total_symbols} active symbols for analysis.")

    fetch_start = time.time()

    # Calculate lookback date
    today_greg = datetime.now().date()
    # ğŸ’¡ G-Trend: Ø§ÙØ²Ø§ÛŒØ´ lookback Ø¨Ø±Ø§ÛŒ Ù¾ÙˆØ´Ø´ SMA200 (Ø§Ú¯Ø±Ú†Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ù‡Ù†ÙˆØ² Ù…Ø­Ø¯ÙˆØ¯ Ø¨Ø§Ø´Ù†Ø¯)
    lookback_greg = today_greg - timedelta(days=max(TECHNICAL_DATA_LOOKBACK_DAYS * 2, 250))
    lookback_jdate_str = convert_gregorian_to_jalali(lookback_greg)
 if not lookback_jdate_str:
        logger.error("âŒ Could not determine lookback date. 
Aborting.")
        return False, "Failed to calculate lookback date."
    # Fetch Historical Data
    hist_records = HistoricalData.query.filter(
        HistoricalData.symbol_id.in_(symbol_ids),
        HistoricalData.jdate >= lookback_jdate_str
    ).all()
    hist_df = pd.DataFrame([h.__dict__ for h in hist_records]).drop(columns=['_sa_instance_state'], errors='ignore')

    # Fetch Technical Data
    tech_records = TechnicalIndicatorData.query.filter(
        TechnicalIndicatorData.symbol_id.in_(symbol_ids),
        TechnicalIndicatorData.jdate >= lookback_jdate_str
    ).all()
    tech_df = pd.DataFrame([t.__dict__ for t 
in tech_records]).drop(columns=['_sa_instance_state'], errors='ignore')

    # ğŸ’¡ G-CleanUp: Ø­Ø°Ù ÙˆØ§Ú©Ø´ÛŒ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„/Ú©Ù†Ø¯Ù„
    today_jdate = get_today_jdate_str()

    # Fetch ML Predictions
    ml_predictions = MLPrediction.query.filter(
        MLPrediction.symbol_id.in_(symbol_ids),
        MLPrediction.jprediction_date == today_jdate,
        MLPrediction.predicted_trend == 'Uptrend'
    ).all()

    fetch_end = time.time()
    logger.info(f"ğŸ“¥ Bulk fetch time: {fetch_end - fetch_start:.2f}s (hist: {len(hist_df)}, tech: {len(tech_df)} rows)")

    # Grouping Data
    group_start = time.time()
    hist_groups = {k: v.sort_values(by='jdate') for k, v in hist_df.groupby("symbol_id")} if not hist_df.empty else {}

    tech_groups = {k: v.sort_values(by='jdate') for k, v in tech_df.groupby("symbol_id")} if not tech_df.empty else {}
    ml_prediction_set = {rec.symbol_id for rec in ml_predictions}

    group_end = time.time()
    logger.info(f"ğŸ“Š Data grouping completed: {group_end - group_start:.2f}s. 
Groups: hist={len(hist_groups)}, tech={len(tech_groups)}")

    logger.info("ğŸ“Š Data grouping completed. Beginning scoring and selection...")

    # Step 3: ğŸ’¡ G-Performance: Scoring (Parallelized)
    loop_start = time.time()
    
    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² n_jobs=-1 Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÙ…Ø§Ù… Ù‡Ø³ØªÙ‡â€ŒÙ‡Ø§
    # verbose=5 Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ù„Ø§Ú¯ Ù¾ÛŒØ´Ø±ÙØª
    parallel = Parallel(n_jobs=-1, verbose=5) 
    
    results = parallel(delayed(_analyze_single_symbol)(
        symbol,
        hist_groups,
        tech_groups,
    ml_prediction_set,
        leading_sectors,
        market_sentiment,
        score_threshold,
        today_jdate
    ) for symbol in symbols_to_analyze)
    
    # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ù†ØªØ§ÛŒØ¬ None (Ø³Ù‡Ø§Ù…ÛŒ Ú©Ù‡ Ø±Ø¯ Ø´Ø¯Ù†Ø¯)
    watchlist_candidates = [res for res in results if res is not None]

    loop_end = time.time()
    processed_count = total_symbols # joblib Ù‡Ù…Ù‡ Ø±Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    skipped_count = total_symbols - len(results) # (ØªØ®Ù…ÛŒÙ†ØŒ Ú†ÙˆÙ† NoneÙ‡Ø§ Ù‡Ù… Ø¯Ø± results Ù‡Ø³ØªÙ†Ø¯)


    logger.info(f"ğŸ”„ Parallel loop completed: Processed {processed_count}/{total_symbols} symbols. 
Loop time: {loop_end - loop_start:.2f}s")

    logger.info(f"âœ… {len(watchlist_candidates)} symbols passed the threshold ({score_threshold}). Saving top 8...")

    # Step 4: Save results
    watchlist_candidates.sort(key=lambda x: x['score'], reverse=True)
    final_watchlist = watchlist_candidates[:8]

    saved_count = 0
    for candidate in final_watchlist:
        existing_result = WeeklyWatchlistResult.query.filter_by(
            symbol_id=candidate['symbol_id'], jentry_date=candidate['jentry_date']
        ).first()

        if existing_result:
       existing_result.entry_price = candidate['entry_price']
            existing_result.outlook = candidate['outlook']
            existing_result.reason = candidate['satisfied_filters']
            existing_result.probability_percent = min(100, candidate['score'] * 4)
            existing_result.created_at = datetime.now()
            setattr(existing_result, 'score', candidate['score']) # ğŸ’¡ G-Fix: Ø°Ø®ÛŒØ±Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø¯Ø± Ø¢Ù¾Ø¯ÛŒØª
        else:
        existing_result = WeeklyWatchlistResult(
                signal_unique_id=str(uuid.uuid4()),
                symbol_id=candidate['symbol_id'],
                symbol_name=candidate['symbol_name'],
                entry_price=candidate['entry_price'],
                entry_date=candidate['entry_date'],
          jentry_date=candidate['jentry_date'],
                outlook=candidate['outlook'],
                reason=candidate['satisfied_filters'],
                probability_percent=min(100, candidate['score'] * 4),
                status='active',
                score=candidate['score'] # ğŸ’¡ G-Fix: Ø°Ø®ÛŒØ±Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø¯Ø± Ø±Ø¯ÛŒÙ Ø¬Ø¯ÛŒØ¯
      )
        db.session.add(existing_result)
        saved_count += 1

    try:
        db.session.commit()
        message = f"Weekly Watchlist selection completed. 
Saved {saved_count} symbols."
        logger.info(message)
        total_time = time.time() - start_time
        logger.info(f"â±ï¸ Full process time: {total_time:.2f}s")
        return True, message
    except Exception as e:
        db.session.rollback()
        logger.error(f"âŒ Database commit failed: {e}", exc_info=True)
        return False, "Database commit failed."



def get_weekly_watchlist_results(jdate_str: Optional[str] = None):
    """
    Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø¢Ø®Ø±ÛŒÙ† Ù†ØªØ§ÛŒØ¬ ÙˆØ§Ú†â€ŒÙ„ÛŒØ³Øª 
Ù‡ÙØªÚ¯ÛŒ (ÛŒØ§ ØªØ§Ø±ÛŒØ® Ù…Ø´Ø®Øµâ€ŒØ´Ø¯Ù‡ Ø¯Ø± ØµÙˆØ±Øª Ø§Ø±Ø³Ø§Ù„).
    Ø®Ø±ÙˆØ¬ÛŒ Ø´Ø§Ù…Ù„ Ø¬Ø²Ø¦ÛŒØ§Øª Ù†Ù…Ø§Ø¯Ù‡Ø§ Ùˆ Ø¢Ø®Ø±ÛŒÙ† ØªØ§Ø±ÛŒØ® Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø§Ø³Øª.
"""

    logger.info("ğŸ“Š Retrieving latest weekly watchlist results...")

    # --- Step 1: ØªØ¹ÛŒÛŒÙ† Ø¢Ø®Ø±ÛŒÙ† ØªØ§Ø±ÛŒØ® ---
    if not jdate_str:
        latest_record = WeeklyWatchlistResult.query.order_by(WeeklyWatchlistResult.jentry_date.desc()).first()
        if not latest_record or not latest_record.jentry_date:
            logger.warning("âš ï¸ No weekly watchlist results found.")
            return {"top_watchlist_stocks": [], "last_updated": "Ù†Ø§Ù…Ø´Ø®Øµ"}
        # ğŸ’¡ FIX: Indent Error (Ø®Ø· Û¶Û¸Û°)
      jdate_str = latest_record.jentry_date

    logger.info(f"ğŸ—“ Latest Weekly Watchlist results date: {jdate_str}")

    # --- Step 2: ÙˆØ§Ú©Ø´ÛŒ Ù†ØªØ§ÛŒØ¬ ---
    results = WeeklyWatchlistResult.query.filter_by(jentry_date=jdate_str)\
                                         .order_by(WeeklyWatchlistResult.score.desc().nullslast(), WeeklyWatchlistResult.created_at.desc()).all()

    if not results:
        logger.warning("âš ï¸ No results found for this date.")
     return {"top_watchlist_stocks": [], "last_updated": jdate_str}

    # --- Step 3: ÙˆØ§Ú©Ø´ÛŒ Ù†Ø§Ù… Ø´Ø±Ú©Øªâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ ÙˆØ§Ú†â€ŒÙ„ÛŒØ³Øª ---
    symbol_ids_in_watchlist = [r.symbol_id for r in results]
    company_name_map = {}

    if symbol_ids_in_watchlist:
        company_name_records = ComprehensiveSymbolData.query.filter(
            ComprehensiveSymbolData.symbol_id.in_(symbol_ids_in_watchlist)
        ).with_entities(
            ComprehensiveSymbolData.symbol_id,
            ComprehensiveSymbolData.company_name
  ).all()
        company_name_map = {sid: cname for sid, cname in company_name_records}

    # --- Step 4: Ø³Ø§Ø®Øª Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù‡Ø§ÛŒÛŒ ---
    output_stocks = []
    for r in results:
        output_stocks.append({
            "signal_unique_id": r.signal_unique_id,
            "symbol_id": r.symbol_id,
            "symbol_name": r.symbol_name,
         "company_name": company_name_map.get(r.symbol_id, r.symbol_name),
            "outlook": r.outlook,
            "reason": r.reason,
            "entry_price": r.entry_price,
            "jentry_date": r.jentry_date,
            "exit_price": r.exit_price,
            "jexit_date": r.jexit_date,
            "profit_loss_percentage": r.profit_loss_percentage,
          "status": r.status,
            "probability_percent": getattr(r, "probability_percent", None),
            "score": getattr(r, "score", None)
        })

    # --- Step 5: Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù…ØªÛŒØ§Ø² ---
    # ğŸ’¡ G-Fix: Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø­ØªÛŒ Ø§Ú¯Ø± score Ø¨Ø±Ø§Ø¨Ø± Ø¨Ø§ None Ø¨Ø§Ø´Ø¯
    output_stocks.sort(key=lambda x: (x.get("score") or 0), reverse=True)

    logger.info(f"âœ… Retrieved and enriched {len(output_stocks)} weekly watchlist results for {jdate_str}.")

# --- Step 6: Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ ---
    return {
        "top_watchlist_stocks": output_stocks,
        "last_updated": jdate_str
    }
}
