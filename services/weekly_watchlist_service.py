# -*- coding: utf-8 -*-
# services/weekly_watchlist_service.py

# --- ğŸ’¡ G-CleanUp: ØªÙ…Ø§Ù… Ø§ÛŒÙ…Ù¾ÙˆØ±Øªâ€ŒÙ‡Ø§ Ø¨Ù‡ Ø³Ø·Ø­ Ø¨Ø§Ù„Ø§ Ù…Ù†ØªÙ‚Ù„ Ø´Ø¯Ù†Ø¯ ---
from extensions import db
from models import (
    HistoricalData, ComprehensiveSymbolData, TechnicalIndicatorData, 
    WeeklyWatchlistResult, SignalsPerformance, AggregatedPerformance, 
    GoldenKeyResult, MLPrediction, DailyIndexData, DailySectorPerformance
)
from flask import current_app
import pandas as pd
from datetime import datetime, timedelta, date
import jdatetime
import uuid
from sqlalchemy import func, text, and_
import logging
import json
import numpy as np
from types import SimpleNamespace
from typing import List, Dict, Tuple, Optional
import time
from sqlalchemy import desc

# ğŸ’¡ G-Performance: Ø§ÛŒÙ…Ù¾ÙˆØ±Øª Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆØ§Ø²ÛŒ
from joblib import Parallel, delayed

# ğŸ’¡ G-Sentiment: Ø§ÛŒÙ…Ù¾ÙˆØ±Øª Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø³Ù†ØªÛŒÙ…Ù†Øª
from services.index_data_fetcher import get_market_indices

# ğŸ’¡ G-CleanUp: 
from services.technical_analysis_utils import (
    get_today_jdate_str, normalize_value, calculate_rsi, 
    calculate_macd, calculate_sma, calculate_bollinger_bands, 
    calculate_volume_ma, calculate_atr, calculate_smart_money_flow, 
    convert_gregorian_to_jalali, calculate_z_score, calculate_static_support
)

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯ÛŒÙ†Ú¯ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù…Ø§Ú˜ÙˆÙ„
logger = logging.getLogger(__name__)

# IMPROVEMENT: Lookback period and minimum history days 
# adjusted for better indicator quality
TECHNICAL_DATA_LOOKBACK_DAYS = 120
MIN_REQUIRED_HISTORY_DAYS = 50

## ğŸ’¡ G-All: Ø¨Ø§Ø²Ø¨ÛŒÙ†ÛŒ Ú©Ø§Ù…Ù„ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ ÙÛŒÙ„ØªØ±Ù‡Ø§
# REVISED: New filter weights dictionary with descriptions for clarity
FILTER_WEIGHTS = {
    # --- High-Impact Leading & Breakout Signals ---
    "Power_Thrust_Signal": {
        "weight": 5,
        "description": "Ø³ÛŒÚ¯Ù†Ø§Ù„ Ù‚Ø¯Ø±Øª: ØªØ±Ú©ÛŒØ¨ÛŒ Ø§Ø² Ø­Ø¬Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù†ÙØ¬Ø§Ø±ÛŒØŒ ÙˆØ±ÙˆØ¯ Ù¾ÙˆÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø³Ù†Ú¯ÛŒÙ† Ø¯Ø± ÛŒÚ© Ø±ÙˆØ² Ù…Ø«Ø¨Øª (Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ† ØµÙ Ø®Ø±ÛŒØ¯/ÙØ±ÙˆØ´)."
    },
    "RSI_Positive_Divergence": {
        "weight": 5,
        "description": "ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ù…Ø«Ø¨Øª Ø¯Ø± RSIØŒ ÛŒÚ© Ø³ÛŒÚ¯Ù†Ø§Ù„ Ù¾ÛŒØ´Ø±Ùˆ Ù‚ÙˆÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø­ØªÙ…Ø§Ù„ Ø¨Ø§Ø²Ú¯Ø´Øª Ø±ÙˆÙ†Ø¯ Ù†Ø²ÙˆÙ„ÛŒ Ø¨Ù‡ ØµØ¹ÙˆØ¯ÛŒ."
    },
    "Resistance_Broken": {
        "weight": 5,
        "description": "Ø´Ú©Ø³Øª ÛŒÚ© Ø³Ø·Ø­ Ù…Ù‚Ø§ÙˆÙ…Øª Ú©Ù„ÛŒØ¯ÛŒØŒ Ù†Ø´Ø§Ù†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ù‚Ø¯Ø±Øª Ø®Ø±ÛŒØ¯Ø§Ø±Ø§Ù† Ùˆ Ù¾ØªØ§Ù†Ø³ÛŒÙ„ Ø´Ø±ÙˆØ¹ ÛŒÚ© Ø­Ø±Ú©Øª ØµØ¹ÙˆØ¯ÛŒ Ø¬Ø¯ÛŒØ¯."
    },
    "Static_Resistance_Broken": {
        "weight": 5,
        "description": "Ø´Ú©Ø³Øª ÛŒÚ© Ù…Ù‚Ø§ÙˆÙ…Øª Ù…Ù‡Ù… Ø§Ø³ØªØ§ØªÛŒÚ© (Ú©Ù„Ø§Ø³ÛŒÚ©)ØŒ Ø³ÛŒÚ¯Ù†Ø§Ù„ÛŒ Ø¨Ø³ÛŒØ§Ø± Ù…Ø¹ØªØ¨Ø± Ø¨Ø±Ø§ÛŒ Ø§Ø¯Ø§Ù…Ù‡ Ø±Ø´Ø¯."
    },
    "Squeeze_Momentum_Fired_Long": {
        "weight": 4,
        "description": "Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± Squeeze Momentum Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø®Ø±ÛŒØ¯ (Ø®Ø±ÙˆØ¬ Ø§Ø² ÙØ´Ø±Ø¯Ú¯ÛŒ) ØµØ§Ø¯Ø± Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª Ú©Ù‡ Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡ Ø§Ø­ØªÙ…Ø§Ù„ ÛŒÚ© Ø­Ø±Ú©Øª Ø§Ù†ÙØ¬Ø§Ø±ÛŒ Ø§Ø³Øª."
    },
    "Stochastic_Bullish_Cross_Oversold": {
        "weight": 4,
        "description": "ØªÙ‚Ø§Ø·Ø¹ ØµØ¹ÙˆØ¯ÛŒ Ø¯Ø± Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± Ø§Ø³ØªÙˆÚ©Ø§Ø³ØªÛŒÚ© Ø¯Ø± Ù†Ø§Ø­ÛŒÙ‡ Ø§Ø´Ø¨Ø§Ø¹ ÙØ±ÙˆØ´ØŒ ÛŒÚ© Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø®Ø±ÛŒØ¯ Ú©Ù„Ø§Ø³ÛŒÚ© Ùˆ Ù…Ø¹ØªØ¨Ø±."
    },
    "Consolidation_Breakout_Candidate": {
        "weight": 3,
        "description": "Ø³Ù‡Ù… Ø¯Ø± ÙØ§Ø² ØªØ±Ø§Ú©Ù… Ùˆ Ù†ÙˆØ³Ø§Ù† Ú©Ù… Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡ ÛŒÚ© Ø­Ø±Ú©Øª Ù‚ÙˆÛŒ (Ø´Ú©Ø³Øª) Ø§Ø³Øª."
    },
    "Near_Major_Static_Support": {
        "weight": 5, 
        "description": "Ù†Ø²Ø¯ÛŒÚ©ÛŒ Ø¨Ù‡ Ø­Ù…Ø§ÛŒØª Ø§ØµÙ„ÛŒ: Ù‚ÛŒÙ…Øª Ø¯Ø± Ù…Ø­Ø¯ÙˆØ¯Ù‡ 0% ØªØ§ 3% Ø¨Ø§Ù„Ø§ÛŒ Ø³Ø·Ø­ Ø­Ù…Ø§ÛŒØª Ø§Ø³ØªØ§ØªÛŒÚ© Ú©Ù„ÛŒØ¯ÛŒ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯ (ÙˆØ±ÙˆØ¯ Ø¨Ø§ Ø±ÛŒØ³Ú© Ú©Ù…)."
    },
    "Bollinger_Lower_Band_Touch": {
        "weight": 1,
        "description": "Ù‚ÛŒÙ…Øª Ø¨Ù‡ Ø¨Ø§Ù†Ø¯ Ù¾Ø§ÛŒÛŒÙ† Ø¨ÙˆÙ„ÛŒÙ†Ú¯Ø± Ø¨Ø±Ø®ÙˆØ±Ø¯ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª Ú©Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ù†Ø´Ø§Ù†Ù‡ Ø¨Ø§Ø²Ú¯Ø´Øª Ú©ÙˆØªØ§Ù‡â€ŒÙ…Ø¯Øª Ù‚ÛŒÙ…Øª Ø¨Ø§Ø´Ø¯."
    },

    # --- Trend Confirmation & Strength Signals ---
    "IsInLeadingSector": {
        "weight": 4,
        "description": "Ø³Ù‡Ù… Ù…ØªØ¹Ù„Ù‚ Ø¨Ù‡ ÛŒÚ©ÛŒ Ø§Ø² ØµÙ†Ø§ÛŒØ¹ Ù¾ÛŒØ´Ø±Ùˆ Ùˆ Ù…ÙˆØ±Ø¯ ØªÙˆØ¬Ù‡ Ø¨Ø§Ø²Ø§Ø± Ø§Ø³Øª Ú©Ù‡ Ø´Ø§Ù†Ø³ Ù…ÙˆÙÙ‚ÛŒØª Ø±Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯."
    },
    "Strong_Uptrend_Confirmed": {
        "weight": 3,
        "description": "ØªØ§ÛŒÛŒØ¯ Ø±ÙˆÙ†Ø¯ ØµØ¹ÙˆØ¯ÛŒ Ù‚ÙˆÛŒ: SMA_20 > SMA_50 Ùˆ Ù‚ÛŒÙ…Øª Ø¨Ø§Ù„Ø§ØªØ± Ø§Ø² SMA_20 Ø§Ø³Øª."
    },
    "Buy_The_Dip_SMA50": {
        "weight": 3,
        "description": "Ù¾Ø§Ø¯Ø§Ø´ Ù¾ÙˆÙ„Ø¨Ú©: Ù‚ÛŒÙ…Øª Ø¯Ø± ÛŒÚ© Ø±ÙˆÙ†Ø¯ ØµØ¹ÙˆØ¯ÛŒØŒ Ø¨Ù‡ SMA_50 Ù¾ÙˆÙ„Ø¨Ú© Ø²Ø¯Ù‡ (Ø¯Ø± Ù…Ø­Ø¯ÙˆØ¯Ù‡ 0% ØªØ§ 3% Ø¨Ø§Ù„Ø§ÛŒ Ø¢Ù†)."
    },
    "Positive_Real_Money_Flow_Trend_10D": {
        "weight": 3,
        "description": "Ø¨Ø±Ø¢ÛŒÙ†Ø¯ ÙˆØ±ÙˆØ¯ Ù¾ÙˆÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯ (Ø­Ù‚ÛŒÙ‚ÛŒ) Ø¯Ø± Û±Û° Ø±ÙˆØ² Ú¯Ø°Ø´ØªÙ‡ Ù…Ø«Ø¨Øª Ø¨ÙˆØ¯Ù‡ Ø§Ø³Øª."
    },
    "Heavy_Individual_Buy_Pressure": {
        "weight": 3,
        "description": "Ø³Ø±Ø§Ù†Ù‡ Ø®Ø±ÛŒØ¯ Ø­Ù‚ÛŒÙ‚ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± Ø±ÙˆØ² Ø¢Ø®Ø± Ø¨Ù‡ Ø·ÙˆØ± Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ÛŒ Ø¨Ø§Ù„Ø§ØªØ± Ø§Ø² Ø³Ø±Ø§Ù†Ù‡ ÙØ±ÙˆØ´ Ø¨ÙˆØ¯Ù‡ Ø§Ø³Øª (ÙˆØ±ÙˆØ¯ Ù¾ÙˆÙ„ Ø³Ù†Ú¯ÛŒÙ†)."
    },
    "MACD_Bullish_Cross_Confirmed": {
        "weight": 2,
        "description": "Ø®Ø· MACD Ø®Ø· Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù‡ Ø³Ù…Øª Ø¨Ø§Ù„Ø§ Ù‚Ø·Ø¹ Ú©Ø±Ø¯Ù‡ Ú©Ù‡ ØªØ§ÛŒÛŒØ¯ÛŒ Ø¨Ø± Ø´Ø±ÙˆØ¹ Ø±ÙˆÙ†Ø¯ ØµØ¹ÙˆØ¯ÛŒ Ø§Ø³Øª."
    },
    "HalfTrend_Buy_Signal": {
        "weight": 2,
        "description": "Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± HalfTrend Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø®Ø±ÛŒØ¯ ØµØ§Ø¯Ø± Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª."
    },
    "High_Volume_On_Up_Day": {
        "weight": 2,
        "description": "Ø­Ø¬Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¯Ø± ÛŒÚ© Ø±ÙˆØ² Ù…Ø«Ø¨Øª Ø¨Ù‡ Ø·ÙˆØ± Ù…Ø¹Ù†Ø§Ø¯Ø§Ø±ÛŒ Ø§ÙØ²Ø§ÛŒØ´ ÛŒØ§ÙØªÙ‡ Ú©Ù‡ Ù†Ø´Ø§Ù† Ø§Ø² Ø­Ù…Ø§ÛŒØª Ù‚ÙˆÛŒ Ø§Ø² Ø±Ø´Ø¯ Ù‚ÛŒÙ…Øª Ø¯Ø§Ø±Ø¯ (Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ† ØµÙ)."
    },
    "Volume_MA_Is_Rising": { # ğŸ’¡ G-Volume: Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡
        "weight": 2,
        "description": "Ø±ÙˆÙ†Ø¯ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø­Ø¬Ù…: Ø´ÛŒØ¨ (Slope) Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø­Ø¬Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª (20 Ø±ÙˆØ²Ù‡) ØµØ¹ÙˆØ¯ÛŒ Ø§Ø³Øª."
    },

    # --- ğŸ’¡ G-Fundamental: Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† ÙÛŒÙ„ØªØ± Ø³Ø§Ø¯Ù‡ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„ ---
    "Reasonable_PE": {
        "weight": 1, 
        "description": "Ù†Ø³Ø¨Øª P/E Ø³Ù‡Ù… Ú©Ù…ØªØ± Ø§Ø² 15 Ø§Ø³Øª Ùˆ Ø³Ù‡Ù… Ø­Ø¨Ø§Ø¨ Ù‚ÛŒÙ…ØªÛŒ Ù†Ø¯Ø§Ø±Ø¯."
    },
    
    # --- ML Prediction Filter ---
    "ML_Predicts_Uptrend": {
        "weight": 2,
        "description": "Ù…Ø¯Ù„ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†ØŒ Ø§Ø­ØªÙ…Ø§Ù„ Ø¨Ø§Ù„Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ø±ÙˆÙ†Ø¯ ØµØ¹ÙˆØ¯ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª."
    },

    # --- Penalties & Negative Scores (Crucial for avoiding peaks) ---
    "Price_Below_SMA200": { # ğŸ’¡ G-Trend: ÙÛŒÙ„ØªØ± Ø¬Ø¯ÛŒØ¯ Ø±ÙˆÙ†Ø¯ Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª
        "weight": -7,
        "description": "Ø¬Ø±ÛŒÙ…Ù‡ Ø±ÙˆÙ†Ø¯ Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª: Ù‚ÛŒÙ…Øª Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ± Ø§Ø² Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú© Û²Û°Û° Ø±ÙˆØ²Ù‡ Ø§Ø³Øª."
    },
    "Strong_Downtrend_Confirmed": {
        "weight": -4,
        "description": "Ø¬Ø±ÛŒÙ…Ù‡ Ø±ÙˆÙ†Ø¯ Ù†Ø²ÙˆÙ„ÛŒ: SMA_20 Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ± Ø§Ø² SMA_50 Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯."
    },
    "MACD_Negative_Divergence": {
        "weight": -6,
        "description": "Ø¬Ø±ÛŒÙ…Ù‡ ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ù…Ù†ÙÛŒ: Ù‚ÛŒÙ…Øª Ø³Ù‚Ù Ø¬Ø¯ÛŒØ¯ Ø²Ø¯Ù‡ Ø§Ù…Ø§ MACD Ø³Ù‚Ù Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ±ÛŒ Ø«Ø¨Øª Ú©Ø±Ø¯Ù‡ (Ù†Ø´Ø§Ù†Ù‡ Ø¶Ø¹Ù Ø´Ø¯ÛŒØ¯ Ø±ÙˆÙ†Ø¯)."
    },
    "RSI_Is_Overbought": {
        "weight": -8,
        "description": "Ø¬Ø±ÛŒÙ…Ù‡ Ø§Ø´Ø¨Ø§Ø¹ Ø®Ø±ÛŒØ¯: RSI Ø¯Ø± Ù†Ø§Ø­ÛŒÙ‡ Ø§Ø´Ø¨Ø§Ø¹ Ø®Ø±ÛŒØ¯ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯ Ú©Ù‡ Ø±ÛŒØ³Ú© Ø§ØµÙ„Ø§Ø­ Ù‚ÛŒÙ…Øª Ø±Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯."
    },
    "Price_Too_Stretched_From_SMA50": {
        "weight": -5,
        "description": "Ø¬Ø±ÛŒÙ…Ù‡ ÙØ§ØµÙ„Ù‡ Ø²ÛŒØ§Ø¯: Ù‚ÛŒÙ…Øª ÙØ§ØµÙ„Ù‡ Ø²ÛŒØ§Ø¯ÛŒ Ø§Ø² Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú© ÛµÛ° Ø±ÙˆØ²Ù‡ Ú¯Ø±ÙØªÙ‡ Ú©Ù‡ Ø§Ø­ØªÙ…Ø§Ù„ Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø±Ø§ Ø¨Ø§Ù„Ø§ Ù…ÛŒâ€ŒØ¨Ø±Ø¯."
    },
    "Negative_Real_Money_Flow_Trend_10D": {
        "weight": -3,
        "description": "Ø¬Ø±ÛŒÙ…Ù‡ Ø®Ø±ÙˆØ¬ Ù¾ÙˆÙ„: Ø¨Ø±Ø¢ÛŒÙ†Ø¯ ÙˆØ±ÙˆØ¯ Ù¾ÙˆÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¯Ø± Û±Û° Ø±ÙˆØ² Ú¯Ø°Ø´ØªÙ‡ Ù…Ù†ÙÛŒ Ø¨ÙˆØ¯Ù‡ Ø§Ø³Øª (Ø®Ø±ÙˆØ¬ Ù¾ÙˆÙ„)."
    },
    "Signal_Against_Market_Trend": {
        "weight": -2,
        "description": "Ø¬Ø±ÛŒÙ…Ù‡ Ø®Ù„Ø§Ù Ø¬Ù‡Øª Ø¨Ø§Ø²Ø§Ø±: Ø³ÛŒÚ¯Ù†Ø§Ù„ ØµØ¹ÙˆØ¯ÛŒ (Ù…Ø«Ù„ MACD Cross) Ø¯Ø± ÛŒÚ© Ø¨Ø§Ø²Ø§Ø± Ø®Ø±Ø³ÛŒ ØµØ§Ø¯Ø± Ø´Ø¯Ù‡ Ø§Ø³Øª."
    },
    "Break_Below_Static_Support": { 
        "weight": -8, 
        "description": "Ø´Ú©Ø³Øª Ø­Ù…Ø§ÛŒØª Ø§ØµÙ„ÛŒ: Ù‚ÛŒÙ…Øª Ø¨Ù‡ Ø²ÛŒØ± Ø³Ø·Ø­ Ø­Ù…Ø§ÛŒØª Ø§Ø³ØªØ§ØªÛŒÚ© Ú©Ù„ÛŒØ¯ÛŒ Ø³Ù‚ÙˆØ· Ú©Ø±Ø¯Ù‡ Ú©Ù‡ Ù†Ø´Ø§Ù†Ù‡ Ø¶Ø¹Ù Ø´Ø¯ÛŒØ¯ ÛŒØ§ ØªØºÛŒÛŒØ± Ø±ÙˆÙ†Ø¯ Ø§Ø³Øª."
    },
    
    # --- ğŸ’¡ NEW FILTERS ADDED HERE (Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø´Ù…Ø§) ---
    "Price_Near_50Day_High": {
        "weight": -4, # Ø¬Ø±ÛŒÙ…Ù‡ Ù…ØªÙˆØ³Ø·
        "description": "Ø¬Ø±ÛŒÙ…Ù‡ Ù†Ø²Ø¯ÛŒÚ©ÛŒ Ø¨Ù‡ Ø³Ù‚Ù ÛµÛ° Ø±ÙˆØ²Ù‡: Ù‚ÛŒÙ…Øª Ø¨Ø³ÛŒØ§Ø± Ù†Ø²Ø¯ÛŒÚ© Ø¨Ù‡ Ø³Ù‚Ù Ø§Ø®ÛŒØ± Ø§Ø³Øª (Ú©Ù…ØªØ± Ø§Ø² ÛµÙª ÙØ§ØµÙ„Ù‡) Ùˆ Ù¾ØªØ§Ù†Ø³ÛŒÙ„ Ø§ØµÙ„Ø§Ø­ Ø¯Ø§Ø±Ø¯."
    },
    "Price_Far_From_Static_Support_160D": {
        "weight": -8, # Ø¬Ø±ÛŒÙ…Ù‡ Ø³Ù†Ú¯ÛŒÙ†
        "description": "Ø¬Ø±ÛŒÙ…Ù‡ ÙØ§ØµÙ„Ù‡ Ø²ÛŒØ§Ø¯ Ø§Ø² Ø­Ù…Ø§ÛŒØª Û±Û¶Û° Ø±ÙˆØ²Ù‡: Ù‚ÛŒÙ…Øª Ø¨ÛŒØ´ Ø§Ø² Û²ÛµÙª Ø¨Ø§Ù„Ø§ØªØ± Ø§Ø² Ø­Ù…Ø§ÛŒØª Ø§Ø³ØªØ§ØªÛŒÚ© Û±Û¶Û° Ø±ÙˆØ²Ù‡ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯ (Ø®Ø·Ø± Ø­Ø¨Ø§Ø¨)."
    }
}


# Ú©Ù…Ú©â€ŒÚ©Ù†Ù†Ø¯Ù‡: Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† ÛŒÚ© Ø³Ø±ÛŒ close Ù‚Ø§Ø¨Ù„â€ŒØ§Ø¹ØªÙ…Ø§Ø¯ Ø§Ø² historical DF
def _get_close_series_from_hist_df(hist_df):
    """
    Accepts a historical dataframe and returns a numeric pandas Series of close prices.
    Tries common column names: 'close_price', 'close', 'final'
    """
    if hist_df is None or hist_df.empty:
        return pd.Series(dtype=float)

    for col in ['close_price', 'close', 'final']:
        if col in hist_df.columns:
            ser = pd.to_numeric(hist_df[col], errors='coerce').dropna()
            if not ser.empty:
                return ser
    return pd.Series(dtype=float)



# --- NEW: Market Sentiment Analysis Function ---
def _get_market_sentiment() -> str:
    """
    Determines short-term market sentiment based on the average daily change of major indices over the last 3 trading days.
    Uses data from DailyIndexData table.
    Returns: 'Bullish', 'Neutral', or 'Bearish'.
    """
    
    try:
        # Get last 3 distinct jdates
        last_jdates_query = db.session.query(DailyIndexData.jdate).distinct().order_by(DailyIndexData.jdate.desc()).limit(3).all()
        if not last_jdates_query:
            logger.warning("No index data available, defaulting to Neutral")
            return "Neutral"

        last_jdates = [row[0] for row in last_jdates_query]

        # ğŸ’¡ G-Sentiment: Ø§ÙØ²ÙˆØ¯Ù† 'TEDPIX' Ø¨Ù‡ Ù„ÛŒØ³Øª Ø¨Ø±Ø±Ø³ÛŒ
        index_types_to_check = ['Total_Index', 'Equal_Weighted_Index', 'TEDPIX']
        
        index_data = db.session.query(DailyIndexData).filter(
            DailyIndexData.jdate.in_(last_jdates),
            DailyIndexData.index_type.in_(index_types_to_check)
        ).all()

        if len(index_data) < 2:  # At least one per index
            logger.warning(f"Insufficient index data for types {index_types_to_check}, defaulting to Neutral")
            return "Neutral"

        # Group by index_type
        # ğŸ’¡ G-Sentiment: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² 'Total_Index' ÛŒØ§ 'TEDPIX' Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø´Ø§Ø®Øµ Ú©Ù„
        total_changes = [d.percent_change for d in index_data if d.index_type == 'Total_Index' or d.index_type == 'TEDPIX']
        equal_changes = [d.percent_change for d in index_data if d.index_type == 'Equal_Weighted_Index']

        # ğŸ’¡ FIX: Indentation error corrected
        if len(total_changes) < 1 or len(equal_changes) < 1:
            logger.warning("Missing data for one index type (Total or Equal), defaulting to Neutral")
            return "Neutral"

        avg_total = np.mean(total_changes)
        avg_equal = np.mean(equal_changes)

        logger.info(f"Market Sentiment Averages over last {len(last_jdates)} days: Total {avg_total:.2f}%, Equal {avg_equal:.2f}%")

        if avg_total > 0.3 and avg_equal > 0.3:
            logger.info(f"Market Sentiment: Bullish (Total: {avg_total:.2f}%, Equal: {avg_equal:.2f}%)")
            return "Bullish"
        
        elif avg_total < -0.3 and avg_equal < -0.3:
            logger.info(f"Market Sentiment: Bearish (Total: {avg_total:.2f}%, Equal: {avg_equal:.2f}%)")
            return "Bearish"
        else:
            logger.info(f"Market Sentiment: Neutral (Total: {avg_total:.2f}%, Equal: {avg_equal:.2f}%)")
            return "Neutral"
            
    # ğŸ’¡ FIX: Indentation error corrected (moved to be aligned with try)
    except Exception as e:
        logger.error(f"Error fetching market sentiment from DB: {e}, defaulting to Neutral")
        return "Neutral"


# --- REVISED: Filter Functions ---
def _check_market_condition_filters(hist_df, tech_df):
    """
    Checks for individual stock conditions like overbought state 
    or consolidation.
    Also includes the NEW 50-day High Proximity Penalty.
    """ 
    satisfied_filters, reason_parts = [], {"market_condition": []}
    if not is_data_sufficient(tech_df, 1) or not is_data_sufficient(hist_df, MIN_REQUIRED_HISTORY_DAYS):
        return satisfied_filters, reason_parts

    last_tech = tech_df.iloc[-1]
    close_ser = _get_close_series_from_hist_df(hist_df)
    if close_ser.empty:
        return satisfied_filters, reason_parts
    last_close = close_ser.iloc[-1]
    last_hist = hist_df.iloc[-1]

    # --- Check 1: RSI Overbought Condition (Penalize only if weak) ---
    if hasattr(last_tech, 'RSI') and last_tech.RSI is not None and last_tech.RSI > 70:
        is_negative_divergence = (
            hasattr(last_tech, 'RSI_Divergence') and last_tech.RSI_Divergence == "Negative"
        )
        historical_volume_series = hist_df.tail(10)['volume']
        average_volume = historical_volume_series.mean() if not historical_volume_series.empty else 0
        is_high_volume = last_hist['volume'] > average_volume * 1.5 if average_volume > 0 else False

        if is_negative_divergence or not is_high_volume:
            satisfied_filters.append("RSI_Is_Overbought")
            reason_parts["market_condition"].append(
                f"RSI ({last_tech.RSI:.2f}) overbought with weakness."
            )
        else:
            reason_parts["market_condition"].append(
                f"RSI ({last_tech.RSI:.2f}) overbought but supported by strong volume (no penalty)."
            )

    # --- Check 2 & 3: Price Stretch (Penalize) OR Buy The Dip (Reward) ---
    sma50 = _get_attr_safe(last_tech, 'SMA_50')
    sma20 = _get_attr_safe(last_tech, 'SMA_20')
    
    if sma50 is not None and sma50 > 0 and sma20 is not None:
        stretch_percent = ((last_close - sma50) / sma50) * 100
        is_uptrend = sma20 > sma50

        # ğŸ’¡ G-2: Ø¬Ø±ÛŒÙ…Ù‡ ÙØ§ØµÙ„Ù‡ Ø²ÛŒØ§Ø¯ (Ø´Ø±Ø·) 
        if stretch_percent > 15:
            satisfied_filters.append("Price_Too_Stretched_From_SMA50")
            reason_parts["market_condition"].append(
                f"Price is overextended ({stretch_percent:.1f}%) from SMA50."
            )
        
        # ğŸ’¡ G-2: Ù¾Ø§Ø¯Ø§Ø´ Ù¾ÙˆÙ„Ø¨Ú© (Ø´Ø±Ø·) 
        elif is_uptrend and 0 <= stretch_percent <= 3:
            satisfied_filters.append("Buy_The_Dip_SMA50")
            reason_parts["market_condition"].append(
                f"Pullback signal: Price is near SMA50 ({stretch_percent:.1f}%) in an uptrend."
            )

    # --- Check 4: Consolidation Pattern (Reward) ---
    if hasattr(last_tech, 'ATR'):
        atr_series = pd.to_numeric(tech_df['ATR'].dropna())
        if len(atr_series) > 30:
            recent_atr_avg = atr_series.tail(10).mean()
            historical_atr_avg = atr_series.tail(30).mean()
            if recent_atr_avg < (historical_atr_avg * 0.7):
                satisfied_filters.append("Consolidation_Breakout_Candidate")
                reason_parts["market_condition"].append(
                     "Stock is in a low-volatility consolidation phase."
                )

    # --- ğŸ’¡ NEW FILTER 1: High Price Proximity Penalty (50-day High) ---
    # Ù‡Ø¯Ù: Ø¬Ø±ÛŒÙ…Ù‡ Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ù‡ Ø³Ù‚Ù ÛµÛ° Ø±ÙˆØ²Ù‡ Ø¨Ø³ÛŒØ§Ø± Ù†Ø²Ø¯ÛŒÚ© Ù‡Ø³ØªÙ†Ø¯ (Gap > -5%)
    if len(close_ser) >= 50:
        recent_high_50d = close_ser.tail(50).max()
        if recent_high_50d > 0:
            gap_percent = ((last_close - recent_high_50d) / recent_high_50d) * 100
            
            # Ø§Ú¯Ø± ÙØ§ØµÙ„Ù‡ (Gap) Ø¨ÛŒØ´ØªØ± Ø§Ø² -5% Ø¨Ø§Ø´Ø¯ (ÛŒØ¹Ù†ÛŒ Ù…Ø«Ù„Ø§ -3%ØŒ -1% ÛŒØ§ 0%)
            if gap_percent > -5:
                satisfied_filters.append("Price_Near_50Day_High")
                reason_parts["market_condition"].append(
                    f"Penalty: Price is too close to 50-day high (Gap: {gap_percent:.1f}%)."
                )

    return satisfied_filters, reason_parts

# ØªØºÛŒÛŒØ± Ø¯Ø± is_data_sufficient: Ø§Ù†Ø¹Ø·Ø§Ùâ€ŒÙ¾Ø°ÛŒØ±ØªØ± Ùˆ Ù…Ù‚Ø§ÙˆÙ…â€ŒØªØ±
def is_data_sufficient(data_df, min_len):
    """
    Checks if the provided DataFrame is not empty and has at least min_len records.
    """
    if data_df is None or data_df.empty:
        return False
    return len(data_df) >= min_len

def convert_jalali_to_gregorian_timestamp(jdate_str):
    """
    Converts a Jalali date string (YYYY-MM-DD) to a pandas Timestamp (Gregorian).
    """
    if pd.notna(jdate_str) and isinstance(jdate_str, str):
        try:
            jy, jm, jd = map(int, jdate_str.split('-'))
            gregorian_date = jdatetime.date(jy, jm, jd).togregorian()
            return pd.Timestamp(gregorian_date)
        except ValueError:
            return pd.NaT
    return pd.NaT

# Helper function to safely get a value
def _get_attr_safe(rec, attr, default=None):
    val = getattr(rec, attr, default)
    if isinstance(val, (pd.Series, pd.DataFrame)):
        # ğŸ’¡ FIX: (Bug) Ø¨Ø§ÛŒØ¯ Ø¢Ø®Ø±ÛŒÙ† Ù…Ù‚Ø¯Ø§Ø± (iloc[-1]) Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù‡ Ø´ÙˆØ¯ØŒ Ù†Ù‡ Ø§ÙˆÙ„ÛŒÙ† (iloc[0])
        return val.iloc[-1] if not val.empty else default
    return val

# --- REFACTORED: Technical filters are broken down into smaller functions ---

# ğŸ’¡ G-Fix: ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ ØµØ­ÛŒØ­ ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ
def _find_divergence(price_series: pd.Series, indicator_series: pd.Series, lookback: int, check_type: str) -> bool:
    """
    ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ ÛŒØ§ÙØªÙ† ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ø³Ø§Ø¯Ù‡ (Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ù…Ù†Ø·Ù‚ Ø§Ø´ØªØ¨Ø§Ù‡ Ù‚Ø¨Ù„ÛŒ)
    check_type: 'positive_rsi' (RD+) ÛŒØ§ 'negative' (RD-)
    """
    if len(price_series) < lookback or len(indicator_series) < lookback:
        return False
        
    price_series = pd.to_numeric(price_series.tail(lookback), errors='coerce').dropna()
    indicator_series = pd.to_numeric(indicator_series.tail(lookback), errors='coerce').dropna()
    
    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ù‡Ù…â€ŒØ±Ø§Ø³ØªØ§ÛŒÛŒ Ø§ÛŒÙ†Ø¯Ú©Ø³â€ŒÙ‡Ø§ Ù¾Ø³ Ø§Ø² Ø­Ø°Ù NA
    common_index = price_series.index.intersection(indicator_series.index)
    
    # ğŸ’¡ FIX: Indentation error corrected
    if len(common_index) < lookback:
        return False # Ø¯Ø§Ø¯Ù‡ Ù…Ø´ØªØ±Ú© Ú©Ø§ÙÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯
         
    price_series = price_series.loc[common_index]
    indicator_series = indicator_series.loc[common_index]

    if price_series.empty or indicator_series.empty or len(price_series) < 2:
        return False

    last_price = price_series.iloc[-1]
    last_indicator = indicator_series.iloc[-1]
    
    lookback_prices = price_series.iloc[:-1]
    lookback_indicators = indicator_series.iloc[:-1]

    if lookback_prices.empty:
        return False

    try:
        if check_type == 'positive_rsi':
            # RD+: Ù‚ÛŒÙ…Øª Ú©Ù Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ± Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯ØŒ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± Ú©Ù Ø¨Ø§Ù„Ø§ØªØ± Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯.
            min_price_idx = lookback_prices.idxmin() # Fix: Indent
            min_price_val = lookback_prices.loc[min_price_idx]
            indicator_at_min_price = lookback_indicators.loc[min_price_idx]

            if pd.isna(min_price_val) or pd.isna(indicator_at_min_price) or pd.isna(last_price) or pd.isna(last_indicator):
                return False

            # Û±.
            price_makes_lower_low = last_price < min_price_val # Fix: Indent
            # Û².
            indicator_makes_higher_low = last_indicator > indicator_at_min_price # Fix: Indent
            # Û³.
            is_at_bottom = last_indicator < 50 # Fix: Indent
            
            return price_makes_lower_low and indicator_makes_higher_low and is_at_bottom

        elif check_type == 'negative':
            # RD-: Ù‚ÛŒÙ…Øª Ø³Ù‚Ù Ø¨Ø§Ù„Ø§ØªØ± Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯ØŒ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± Ø³Ù‚Ù Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ± Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯.
            max_price_idx = lookback_prices.idxmax() # Fix: Indent
            max_price_val = lookback_prices.loc[max_price_idx]
            indicator_at_max_price = lookback_indicators.loc[max_price_idx]

            if pd.isna(max_price_val) or pd.isna(indicator_at_max_price) or pd.isna(last_price) or pd.isna(last_indicator):
                return False

            # Û±.
            price_makes_higher_high = last_price > max_price_val # Fix: Indent
            # Û².
            indicator_makes_lower_high = last_indicator < indicator_at_max_price # Fix: Indent
            
            return price_makes_higher_high and indicator_makes_lower_high

    except Exception as e:
        # e.g., if series is empty or idxmin fails
        logger.warning(f"Divergence check failed: {e}")
        return False
        
    # ğŸ’¡ FIX: Indentation error corrected (aligned with the function body)
    return False


def _check_oscillator_signals(tech_df: pd.DataFrame, close_ser: pd.Series, technical_rec, prev_tech_rec):
    """
    Checks oscillator-based signals like RSI and Stochastic.
    ğŸ’¡ G-Fix: Ø´Ø§Ù…Ù„ Ù…Ø­Ø§Ø³Ø¨Ù‡ ØµØ­ÛŒØ­ ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ Ø¨Ø§ Ù†Ú¯Ø§Ù‡ Ø¨Ù‡ Ú¯Ø°Ø´ØªÙ‡ (Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ù…Ù†Ø·Ù‚ Ø§Ø´ØªØ¨Ø§Ù‡ Û² Ø±ÙˆØ²Ù‡).
    """
    satisfied_filters, reason_parts = [], {"technical": []}
    
    # ØªØ¹Ø±ÛŒÙ Ø¯ÙˆØ±Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ§Ú¯Ø±Ø§ÛŒÛŒ
    DIVERGENCE_LOOKBACK = 20 

    # --- RSI Positive Divergence ---
    rsi_series = tech_df['RSI'] if 'RSI' in tech_df.columns else pd.Series(dtype=float)
    if _find_divergence(close_ser, rsi_series, DIVERGENCE_LOOKBACK, 'positive_rsi'):
        current_rsi = _get_attr_safe(technical_rec, 'RSI')
        satisfied_filters.append("RSI_Positive_Divergence")
        reason_parts["technical"].append(f"Positive divergence on RSI ({current_rsi:.2f}).")

    
    # --- ğŸ’¡ G-4: MACD Negative Divergence (Penalty) ---
    macd_series = tech_df['MACD'] if 'MACD' in tech_df.columns else pd.Series(dtype=float)
        
    if _find_divergence(close_ser, macd_series, DIVERGENCE_LOOKBACK, 'negative'):
        current_macd = _get_attr_safe(technical_rec, 'MACD')
        satisfied_filters.append("MACD_Negative_Divergence")
        reason_parts["technical"].append(f"Negative divergence on MACD (Price up, MACD down).")

    # --- Stochastic Oscillator ---
    current_stoch_k = _get_attr_safe(technical_rec, 'Stochastic_K')
    current_stoch_d = _get_attr_safe(technical_rec, 'Stochastic_D')
    prev_stoch_k = _get_attr_safe(prev_tech_rec, 'Stochastic_K')
    prev_stoch_d = _get_attr_safe(prev_tech_rec, 'Stochastic_D')
    if all(x is not None for x in [current_stoch_k, current_stoch_d, prev_stoch_k, prev_stoch_d]):
        if current_stoch_k > current_stoch_d and prev_stoch_k <= prev_stoch_d and current_stoch_d < 25:
            satisfied_filters.append("Stochastic_Bullish_Cross_Oversold")
            reason_parts["technical"].append("Stochastic bullish cross in oversold area.")
            
    # ğŸ’¡ FIX: Indentation error corrected
    return satisfied_filters, reason_parts

def _check_trend_signals(technical_rec, prev_tech_rec, last_close_val, market_sentiment: str):
    """
    Checks trend-following signals.
    """
    satisfied_filters, reason_parts = [], {"technical": []}
    
    # --- ğŸ’¡ G-1: Trend State Definition (using SMA) ---
    sma20 = _get_attr_safe(technical_rec, 'SMA_20')
    sma50 = _get_attr_safe(technical_rec, 'SMA_50')
    is_uptrend = False

    if sma20 is not None and sma50 is not None:
        if sma20 > sma50:
            is_uptrend = True
            if last_close_val > sma20:
                satisfied_filters.append("Strong_Uptrend_Confirmed")
                reason_parts["technical"].append(f"Strong Uptrend (SMA20 > SMA50, Close > SMA20).")
        else:
            satisfied_filters.append("Strong_Downtrend_Confirmed") # Penalty
            reason_parts["technical"].append(f"Downtrend Confirmed (SMA20 < SMA50).")

    # --- ğŸ’¡ G-Trend: Long-Term Trend Penalty (SMA200) ---
    sma200 = _get_attr_safe(technical_rec, 'SMA_200') 
    # ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… SMA_200 Ù…Ø­Ø§Ø³Ø¨Ù‡ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡
    if sma200 is not None and last_close_val < sma200:
        satisfied_filters.append("Price_Below_SMA200")
        reason_parts["technical"].append(f"Penalty: Price is below long-term SMA200.")

    # --- MACD Cross ---
    current_macd = _get_attr_safe(technical_rec, 'MACD')
    current_macd_signal = _get_attr_safe(technical_rec, 'MACD_Signal')
    prev_macd = _get_attr_safe(prev_tech_rec, 'MACD')
    prev_macd_signal = _get_attr_safe(prev_tech_rec, 'MACD_Signal')
    macd_crossed = False
    
    if all(x is not None for x in [current_macd, current_macd_signal, prev_macd, prev_macd_signal]):
        # ğŸ’¡ FIX: Indentation error corrected
        if current_macd > current_macd_signal and prev_macd <= prev_macd_signal:
            satisfied_filters.append("MACD_Bullish_Cross_Confirmed")
            macd_crossed = True
        
    # --- HalfTrend ---
    current_halftrend = _get_attr_safe(technical_rec, 'halftrend_signal')
    prev_halftrend = _get_attr_safe(prev_tech_rec, 'halftrend_signal')
    halftrend_buy = False
    if current_halftrend == 1 and prev_halftrend != 1:
        satisfied_filters.append("HalfTrend_Buy_Signal")
        halftrend_buy = True
        
    # --- Dynamic Resistance Break (from model) ---
    resistance_broken = _get_attr_safe(technical_rec, 'resistance_broken')
    if resistance_broken:
        satisfied_filters.append("Resistance_Broken")
        res_level = _get_attr_safe(technical_rec, 'resistance_level_50d', 'N/A')
        reason_parts["technical"].append(f"Broke a key dynamic resistance level around {res_level}.")

    # --- ğŸ’¡ G-3: Market Sentiment Penalty (Expanded) ---
    if market_sentiment == "Bearish" and (macd_crossed or halftrend_buy or resistance_broken):
        satisfied_filters.append("Signal_Against_Market_Trend")
        reason_parts["technical"].append(f"Penalty: Bullish signal detected in a Bearish market.")

    return satisfied_filters, reason_parts

def _check_volatility_signals(hist_df, technical_rec, last_close_val):
    """Checks volatility-based signals like Bollinger Bands and Squeeze Momentum."""
    satisfied_filters, reason_parts = [], {"technical": []}

    # Bollinger Lower Band
    bollinger_low = _get_attr_safe(technical_rec, 'Bollinger_Low')
    if bollinger_low is not None and last_close_val < bollinger_low:
        satisfied_filters.append("Bollinger_Lower_Band_Touch")

    # Squeeze Momentum
    prev_tech_rec = hist_df.iloc[-2] if len(hist_df) > 1 else technical_rec
    current_squeeze_on = _get_attr_safe(technical_rec, 'squeeze_on')
    prev_squeeze_on = _get_attr_safe(prev_tech_rec, 'squeeze_on')
    if current_squeeze_on == False and prev_squeeze_on == True:
        satisfied_filters.append("Squeeze_Momentum_Fired_Long")
        reason_parts["technical"].append("Squeeze Momentum indicator fired long.")
        
    return satisfied_filters, reason_parts

def _check_volume_signals(hist_df, tech_df, technical_rec, close_ser):
    """
    Checks for volume-based signals including Z-Score (last day spike) 
    and Volume MA trend (sustained interest).
    """
    satisfied_filters, reason_parts = [], {"technical": []}
    
    # ğŸ’¡ G-Volume: Ú†Ú© Ú©Ø±Ø¯Ù† Ù‚ÙÙ„ ØµÙ (Locked Market)
    is_locked_market = False
    if not hist_df.empty:
        last_hist = hist_df.iloc[-1]
        if last_hist['high'] == last_hist['low']:
            is_locked_market = True
            reason_parts["technical"].append(f"Note: Market was locked (High == Low). Volume spikes ignored.")
     
    # 1. High Volume On Up Day (Z-Score)
    if 'volume' in hist_df.columns and len(hist_df) >= 20 and len(close_ser) > 1:
        volume_z_score = calculate_z_score(pd.to_numeric(hist_df['volume'], errors='coerce').dropna().iloc[-20:])
        # ğŸ’¡ G-Volume: Ø³ÛŒÚ¯Ù†Ø§Ù„ ÙÙ‚Ø· Ø¯Ø± ØµÙˆØ±ØªÛŒ ÙØ¹Ø§Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ ØµÙ Ù†Ø¨Ø§Ø´Ø¯
        if volume_z_score is not None and volume_z_score > 1.5 and close_ser.iloc[-1] > close_ser.iloc[-2] and not is_locked_market:
            satisfied_filters.append("High_Volume_On_Up_Day")
            reason_parts["technical"].append(f"High volume (Z-Score: {volume_z_score:.2f}) on a positive day.")

    # 2. ğŸ’¡ G-Volume: Volume MA Is Rising (Using Slope)
    if is_data_sufficient(tech_df, 10): # Ø­Ø¯Ø§Ù‚Ù„ Û±Û° Ø±ÙˆØ² Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´ÛŒØ¨
        try:
            # Û±Û° Ø¯ÛŒØªØ§ÛŒ Ø¢Ø®Ø± Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø­Ø¬Ù… Û²Û° Ø±ÙˆØ²Ù‡
            vol_ma_series = pd.to_numeric(tech_df['Volume_MA_20'].dropna().tail(10))
            if len(vol_ma_series) >= 5: # Ø­Ø¯Ø§Ù‚Ù„ Ûµ 
                x = np.arange(len(vol_ma_series))
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´ÛŒØ¨ Ø®Ø· Ø±Ú¯Ø±Ø³ÛŒÙˆÙ†
                slope = np.polyfit(x, vol_ma_series, 1)[0] 
                
                # Ø§Ú¯Ø± Ø´ÛŒØ¨ Ù…Ø«Ø¨Øª Ø¨Ø§Ø´Ø¯ 
                if slope > 0:
                    satisfied_filters.append("Volume_MA_Is_Rising")
                    reason_parts["technical"].append(f"Volume MA (20d) slope is positive ({slope:,.0f}).")
        except Exception as e:
            logger.warning(f"Could not calculate volume MA slope: {e}")
            pass
            
    # ğŸ’¡ FIX: Indentation error corrected
    return satisfied_filters, reason_parts

def _check_technical_filters(hist_df, tech_df, market_sentiment: str):
    """
    Checks all technical indicators by calling specialized sub-functions.
    (Coordinator function)
    """
    all_satisfied_filters, all_reason_parts = [], {"technical": []}
    if not is_data_sufficient(tech_df, 2) or not is_data_sufficient(hist_df, MIN_REQUIRED_HISTORY_DAYS):
        return all_satisfied_filters, all_reason_parts

    technical_rec = tech_df.iloc[-1]
    prev_tech_rec = tech_df.iloc[-2]
    
    close_ser = _get_close_series_from_hist_df(hist_df)
    last_close_val = close_ser.iloc[-1] if not close_ser.empty else None
    if last_close_val is None:
        return all_satisfied_filters, all_reason_parts

    # ğŸ’¡ FIX: Indent Error (Ø®Ø· Û³ÛµÛ°) (Ensuring the for loop is at the correct level)
    # Call sub-functions and aggregate results
    for func in [_check_oscillator_signals, _check_trend_signals, _check_volatility_signals, _check_volume_signals]:
        # Adjust arguments as needed per function signature
        
        # ğŸ’¡ G-Fix: Ø§ØµÙ„Ø§Ø­ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ ØªØ§Ø¨Ø¹ _check_oscillator_signals Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ tech_df Ú©Ø§Ù…Ù„
        if func == _check_oscillator_signals:
             satisfied, reasons = func(tech_df, close_ser, technical_rec, prev_tech_rec)
        elif func == _check_trend_signals:
             satisfied, reasons = func(technical_rec, prev_tech_rec, last_close_val, market_sentiment)
         
        elif func == _check_volatility_signals:
            satisfied, reasons = func(hist_df, technical_rec, last_close_val)
        
        elif func == _check_volume_signals:
            satisfied, reasons = func(hist_df, tech_df, technical_rec, close_ser)
   
        else:
            continue

        # ğŸ’¡ FIX: Indentation error corrected (this belongs in the loop)
        all_satisfied_filters.extend(satisfied) # Fix: Indent
        if "technical" in reasons:
            all_reason_parts.setdefault("technical", []).extend(reasons["technical"]) # Fix: Indent

    return all_satisfied_filters, all_reason_parts

# --- END REFACTORED SECTION ---

# ğŸ’¡ G-Fundamental: ØªØ§Ø¨Ø¹ Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù‡ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„ØªØ± Ø³Ø§Ø¯Ù‡ P/E
def _check_simple_fundamental_filters(symbol_data_rec):
    """
    Ø¨Ø±Ø±Ø³ÛŒ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø³Ø§Ø¯Ù‡ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„ (P/E) Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø³Ù‡Ø§Ù… Ø­Ø¨Ø§Ø¨ÛŒ.
    """
    satisfied_filters, reason_parts = [], {"fundamental": []}
    if symbol_data_rec:
        # pe_ratio Ø§Ø² ComprehensiveSymbolData Ø®ÙˆØ§Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
        pe = getattr(symbol_data_rec, 'pe_ratio', None)
        if pe is not None and 0 < pe < 15:
            satisfied_filters.append("Reasonable_PE")
            reason_parts["fundamental"].append(f"P/E ({pe:.2f}) is reasonable (< 15).")
    return satisfied_filters, reason_parts

def _check_smart_money_filters(hist_df):
    """REVISED: Now also checks for heavy individual buy pressure on the last day."""
    satisfied_filters = []
    reason_parts = {"smart_money": []}
    trend_lookback = 10
    if hist_df is None or hist_df.empty or 'buy_i_volume' not in hist_df.columns:
        return satisfied_filters, reason_parts

    # Check 1: 10-Day Real Money Flow Trend
    if len(hist_df) >= trend_lookback:
        smart_money_flow_df = calculate_smart_money_flow(hist_df)
        if not smart_money_flow_df.empty and len(smart_money_flow_df) >= trend_lookback:
            trend_net_flow = smart_money_flow_df['individual_net_flow'].iloc[-trend_lookback:].sum()
            if trend_net_flow > 0:
                satisfied_filters.append("Positive_Real_Money_Flow_Trend_10D")
                reason_parts["smart_money"].append(f"Positive real money inflow over the last {trend_lookback} days.")
            elif trend_net_flow < 0:
                satisfied_filters.append("Negative_Real_Money_Flow_Trend_10D")
                reason_parts["smart_money"].append(f"Negative real money outflow over the last {trend_lookback} days.")

    #NEW Check 2: Heavy Individual Buy Pressure on the last day
    last_day = hist_df.iloc[-1]
    required_cols = ['buy_i_volume', 'buy_count_i', 'sell_i_volume', 'sell_count_i']
    if all(col in last_day and pd.notna(last_day[col]) for col in required_cols):
        if last_day['buy_count_i'] > 0 and last_day['sell_count_i'] > 0:
            per_capita_buy = last_day['buy_i_volume'] / last_day['buy_count_i']
            per_capita_sell = last_day['sell_i_volume'] / last_day['sell_count_i']
            # Check if per capita buy is 2.5x greater than sell
            if per_capita_buy > (per_capita_sell * 2.5): 
                satisfied_filters.append("Heavy_Individual_Buy_Pressure")
                reason_parts["smart_money"].append(f"Significant buy pressure detected (Per capita buy: {per_capita_buy:,.0f} vs sell: {per_capita_sell:,.0f}).")
    return satisfied_filters, reason_parts

def _check_power_thrust_signal(hist_df, close_ser):
    """
    Checks for a Power Thrust Signal: a combination of high volume,
    heavy individual buy pressure, and a positive closing day.
    """
    satisfied_filters, reason_parts = [], {"power_thrust": []}
    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø¯Ù‡ Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„
    if hist_df is None or len(hist_df) < 20 or close_ser.empty or len(close_ser) < 2:
        return satisfied_filters, reason_parts

    last_day = hist_df.iloc[-1]
    # --- ğŸ’¡ G-Volume: Ú†Ú© Ú©Ø±Ø¯Ù† Ù‚ÙÙ„ ØµÙ ---
    is_locked_market = last_day['high'] == last_day['low']
    if is_locked_market:
        return satisfied_filters, reason_parts # Ø³ÛŒÚ¯Ù†Ø§Ù„ Ù‚Ø¯Ø±Øª Ø¯Ø± ØµÙ Ø®Ø±ÛŒØ¯/ÙØ±ÙˆØ´ØŒ Ù…Ø¹ØªØ¨Ø± Ù†ÛŒØ³Øª

    
    # --- Ø´Ø±Ø· Û±: Ø±ÙˆØ² Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ Ù…Ø«Ø¨Øª ---
    is_up_day = close_ser.iloc[-1] > close_ser.iloc[-2]
    if not is_up_day:
        return satisfied_filters, reason_parts # Ø§Ú¯Ø± Ø±ÙˆØ² Ù…Ø«Ø¨Øª Ù†ÛŒØ³ØªØŒ Ø§Ø¯Ø§Ù…Ù‡ Ù†Ø¯Ù‡

    # --- Ø´Ø±Ø· Û²: Ø­Ø¬Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¨Ø³ÛŒØ§Ø± Ø¨Ø§Ù„Ø§ 
    # (Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Z-Score) ---
    volume_series = pd.to_numeric(hist_df['volume'], errors='coerce').dropna().tail(20)
    volume_z_score = calculate_z_score(volume_series)
    # Ø¢Ø³ØªØ§Ù†Ù‡ Ø¨Ø§Ù„Ø§ Ø¨Ø±Ø§ÛŒ Ø­Ø¬Ù…ØŒ Ù…Ø«Ù„Ø§ Z-Score Ø¨ÛŒØ´ØªØ± Ø§Ø² 1.8
    is_high_volume = volume_z_score is not None and volume_z_score > 1.8

    # --- Ø´Ø±Ø· Û³: ÙØ´Ø§Ø± Ø®Ø±ÛŒØ¯ Ø³Ù†Ú¯ÛŒÙ† Ø­Ù‚ÛŒÙ‚ÛŒâ€ŒÙ‡Ø§ ---
    is_heavy_buy_pressure = False
    required_cols = ['buy_i_volume', 'buy_count_i', 'sell_i_volume', 'sell_count_i']
    if all(col in last_day and pd.notna(last_day[col]) for col in required_cols):
        if last_day['buy_count_i'] > 0 and last_day['sell_count_i'] > 0:
            per_capita_buy = last_day['buy_i_volume'] / last_day['buy_count_i']
            per_capita_sell = last_day['sell_i_volume'] / last_day['sell_count_i']
            # Ø¢Ø³ØªØ§Ù†Ù‡ Ø¨Ø§Ù„Ø§ Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø§Ù†Ù‡ Ø®Ø±ÛŒØ¯ØŒ Ù…Ø«Ù„Ø§ Û².Ûµ Ø¨Ø±Ø§Ø¨Ø± Ø³Ø±Ø§Ù†Ù‡ ÙØ±ÙˆØ´
            if per_capita_buy > (per_capita_sell * 2.5): 
                is_heavy_buy_pressure = True

    # --- ØªØ±Ú©ÛŒØ¨ Ù†Ù‡Ø§ÛŒÛŒ ---
    if is_high_volume and is_heavy_buy_pressure:
        satisfied_filters.append("Power_Thrust_Signal")
        reason_parts["power_thrust"].append(
            f"Power signal detected! High Volume (Z-Score: {volume_z_score:.2f}) and Heavy Buy Pressure."
        )
    return satisfied_filters, reason_parts

# --- NEW: Functions for new strategic filters ---
def _get_leading_sectors():
    """ 
    Ø¨Ø§ Ú©ÙˆØ¦Ø±ÛŒ Ø¨Ù‡ Ø¬Ø¯ÙˆÙ„ DailySectorPerformanceØŒ Ù„ÛŒØ³Øª ØµÙ†Ø§ÛŒØ¹ Ù¾ÛŒØ´Ø±Ùˆ (Ù…Ø«Ù„Ø§Ù‹ 4 ØµÙ†Ø¹Øª Ø¨Ø±ØªØ±) Ø¯Ø± Ø¢Ø®Ø±ÛŒÙ† Ø±ÙˆØ² ØªØ­Ù„ÛŒÙ„ Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
    """
    try:
        latest_date_query = db.session.query(func.max(DailySectorPerformance.jdate)).scalar()
        if not latest_date_query:
            logger.warning("Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¯Ø± Ø¬Ø¯ÙˆÙ„ ØªØ­Ù„ÛŒÙ„ ØµÙ†Ø§ÛŒØ¹ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø§Ø² Ù„ÛŒØ³Øª Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
            return {"Ø®ÙˆØ¯Ø±Ùˆ Ùˆ Ø³Ø§Ø®Øª Ù‚Ø·Ø¹Ø§Øª"} # Fallback

        # Ø¯Ø±ÛŒØ§ÙØª 4 ØµÙ†Ø¹Øª Ø¨Ø±ØªØ± Ø¯Ø± Ø¢Ø®Ø±ÛŒÙ† Ø±ÙˆØ²
        leading_sectors_query = db.session.query(DailySectorPerformance.sector_name)\
            .filter(DailySectorPerformance.jdate == latest_date_query)\
            .order_by(DailySectorPerformance.rank.asc())\
            .limit(4).all()
        leading_sectors = {row[0] for row in leading_sectors_query}
        logger.info(f"ØµÙ†Ø§ÛŒØ¹ Ù¾ÛŒØ´Ø±Ùˆ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡ Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {leading_sectors}")
        return leading_sectors
    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª ØµÙ†Ø§ÛŒØ¹ Ù¾ÛŒØ´Ø±Ùˆ Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {e}")
        # ğŸ’¡ FIX: Indentation error corrected
        return {"Ø®ÙˆØ¯Ø±Ùˆ Ùˆ Ø³Ø§Ø®Øª Ù‚Ø·Ø¹Ø§Øª"} # Fallback in case of error

def _check_sector_strength_filter(symbol_sector, leading_sectors):
    """Checks if the symbol belongs to a leading sector."""
    satisfied_filters, reason_parts = [], {"sector_strength": []}
    if symbol_sector in leading_sectors:
        satisfied_filters.append("IsInLeadingSector")
        reason_parts["sector_strength"].append(f"Symbol is in a leading sector: {symbol_sector}.")
    return satisfied_filters, reason_parts

def _check_static_levels_filters(hist_df: pd.DataFrame, technical_rec: pd.Series, last_close_val: float) -> Tuple[List[str], Dict[str, List[str]]]:
    """ 
    ØªÙ„ÙÛŒÙ‚ Ù…Ù†Ø·Ù‚: Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯ÛŒÙ†Ø§Ù…ÛŒÚ© Ø­Ù…Ø§ÛŒØª Ø§Ø³ØªØ§ØªÛŒÚ© Ùˆ Ø¨Ø±Ø±Ø³ÛŒ Ù…Ù‚Ø§ÙˆÙ…Øª Ø§Ø³ØªØ§ØªÛŒÚ© Ø§Ø² Ù¾ÛŒØ´ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¯Ù‡.
    """
    satisfied_filters, reason_parts = [], {"static_levels": []}

    # --- A. Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯ÛŒÙ†Ø§Ù…ÛŒÚ© Ø­Ù…Ø§ÛŒØª Ø§Ø³ØªØ§ØªÛŒÚ© --- (Ù…Ù†Ø·Ù‚ Ø¬Ø¯ÛŒØ¯)
    # Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø±Ø§ ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú©Ù‡ Ø¨Ø§ DataFrame Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    MAJOR_STATIC_SUPPORT = calculate_static_support(hist_df, lookback_period=120)

    if MAJOR_STATIC_SUPPORT is not None:
        reason_parts["static_levels"].append(f"Ø­Ù…Ø§ÛŒØª Ø§Ø³ØªØ§ØªÛŒÚ© Ø§ØµÙ„ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡: {MAJOR_STATIC_SUPPORT:,.0f}")
        
        # ÙÛŒÙ„ØªØ± Ù†Ø²Ø¯ÛŒÚ©ÛŒ Ø¨Ù‡ Ø­Ù…Ø§ÛŒØª (Near_Major_Static_Support)
        PROXIMITY_THRESHOLD = 0.07 # 7%
        distance_from_support = ((last_close_val - MAJOR_STATIC_SUPPORT) / MAJOR_STATIC_SUPPORT) * 100
        
        if 0 <= distance_from_support <= PROXIMITY_THRESHOLD * 100:
            satisfied_filters.append("Near_Major_Static_Support") # Ù†Ø§Ù… ÙÛŒÙ„ØªØ± Ø¬Ø¯ÛŒØ¯
            
        # ÙÛŒÙ„ØªØ± Ø¬Ø±ÛŒÙ…Ù‡ Ø´Ú©Ø³Øª Ø­Ù…Ø§ÛŒØª (Break_Below_Static_Support)
        if last_close_val < MAJOR_STATIC_SUPPORT:
            satisfied_filters.append("Break_Below_Static_Support") # Ù†Ø§Ù… ÙÛŒÙ„ØªØ± Ø¬Ø±ÛŒÙ…Ù‡

    # --- B. Ø¨Ø±Ø±Ø³ÛŒ Ù…Ù‚Ø§ÙˆÙ…Øª Ø§Ø³ØªØ§ØªÛŒÚ© (Ù…Ù†Ø·Ù‚ Ù‚Ø¯ÛŒÙ…ÛŒ Ø´Ù…Ø§) ---
    # ğŸ“¢ ØªÙˆØ¬Ù‡: Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù‡Ù…Ú†Ù†Ø§Ù† Ù†ÛŒØ§Ø² Ø¨Ù‡ ÙÛŒÙ„Ø¯ 'static_resistance_level' Ø¯Ø± technical_rec Ø¯Ø§Ø±Ø¯.
    static_resistance = _get_attr_safe(technical_rec, 'static_resistance_level')
    if static_resistance is not None and static_resistance > 0:
        distance_from_resistance = ((last_close_val - static_resistance) / static_resistance) * 100
        if distance_from_resistance > 1:
            satisfied_filters.append("Static_Resistance_Broken")
            reason_parts["static_levels"].append(f"Static resistance broken ({static_resistance:,.0f}, {distance_from_resistance:.1f}% above).")

    # --- ğŸ’¡ NEW FILTER 2: Static Support Distance Penalty (160D) ---
    # Ù‡Ø¯Ù: Ø¬Ø±ÛŒÙ…Ù‡ Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒÛŒ Ú©Ù‡ ÙØ§ØµÙ„Ù‡ Ø¨Ø³ÛŒØ§Ø± Ø²ÛŒØ§Ø¯ÛŒ (Ø¨ÛŒØ´ Ø§Ø² Û²ÛµÙª) Ø§Ø² Ø­Ù…Ø§ÛŒØª Ø§Ø³ØªØ§ØªÛŒÚ© Û±Û¶Û° Ø±ÙˆØ²Ù‡ Ú¯Ø±ÙØªÙ‡â€ŒØ§Ù†Ø¯
    static_support_160 = calculate_static_support(hist_df, lookback_period=160)
    if static_support_160 is not None and static_support_160 > 0:
        diff_percent_160 = ((last_close_val - static_support_160) / static_support_160) * 100
        
        if diff_percent_160 > 25:
            satisfied_filters.append("Price_Far_From_Static_Support_160D")
            reason_parts["static_levels"].append(
                f"Penalty: Price is {diff_percent_160:.1f}% above 160-day static support (Risk of bubble)."
            )

    return satisfied_filters, reason_parts

def _check_ml_prediction_filter(symbol_id):
    """
    Checks the MLPrediction table for a positive prediction for the symbol.
    """
    satisfied_filters, reason_parts = [], {"ml_prediction": []}

    try:
        # ğŸ’¡ G-ML: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² latest_prediction_date
        latest_prediction_date = db.session.query(func.max(MLPrediction.jprediction_date)).scalar()
        
        if not latest_prediction_date:
            return satisfied_filters, reason_parts

        prediction = db.session.query(MLPrediction).filter(
            MLPrediction.symbol_id == symbol_id,
            MLPrediction.jprediction_date == latest_prediction_date
        ).first()

        # ğŸ’¡ G-ML: Ø´Ø±Ø· Ø¨Ø±Ø§ÛŒ Ø±ÙˆÙ†Ø¯ ØµØ¹ÙˆØ¯ÛŒ: 'Uptrend' Ùˆ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø¨Ø§Ù„Ø§ (Ù…Ø«Ù„Ø§Ù‹ > 60%)
        if prediction and prediction.prediction == 'Uptrend' and prediction.probability_percent > 60:
            satisfied_filters.append("ML_Predicts_Uptrend")
            reason_parts["ml_prediction"].append(f"ML predicts uptrend (Prob: {prediction.probability_percent:.1f}%).")

    except Exception as e:
        logger.error(f"Error checking ML prediction for {symbol_id}: {e}")
        pass

    return satisfied_filters, reason_parts

class WeeklyWatchlistService:
    """
    Ø³Ø±ÙˆÛŒØ³ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ùˆ ØªÙˆÙ„ÛŒØ¯ Ù„ÛŒØ³Øª Ù‡ÙØªÚ¯ÛŒ Ø³Ù‡Ø§Ù… Ù…Ø³ØªØ¹Ø¯ Ø±Ø´Ø¯ (Watchlist).
    """

    def __init__(self):
        # ğŸ’¡ ÙØ±Ø¶: ØªØ§Ø¨Ø¹ _get_leading_sectors ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ø§Ø³Øª
        self.leading_sectors = _get_leading_sectors() 

    def _get_history_and_tech_data(self, symbol_id, symbol_name, days_back=200):
        # ğŸ’¡ G-Fix: ØªØ¨Ø¯ÛŒÙ„ start_date Ú¯Ø±Ú¯ÙˆØ±ÛŒ Ø¨Ù‡ Ø±Ø´ØªÙ‡ Ø¬Ù„Ø§Ù„ÛŒ
        start_date = (datetime.now().date() - timedelta(days=days_back))
        # ğŸ’¡ ÙØ±Ø¶: ØªØ§Ø¨Ø¹ convert_gregorian_to_jalali ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ø§Ø³Øª
        start_jdate_str = convert_gregorian_to_jalali(start_date)

        hist_df, tech_df = pd.DataFrame(), pd.DataFrame()
        
        # ğŸ’¡ G-Fix: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÛŒÚ© Ù†Ø´Ø³Øª Ù…ÙˆÙ‚Øª Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Thread
        local_session = db.session.session_factory()
        
        try:
            # --- ÙˆØ§Ú©Ø´ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ (HistoricalData) ---
            # ÙÛŒÙ„ØªØ± Ø¨Ø± Ø§Ø³Ø§Ø³ Ø±Ø´ØªÙ‡ Ø¬Ù„Ø§Ù„ÛŒ
        
            hist_records = local_session.query(HistoricalData).filter(
                HistoricalData.symbol_id == symbol_id,
                HistoricalData.jdate >= start_jdate_str 
            ).order_by(HistoricalData.jdate.asc()).all()
            
            # --- ÙˆØ§Ú©Ø´ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ ÙÙ†ÛŒ (TechnicalIndicatorData) ---
            # ÙÛŒÙ„ØªØ± Ø¨Ø± Ø§Ø³Ø§Ø³ Ø±Ø´ØªÙ‡ Ø¬Ù„Ø§Ù„ÛŒ
            tech_records = local_session.query(TechnicalIndicatorData).filter(
                TechnicalIndicatorData.symbol_id == symbol_id,
                TechnicalIndicatorData.jdate >= start_jdate_str 
            ).order_by(TechnicalIndicatorData.jdate.asc()).all()

            if hist_records:
                hist_df = pd.DataFrame([r.__dict__ for r in hist_records])
                # ğŸ’¡ G-Fix: ØªØ¨Ø¯ÛŒÙ„ jdate Ø¨Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø²Ù…Ø§Ù†ÛŒ Ú¯Ø±Ú¯ÙˆØ±ÛŒ
                hist_df['jdate'] = hist_df['jdate'].astype(str) # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ø±Ø´ØªÙ‡ Ø§Ø³Øª
                # ğŸ’¡ ÙØ±Ø¶: ØªØ§Ø¨Ø¹ convert_jalali_to_gregorian_timestamp ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ø§Ø³Øª
                hist_df['date'] = hist_df['jdate'].apply(convert_jalali_to_gregorian_timestamp)
                hist_df.set_index('date', inplace=True)
                hist_df = hist_df.drop(columns=['_sa_instance_state'], errors='ignore')
                
            if tech_records:
                tech_df = pd.DataFrame([r.__dict__ for r in tech_records])
                # ğŸ’¡ G-Fix: ØªØ¨Ø¯ÛŒÙ„ jdate Ø¨Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø²Ù…Ø§Ù†ÛŒ Ú¯Ø±Ú¯ÙˆØ±ÛŒ
                tech_df['jdate'] = tech_df['jdate'].astype(str) # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ø±Ø´ØªÙ‡ Ø§Ø³Øª
                # ğŸ’¡ ÙØ±Ø¶: ØªØ§Ø¨Ø¹ convert_jalali_to_gregorian_timestamp ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ø§Ø³Øª
                tech_df['date'] = tech_df['jdate'].apply(convert_jalali_to_gregorian_timestamp)
                tech_df.set_index('date', inplace=True)
                tech_df = tech_df.drop(columns=['_sa_instance_state'], errors='ignore')

            # ğŸ’¡ Log Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ø´Ù…Ø§
            logger.debug(f"Symbol {symbol_name} ({symbol_id}) - hist_rows: {len(hist_df) if isinstance(hist_df, pd.DataFrame) else 0}, tech_rows: {len(tech_df) if isinstance(tech_df, pd.DataFrame) else 0}, start_jdate_str: {start_jdate_str}")
                
        except Exception as e:
            logger.error(f"Error fetching data for {symbol_name} ({symbol_id}): {e}")
            hist_df, tech_df = pd.DataFrame(), pd.DataFrame() # Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ Ø¯Ø± ØµÙˆØ±Øª Ø¨Ø±ÙˆØ² Ø®Ø·Ø§

        finally:
            # ğŸš¨ Ø¨Ø³ØªÙ† ØµØ±ÛŒØ­ Ù†Ø´Ø³Øª Ù…ÙˆÙ‚Øª (Ø­Ù„ Ù…Ø´Ú©Ù„ Ù‡Ù…Ø²Ù…Ø§Ù†ÛŒ/Locking)
            local_session.close() 

        return hist_df, tech_df

    def _analyze_symbol(self, symbol_data_rec):
        """
        Ø¢Ù†Ø§Ù„ÛŒØ² Ú©Ø§Ù…Ù„ ÛŒÚ© Ù†Ù…Ø§Ø¯ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ùˆ ÙÙ†ÛŒ.
        Ø®Ø±ÙˆØ¬ÛŒ: (symbol_id, symbol_name, outlook, score, reason)
        """
        symbol_id = symbol_data_rec.symbol_id
        symbol_name = symbol_data_rec.symbol_name
        
        # 1. ÙˆØ§Ú©Ø´ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        hist_df, tech_df = self._get_history_and_tech_data(symbol_id, symbol_name)

        # ğŸ’¡ ÙØ±Ø¶: ØªÙˆØ§Ø¨Ø¹ is_data_sufficient Ùˆ MIN_REQUIRED_HISTORY_DAYS ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯
        if not is_data_sufficient(hist_df, MIN_REQUIRED_HISTORY_DAYS):
            logger.warning(f"Insufficient historical data for {symbol_name}.")
            return None
        
        # ğŸ’¡ G-Check: Ø¨Ø±Ø±Ø³ÛŒ Ú©ÙØ§ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙÙ†ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„
        if not is_data_sufficient(tech_df, 2):
            logger.warning(f"Insufficient technical data for {symbol_name}.")
            return None
            
        # 2. ØªØ¹ÛŒÛŒÙ† Ø³Ù†ØªÛŒÙ…Ù†Øª Ø¨Ø§Ø²Ø§Ø±
        # ğŸ’¡ ÙØ±Ø¶: ØªØ§Ø¨Ø¹ _get_market_sentiment ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ø§Ø³Øª
        market_sentiment = _get_market_sentiment() 
        
        # 3. Ø§Ø¬Ø±Ø§ÛŒ ØªÙ…Ø§Ù… ÙÛŒÙ„ØªØ±Ù‡Ø§
        
        # 3.1. Technical Filters
        # ğŸ’¡ ÙØ±Ø¶: ØªØ§Ø¨Ø¹ _check_technical_filters ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ø§Ø³Øª
        tech_filters, tech_reasons = _check_technical_filters(hist_df, tech_df, market_sentiment)
        
        # 3.2. Market/Volatility Filters
        # ğŸ’¡ ÙØ±Ø¶: ØªØ§Ø¨Ø¹ _check_market_condition_filters ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ø§Ø³Øª
        market_filters, market_reasons = _check_market_condition_filters(hist_df, tech_df)
        
        # 3.3. Smart Money Filters
        # ğŸ’¡ ÙØ±Ø¶: ØªØ§Ø¨Ø¹ _check_smart_money_filters ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ø§Ø³Øª
        money_filters, money_reasons = _check_smart_money_filters(hist_df)
        
        # 3.4. Power Thrust Filter
        # ğŸ’¡ G-Fix: Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§Ø±Ø³Ø§Ù„ close_ser ØµØ­ÛŒØ­
        # ğŸ’¡ ÙØ±Ø¶: ØªØ§Ø¨Ø¹ _get_close_series_from_hist_df Ùˆ _check_power_thrust_signal ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯
        close_ser = _get_close_series_from_hist_df(hist_df) 
        power_filters, power_reasons = _check_power_thrust_signal(hist_df, close_ser)
        
        # 3.5. Static Levels Filters
        last_tech = tech_df.iloc[-1]
        last_close = close_ser.iloc[-1]
        # ğŸ“¢ ØªØºÛŒÛŒØ±: Ø§Ø±Ø³Ø§Ù„ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… ØªØ§Ø±ÛŒØ®ÛŒ (hist_df) Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯ÛŒÙ†Ø§Ù…ÛŒÚ© Ø­Ù…Ø§ÛŒØª
        # ğŸ’¡ ÙØ±Ø¶: ØªØ§Ø¨Ø¹ _check_static_levels_filters ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ø§Ø³Øª
        static_filters, static_reasons = _check_static_levels_filters(hist_df, last_tech, last_close)
        
        # 3.6. Sector Strength Filter
        # ğŸ’¡ ÙØ±Ø¶: ØªØ§Ø¨Ø¹ _check_sector_strength_filter ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ø§Ø³Øª
        sector_filters, sector_reasons = _check_sector_strength_filter(symbol_data_rec.group_name, self.leading_sectors)

        # 3.7. Simple Fundamental Filter
        # ğŸ’¡ ÙØ±Ø¶: ØªØ§Ø¨Ø¹ _check_simple_fundamental_filters ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ø§Ø³Øª
        fundamental_filters, fundamental_reasons = _check_simple_fundamental_filters(symbol_data_rec)

        # 3.8. ML Prediction Filter
        # ğŸ’¡ ÙØ±Ø¶: ØªØ§Ø¨Ø¹ _check_ml_prediction_filter ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ø§Ø³Øª
        ml_filters, ml_reasons = _check_ml_prediction_filter(symbol_id)
        
        # 4. ØªØ¬Ù…ÛŒØ¹ Ù†ØªØ§ÛŒØ¬
        all_satisfied_filters = tech_filters + market_filters + money_filters + power_filters + static_filters + sector_filters + fundamental_filters + ml_filters
        
        all_reasons = {
            "technical": tech_reasons.get("technical", []),
            "market_condition": market_reasons.get("market_condition", []),
            "smart_money": money_reasons.get("smart_money", []),
            "power_thrust": power_reasons.get("power_thrust", []),
            "static_levels": static_reasons.get("static_levels", []),
            "sector_strength": sector_reasons.get("sector_strength", []),
            "fundamental": fundamental_reasons.get("fundamental", []),
            "ml_prediction": ml_reasons.get("ml_prediction", []),
        }

        # 5. Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ù†Ù‡Ø§ÛŒÛŒ (Score Calculation)
        # ğŸ’¡ ÙØ±Ø¶: Ø«Ø§Ø¨Øª FILTER_WEIGHTS ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ø§Ø³Øª
        total_score = sum(
            FILTER_WEIGHTS.get(f, {}).get("weight", 0) for f in all_satisfied_filters
        )
        
        # 6. ØªØ¹ÛŒÛŒÙ† Ú†Ø´Ù…â€ŒØ§Ù†Ø¯Ø§Ø² (Outlook)
        if total_score >= 10 and len([f for f in all_satisfied_filters if FILTER_WEIGHTS.get(f, {}).get("weight", 0) > 0]) >= 4:
            outlook = "Strong Buy"
        elif total_score >= 5:
            outlook = "Buy"
        elif total_score >= 0:
            outlook = "Neutral"
        elif total_score >= -5:
            outlook = "Sell"
        else:
            outlook = "Strong Sell"
            
        # 7. Ø³Ø§Ø®Øª Ø±Ø´ØªÙ‡ Ø¯Ù„Ø§ÛŒÙ„ (Reason String Construction)
        positive_filters_reasons = []
        negative_filters_reasons = []
        
        for filter_name in all_satisfied_filters:
            weight = FILTER_WEIGHTS.get(filter_name, {}).get("weight", 0)
            description = FILTER_WEIGHTS.get(filter_name, {}).get("description", "Ø¨Ø¯ÙˆÙ† ØªÙˆØ¶ÛŒØ­Ø§Øª")
            
            if weight > 0:
                positive_filters_reasons.append(f"âœ… {filter_name} (+{weight}): {description}")
            elif weight < 0:
                negative_filters_reasons.append(f"âŒ {filter_name} ({weight}): {description}")
            else:
                pass # weight 0 is ignored

        final_reasons_list = [
            f"Ø§Ù…ØªÛŒØ§Ø² Ù†Ù‡Ø§ÛŒÛŒ: {total_score}",
            f"Ú†Ø´Ù…â€ŒØ§Ù†Ø¯Ø§Ø² Ø¨Ø§Ø²Ø§Ø±: {market_sentiment}",
            "--- Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø«Ø¨Øª ---",
        ] + positive_filters_reasons + [
            "--- Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ù†ÙÛŒ (Ø¬Ø±ÛŒÙ…Ù‡â€ŒÙ‡Ø§) ---",
        ] + negative_filters_reasons + [
            "--- ØªÙˆØ¶ÛŒØ­Ø§Øª ÙÙ†ÛŒ ØªÚ©Ù…ÛŒÙ„ÛŒ ---",
        ] + all_reasons["technical"] + all_reasons["market_condition"] + all_reasons["smart_money"] + all_reasons["power_thrust"] + all_reasons["static_levels"] + all_reasons["sector_strength"] + all_reasons["fundamental"] + all_reasons["ml_prediction"]
        
        # ğŸ’¡ G-CleanUp: Ø­Ø°Ù Ø¯Ù„Ø§ÛŒÙ„ ØªÚ©Ø±Ø§Ø±ÛŒ Ùˆ Ø®Ø§Ù„ÛŒ
        final_reasons_list = [r for r in final_reasons_list if r not in ["--- Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ù†ÙÛŒ (Ø¬Ø±ÛŒÙ…Ù‡â€ŒÙ‡Ø§) ---", "--- ØªÙˆØ¶ÛŒØ­Ø§Øª ÙÙ†ÛŒ ØªÚ©Ù…ÛŒÙ„ÛŒ ---"]]
        final_reasons_list = list(dict.fromkeys(r for r in final_reasons_list if r.strip()))
        
        final_reason_string = "\n".join(final_reasons_list)

        return SimpleNamespace(
            symbol_id=symbol_id,
            symbol_name=symbol_name,
            outlook=outlook,
            score=total_score,
            reason=final_reason_string
        )


    def _save_result(self, result: SimpleNamespace):
        """
        Ø°Ø®ÛŒØ±Ù‡ Ù†ØªÛŒØ¬Ù‡ Ø¢Ù†Ø§Ù„ÛŒØ² Ø¯Ø± Ø¬Ø¯ÙˆÙ„ WeeklyWatchlistResult.
        """
        # ğŸ’¡ FIX: ØªØ¹Ø±ÛŒÙ Ù…ØªØºÛŒØ± today_gregorian_date Ø¨Ø±Ø§ÛŒ Ø±ÙØ¹ NameError
        today_gregorian_date = datetime.now().date() 
        # ğŸ’¡ ÙØ±Ø¶: ØªØ§Ø¨Ø¹ get_today_jdate_str ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ø§Ø³Øª
        today_jdate = get_today_jdate_str()
        
        try:
            # ğŸ’¡ G-Check: Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÚ©Ø±Ø§Ø± ØªØ­Ù„ÛŒÙ„ Ø¯Ø± ÛŒÚ© ØªØ§Ø±ÛŒØ®
            
            # Ø¬Ø³ØªØ¬ÙˆÛŒ Ø±Ú©ÙˆØ±Ø¯ Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø±Ø§ÛŒ Ø§Ù…Ø±ÙˆØ²
            existing_record = db.session.query(WeeklyWatchlistResult).filter(
                WeeklyWatchlistResult.symbol_id == result.symbol_id,
                # ğŸ’¡ FIX: Ø§ØµÙ„Ø§Ø­ jentry_date Ø¨Ù‡ jdate (ÙÛŒÙ„Ø¯ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù…Ø¯Ù„ Ø´Ù…Ø§)
                WeeklyWatchlistResult.jentry_date == today_jdate, 
                WeeklyWatchlistResult.status == 'Open'
            ).first()

            if existing_record:
                # ğŸ’¡ G-Update: Ø¨Ù‡ Ø±ÙˆØ² Ø±Ø³Ø§Ù†ÛŒ Ø±Ú©ÙˆØ±Ø¯ Ù…ÙˆØ¬ÙˆØ¯
                existing_record.outlook = result.outlook
                existing_record.score = result.score
                existing_record.reason = result.reason
                existing_record.entry_price = result.entry_price
                existing_record.probability_percent = result.probability_percent
                existing_record.updated_at = datetime.utcnow()
                logger.info(f"Updated watchlist for {result.symbol_name} (Score: {result.score})")
            else:
                # ğŸ’¡ G-Create: Ø§ÛŒØ¬Ø§Ø¯ Ø±Ú©ÙˆØ±Ø¯ Ø¬Ø¯ÛŒØ¯
                new_result = WeeklyWatchlistResult(
                    signal_unique_id=str(uuid.uuid4()),
                    symbol_id=result.symbol_id,
                    symbol_name=result.symbol_name,
                    outlook=result.outlook,
                    score=result.score,
                    reason=result.reason,
                    entry_price=result.entry_price,

                    # ğŸ’¥ FIX: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…ØªØºÛŒØ± ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ùˆ ÙÛŒÙ„Ø¯ ØµØ­ÛŒØ­ (jdate)
                    entry_date=today_gregorian_date, 
                    jentry_date=today_jdate, # <-- ÙÛŒÙ„Ø¯ ØµØ­ÛŒØ­ Ø¯Ø± Ù…Ø¯Ù„
                    status='Open', # ØªÙ…Ø§Ù… Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù‡ÙØªÚ¯ÛŒ Ø¨Ø§Ø² ØªÙ„Ù‚ÛŒ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯
                    probability_percent=result.probability_percent
                )
                db.session.add(new_result)
                logger.info(f"Added new watchlist signal for {result.symbol_name} (Score: {result.score})")
                
            db.session.commit()
            return True
        except Exception as e:
            db.session.rollback()
            logger.error(f"Error saving result for {result.symbol_name}: {e}")
            return False

    def _process_one_symbol(self, symbol_data_rec):
        """
        ØªØ§Ø¨Ø¹ Ø¨Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒÚ© Ù†Ù…Ø§Ø¯ Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§Ù‡Ø§ Ùˆ Ù„Ø§Ú¯ÛŒÙ†Ú¯.
        """
        # ğŸ’¡ G-Fix: Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù‚ÛŒÙ…Øª ÙˆØ±ÙˆØ¯ (Entry Price)
        # ğŸ’¡ ÙØ±Ø¶: ØªØ§Ø¨Ø¹ _get_history_and_tech_data Ø¯Ø± Ø¨Ø§Ù„Ø§ ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ø§Ø³Øª
        hist_df, _ = self._get_history_and_tech_data(symbol_data_rec.symbol_id, symbol_data_rec.symbol_name)
        # ğŸ’¡ ÙØ±Ø¶: ØªØ§Ø¨Ø¹ _get_close_series_from_hist_df ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ø§Ø³Øª
        close_ser = _get_close_series_from_hist_df(hist_df)
        entry_price = close_ser.iloc[-1] if not close_ser.empty else None
        
        # Ø§Ú¯Ø± Ù‚ÛŒÙ…Øª Ø§Ù…Ø±ÙˆØ² ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ØŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†Ú©Ù†
        if entry_price is None:
            logger.warning(f"Skipping {symbol_data_rec.symbol_name}: Could not determine entry price.")
            return None

        analysis_result = self._analyze_symbol(symbol_data_rec)
        
        if analysis_result is None:
            return None

        # ğŸ’¡ G-Update: Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´ÛŒØ¡ Ù†ØªÛŒØ¬Ù‡ Ø¨Ø§ Ù‚ÛŒÙ…Øª ÙˆØ±ÙˆØ¯ Ùˆ Ø§Ø­ØªÙ…Ø§Ù„
        analysis_result.entry_price = entry_price
        # ğŸ’¡ G-Fix: Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ ÙÛŒÙ„Ø¯ probability_percent Ø¯Ø± ML
        ml_prediction_reason = next((r for r in analysis_result.reason.split('\n') if "ML predicts uptrend" in r), None)
        if ml_prediction_reason:
            try:
                prob_str = ml_prediction_reason.split('(Prob: ')[1].split('%')[0]
                analysis_result.probability_percent = float(prob_str)
            except:
                analysis_result.probability_percent = None
        else:
            analysis_result.probability_percent = None

        if analysis_result.outlook in ["Strong Buy", "Buy"]:
            # self._save_result(analysis_result) <-- Ø§ÛŒÙ† Ø®Ø· Ø­Ø°Ù Ø´Ø¯ Ùˆ Ø¨Ù‡ run_watchlist_generation Ù…Ù†ØªÙ‚Ù„ Ø´Ø¯
            return analysis_result
        
        return None

    # -----------------------------------------------------------------
    # --- ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ ØªÙˆÙ„ÛŒØ¯ ÙˆØ§Ú† Ù„ÛŒØ³Øª (Ø¨Ø§ Ø§Ø¹Ù…Ø§Ù„ ÙÛŒÙ„ØªØ± ØµÙ†Ø¯ÙˆÙ‚â€ŒÙ‡Ø§) ---
    # -----------------------------------------------------------------
    def run_watchlist_generation(self, parallel=True, max_workers=8):
        """
        ÙØ±Ø§ÛŒÙ†Ø¯ Ø§ØµÙ„ÛŒ ØªÙˆÙ„ÛŒØ¯ ÙˆØ§Ú† Ù„ÛŒØ³Øª.
        Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø±Ø§ ØªØ­Ù„ÛŒÙ„ Ú©Ø±Ø¯Ù‡ØŒ Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù…ØªÛŒØ§Ø² (Score) Ù…Ø±ØªØ¨ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ
        Ùˆ ÙÙ‚Ø· 8 Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø¨Ø±ØªØ± Ø±Ø§ Ø¯Ø± Ø¬Ø¯ÙˆÙ„ WeeklyWatchlistResult Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÙ†Ù…Ø§ÛŒØ¯.
        """
        logger.info("Starting Weekly Watchlist Generation...")
        start_time = time.time()
        
        # ğŸ’¡ G-Fix: Ú¯Ø±ÙØªÙ† Ú©Ø§Ù†ØªÚ©Ø³Øª Ø§Ù¾Ù„ÛŒÚ©ÛŒØ´Ù† Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆØ§Ø²ÛŒ
        app = current_app._get_current_object()
        
        # --- Step 1: ÙˆØ§Ú©Ø´ÛŒ Ù„ÛŒØ³Øª Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø± (Ø¨Ø§ Ø§Ø¹Ù…Ø§Ù„ ÙÛŒÙ„ØªØ± ØµÙ†Ø¯ÙˆÙ‚â€ŒÙ‡Ø§) ---
        try:
            logger.info("Fetching valid symbols from ComprehensiveSymbolData table (excluding funds)...")
            
            # --- Ù„ÛŒØ³Øª Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø­Ø°Ù ØµÙ†Ø¯ÙˆÙ‚â€ŒÙ‡Ø§ Ùˆ Ø§ÙˆØ±Ø§Ù‚ ---
            EXCLUDE_KEYWORDS = [
                'ØµÙ†Ø¯ÙˆÙ‚', 'Ø³Ø±Ù…Ø§ÛŒÙ‡ Ú¯Ø°Ø§Ø±ÛŒ Ù‚Ø§Ø¨Ù„ Ù…Ø¹Ø§Ù…Ù„Ù‡', 'Ø§ÙˆØ±Ø§Ù‚', 'ØªØ³Ù‡ÛŒÙ„Ø§Øª', 'Ø§Ø¬Ø§Ø±Ù‡', 
                'Ù…Ø´Ø§Ø±Ú©Øª', 'Ú¯ÙˆØ§Ù‡ÛŒ', 'Ú©Ø§Ù„Ø§', 'Ø§Ù†Ø±Ú˜ÛŒ', 'Ø§Ø®ØªÛŒØ§Ø±', 'Ø¢ØªÛŒ', 'Ø³Ù„Ù', 'Ø¨Ø¯Ù‡ÛŒ', 
                'Ø±ÛŒØ§Ù„ÛŒ', 'Ø§Ø±Ø²ÛŒ', 'Ù‚Ø§Ø¨Ù„ Ø§Ù†ØªÙ‚Ø§Ù„'
            ]

            # Ø³Ø§Ø®Øª Ø´Ø±ÙˆØ· ÙÛŒÙ„ØªØ±ÛŒÙ†Ú¯ (NOT LIKE)
            conditions = []
            for kw in EXCLUDE_KEYWORDS:
                # ÙÛŒÙ„ØªØ± Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†Ø§Ù… ØµÙ†Ø¹ØªØŒ Ú¯Ø±ÙˆÙ‡ Ùˆ Ù†Ø§Ù… Ø´Ø±Ú©Øª
                conditions.append(~ComprehensiveSymbolData.industry.like(f'%{kw}%'))
                conditions.append(~ComprehensiveSymbolData.group_name.like(f'%{kw}%'))
                conditions.append(~ComprehensiveSymbolData.company_name.like(f'%{kw}%'))
            
            # Ø§Ø¬Ø±Ø§ÛŒ Ú©ÙˆØ¦Ø±ÛŒ Ø¨Ø§ Ø§Ø¹Ù…Ø§Ù„ ØªÙ…Ø§Ù… Ø´Ø±ÙˆØ·
            query = db.session.query(ComprehensiveSymbolData).filter(and_(*conditions))
            active_symbols = query.all()
            
            # ğŸ’¡ G-Fix: Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ symbol_id Ùˆ symbol_name (Ø§ÛŒÙ† ÙÛŒÙ„ØªØ± Ù‡Ù…Ú†Ù†Ø§Ù† Ù„Ø§Ø²Ù… Ø§Ø³Øª)
            active_symbols = [s for s in active_symbols if s.symbol_id and s.symbol_name]
            
            if not active_symbols:
                logger.warning("No VALID symbols found after filtering funds and non-stocks.")
                return [] 
                
            logger.info(f"Found {len(active_symbols)} valid symbols to analyze after filtering.")
        
        except Exception as e:
            logger.error(f"Error fetching and filtering symbols: {e}")
            return [] 

        # --- Step 2: Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆØ§Ø²ÛŒ ÛŒØ§ ØªØ±ØªÛŒØ¨ÛŒ (ÙÙ‚Ø· ØªÙˆÙ„ÛŒØ¯ Ù†ØªÛŒØ¬Ù‡ØŒ Ø¨Ø¯ÙˆÙ† Ø°Ø®ÛŒØ±Ù‡) ---
        
        if parallel:
            logger.info(f"Starting parallel processing with {max_workers} workers.")
            
            # ğŸ’¡ ØªÙˆØ¬Ù‡: _process_one_symbol_with_context Ù†Ø¨Ø§ÛŒØ¯ Ø¯ÛŒÚ¯Ø± _save_result Ø±Ø§ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ú©Ù†Ø¯.
            results = Parallel(n_jobs=max_workers, backend='threading', verbose=0)(
                delayed(self._process_one_symbol_with_context)(symbol, app) for symbol in active_symbols
            )
            
        else:
            logger.info("Starting sequential processing.")
            results = []
            for symbol in active_symbols:
                # ğŸ’¡ ØªÙˆØ¬Ù‡: _process_one_symbol Ù†Ø¨Ø§ÛŒØ¯ Ø¯ÛŒÚ¯Ø± _save_result Ø±Ø§ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ú©Ù†Ø¯.
                results.append(self._process_one_symbol(symbol))

        # --- Step 3: Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒØŒ Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ù†ØªØ§ÛŒØ¬ ---
        
        # Ø­Ø°Ù Ù†ØªØ§ÛŒØ¬ None (Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¶Ø¹ÛŒÙ)
        successful_results = [r for r in results if r is not None]
        
        logger.info(f"Found {len(successful_results)} successful signals before final filtering.")

        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù…ØªÛŒØ§Ø² (Score) Ø¨Ù‡ ØµÙˆØ±Øª Ù†Ø²ÙˆÙ„ÛŒ
        # ğŸ’¡ G-Sort: Ø§Ø² 'score' Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…ØŒ Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´Øª -1 ÙØ±Ø¶ Ù…ÛŒâ€ŒØ´ÙˆØ¯
        successful_results.sort(key=lambda x: getattr(x, 'score', -1), reverse=True)
        
        # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø¨Ù‡ 8 Ù†Ù…Ø§Ø¯ Ø¨Ø±ØªØ±
        top_n = 8
        top_results_to_save = successful_results[:top_n]
        
        logger.info(f"Selecting and saving ONLY the Top {len(top_results_to_save)} signals based on Score.")

        # --- Step 4: Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙÙ‚Ø· 8 Ù†Ù…Ø§Ø¯ Ø¨Ø±ØªØ± ---
        # ğŸ’¡ G-Save: Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø± Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯
        for result in top_results_to_save:
            # ğŸ’¡ ØªÙˆØ¬Ù‡: ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ _save_result Ø¯Ø± Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ Ø¨Ø±Ø§ÛŒ Top 8
            self._save_result(result) 
            
        # --- Step 5: Ù¾Ø§ÛŒØ§Ù† ÙØ±Ø§ÛŒÙ†Ø¯ ---
        logger.info(f"Watchlist Generation Completed.")
        logger.info(f"Total time elapsed: {time.time() - start_time:.2f} seconds.")
        
        return top_results_to_save


    def _process_one_symbol_with_context(self, symbol, app):
        """
        ÛŒÚ© Wrapper Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ _process_one_symbol Ø¯Ø± ÛŒÚ© Application Context Ø¬Ø¯ÛŒØ¯.
        Ø¨Ø§ Ø§Ø¬Ø±Ø§ÛŒ db.session.remove() Ø¯Ø±ÙˆÙ† Ú©Ø§Ù†ØªÚ©Ø³Øª ÙØ¹Ø§Ù„ØŒ Ø®Ø·Ø§ÛŒ Runtime Ø±Ø§ Ø­Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        """
        result = None
        
        # ğŸ’¡ G-Fix: ØªØ¶Ù…ÛŒÙ† ÙØ¹Ø§Ù„ Ø¨ÙˆØ¯Ù† Ú©Ø§Ù†ØªÚ©Ø³Øª Ø¯Ø± ØªÙ…Ø§Ù… Ù…Ø±Ø§Ø­Ù„ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³
        with app.app_context():
            try:
                # 1. Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ø± Ø§ØµÙ„ÛŒ (Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ù…Ø¬Ø§Ø² Ø§Ø³Øª)
                result = self._process_one_symbol(symbol)
            finally:
                # 2. ğŸš¨ G-Fix: Ø­Ø°Ù ØµØ±ÛŒØ­ Ù†Ø´Ø³Øª Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø§ÛŒØ¯ Ù‚Ø¨Ù„ Ø§Ø² Ø®Ø±ÙˆØ¬ Ø§Ø² Ø¨Ù„ÙˆÚ© with Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯.
                db.session.remove() 
        return result


    def get_latest_watchlist(self, limit=10, include_history=False):
        """
        Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ø¢Ø®Ø±ÛŒÙ† ÙˆØ§Ú† Ù„ÛŒØ³Øª (Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ 'Open' Ø¨Ø§ Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø²).
        """
        logger.info(f"Fetching latest watchlist results (Limit: {limit}, Include History: {include_history})")
        
        # --- Step 1: ØªÙ†Ø¸ÛŒÙ… Ú©ÙˆØ¦Ø±ÛŒ Ù¾Ø§ÛŒÙ‡ Ùˆ Ø§Ø¹Ù…Ø§Ù„ ÙÛŒÙ„ØªØ± (FIX: ÙÛŒÙ„ØªØ± Ù‚Ø¨Ù„ Ø§Ø² Limit/Order) ---
        query = db.session.query(WeeklyWatchlistResult)
        
        # ğŸ’¡ FIX: ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ ÙˆØ¶Ø¹ÛŒØª (Open) Ø±Ø§ Ù‚Ø¨Ù„ Ø§Ø² Ù…Ø±ØªØ¨ Ø³Ø§Ø²ÛŒ Ùˆ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø§Ø¹Ù…Ø§Ù„ Ù…ÛŒ Ú©Ù†ÛŒÙ….
        if not include_history:
            # ØªÙ†Ù‡Ø§ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ÙØ¹Ø§Ù„ (Open) Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
            query = query.filter(WeeklyWatchlistResult.status == 'Open')

        # Ø§Ø¹Ù…Ø§Ù„ Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª
        query = query.order_by(
            WeeklyWatchlistResult.score.desc(),
            # ğŸ’¡ FIX: Ø§ØµÙ„Ø§Ø­ jentry_date Ø¨Ù‡ jdate (ÙÛŒÙ„Ø¯ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù…Ø¯Ù„ Ø´Ù…Ø§)
            WeeklyWatchlistResult.jentry_date.desc()
        ).limit(limit)

        # --- Step 2: Ø§Ø¬Ø±Ø§ÛŒ Ú©ÙˆØ¦Ø±ÛŒ ---
        try:
            results = query.all()
        except Exception as e:
            logger.error(f"Error fetching latest watchlist: {e}")
            return []

        # --- Step 3: Ø¯Ø±ÛŒØ§ÙØª Ù†Ø§Ù… Ú©Ø§Ù…Ù„ Ø´Ø±Ú©Øªâ€ŒÙ‡Ø§ ---
        symbol_ids = [r.symbol_id for r in results]
        company_name_map = {}
        if symbol_ids:
            company_name_records = db.session.query(
                ComprehensiveSymbolData.symbol_id,
                ComprehensiveSymbolData.company_name
            ).filter(ComprehensiveSymbolData.symbol_id.in_(symbol_ids)).all()
            
            company_name_map = {sid: cname for sid, cname in company_name_records}

        # --- Step 4: Ø³Ø§Ø®Øª Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù‡Ø§ÛŒÛŒ ---
        output_stocks = []
        for r in results:
            output_stocks.append({
                "signal_unique_id": r.signal_unique_id,
                "symbol_id": r.symbol_id,
                "symbol_name": r.symbol_name,
                "company_name": company_name_map.get(r.symbol_id, r.symbol_name),
                "outlook": r.outlook,
                "reason": r.reason,
                "entry_price": r.entry_price,
                # ğŸ’¡ FIX: Ø§ØµÙ„Ø§Ø­ jentry_date Ø¨Ù‡ jdate
                "jentry_date": r.jentry_date, 
                "exit_price": r.exit_price,
                "jexit_date": r.jexit_date,
                "profit_loss_percentage": r.profit_loss_percentage,
                "status": r.status,
                "probability_percent": getattr(r, "probability_percent", None),
                "score": getattr(r, "score", None)
            })

        # --- Step 5: Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù…ØªÛŒØ§Ø² ---
        # Ø§ÛŒÙ† Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø±ØªØ¨ Ùˆ Ø³Ø§Ø²Ú¯Ø§Ø± Ø¨Ø§ Ø®Ø±ÙˆØ¬ÛŒ API Ø§Ø³Øª.
        output_stocks.sort(key=lambda x: x.get('score') if x.get('score') is not None else -100, reverse=True)

        logger.info(f"Successfully retrieved {len(output_stocks)} watchlist results.")
        return output_stocks
