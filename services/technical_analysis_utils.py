# -*- coding: utf-8 -*-
# analysis/technical_analysis_utils.py
# ØªÙˆØ§Ø¨Ø¹ Ø®Ø§Ù… Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ØŒ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„ Ùˆ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ.
# Ø§ÛŒÙ† ØªÙˆØ§Ø¨Ø¹ Ù‡ÛŒÚ† ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒØ§ÛŒ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ù†Ø¯Ø§Ø±Ù†Ø¯ Ùˆ ØªÙ†Ù‡Ø§ Ø¨Ø§ DataFrame/Series Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯.

import pandas as pd
import numpy as np
import logging
import jdatetime
import datetime
from sqlalchemy import func
from functools import lru_cache
from typing import Union, List, Dict, Optional, Tuple, Any
import time 

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯ÛŒÙ†Ú¯
logger = logging.getLogger(__name__)

# --- ØªÙˆØ§Ø¨Ø¹ Ø¹Ù…ÙˆÙ…ÛŒ Ùˆ ØªØ¨Ø¯ÛŒÙ„ ØªØ§Ø±ÛŒØ® ---

def convert_gregorian_to_jalali(gregorian_date_obj: Union[datetime.date, datetime.datetime, Any]) -> Optional[str]:
    """
    ØªØ¨Ø¯ÛŒÙ„ ÛŒÚ© Ø´ÛŒØ¡ datetime.date ÛŒØ§ datetime.datetime Ø¨Ù‡ Ø±Ø´ØªÙ‡ ØªØ§Ø±ÛŒØ® Ø¬Ù„Ø§Ù„ÛŒ (YYYY-MM-DD).
    """
    try:
        if pd.isna(gregorian_date_obj):
            return None

        if isinstance(gregorian_date_obj, datetime.datetime):
            gregorian_dt = gregorian_date_obj
        elif isinstance(gregorian_date_obj, datetime.date):
            gregorian_dt = datetime.datetime(gregorian_date_obj.year, gregorian_date_obj.month, gregorian_date_obj.day)
        else:
            logger.warning(f"Ù†ÙˆØ¹ ÙˆØ±ÙˆØ¯ÛŒ Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ ØªØ§Ø±ÛŒØ®: {type(gregorian_date_obj)}")
            return None

        jdate_obj = jdatetime.date.fromgregorian(
            year=gregorian_dt.year,
            month=gregorian_dt.month,
            day=gregorian_dt.day
        ).strftime('%Y-%m-%d')

        return jdate_obj
    except (ValueError, TypeError) as e:
        logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ¨Ø¯ÛŒÙ„ ØªØ§Ø±ÛŒØ® Ù…ÛŒÙ„Ø§Ø¯ÛŒ Ø¨Ù‡ Ø¬Ù„Ø§Ù„ÛŒ: {e} - ÙˆØ±ÙˆØ¯ÛŒ: {gregorian_date_obj}")
        return None
    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡ Ø¯Ø± ØªØ¨Ø¯ÛŒÙ„ ØªØ§Ø±ÛŒØ® Ù…ÛŒÙ„Ø§Ø¯ÛŒ Ø¨Ù‡ Ø¬Ù„Ø§Ù„ÛŒ: {e} - ÙˆØ±ÙˆØ¯ÛŒ: {gregorian_date_obj}")
        return None

def get_today_jdate_str() -> str:
    """
    Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† ØªØ§Ø±ÛŒØ® Ø§Ù…Ø±ÙˆØ² Ø¨Ù‡ ÙØ±Ù…Øª Ø¬Ù„Ø§Ù„ÛŒ (Ø´Ù…Ø³ÛŒ) Ø¨Ù‡ ØµÙˆØ±Øª Ø±Ø´ØªÙ‡ YYYY-MM-DD.
    """
    return jdatetime.date.today().strftime('%Y-%m-%d')

def normalize_value(val: Any) -> Optional[Union[float, int]]:
    """
    Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ÛŒÚ© Ù…Ù‚Ø¯Ø§Ø±ØŒ Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§ØŒ Pandas Series Ùˆ ÙØ±Ù…Øªâ€ŒÙ‡Ø§ÛŒ Ø±Ø´ØªÙ‡â€ŒØ§ÛŒ Ø®Ø§Øµ
    Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÛŒÚ© Ù…Ù‚Ø¯Ø§Ø± Ø¹Ø¯Ø¯ÛŒ Ø§Ø³Ú©Ø§Ù„Ø±.
    """
    if isinstance(val, (list, pd.Series)):
        return val.iloc[0] if len(val) > 0 else None
    elif isinstance(val, str):
        if 'Name:' in val:
            try:
                parts = val.split()
                for part in parts:
                    if part.replace('.', '', 1).isdigit():
                        return float(part)
            except ValueError:
                logger.warning(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ¨Ø¯ÛŒÙ„ Ø±Ø´ØªÙ‡ '{val}' Ø¨Ù‡ Ø¹Ø¯Ø¯.")
                return None
        try:
            return float(val)
        except ValueError:
            logger.warning(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ¨Ø¯ÛŒÙ„ Ø±Ø´ØªÙ‡ '{val}' Ø¨Ù‡ Ø¹Ø¯Ø¯.")
            return None
    return val

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª API Ùˆ ØªØ§Ø®ÛŒØ± ---
DEFAULT_PER_SYMBOL_DELAY: float = 0.3 # ØªØ§Ø®ÛŒØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Û°.Û³ Ø«Ø§Ù†ÛŒÙ‡ Ø¨ÛŒÙ† Ù‡Ø± Ø¯Ø±Ø®ÙˆØ§Ø³Øª API
DEFAULT_REQUEST_TIMEOUT: int = 15 # Timeout Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¨Ø±Ø§ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ HTTP (Ø«Ø§Ù†ÛŒÙ‡)

def safe_sleep(seconds: float, log_message: str = "") -> None:
    """
    ØªØ£Ø®ÛŒØ± Ø§ÛŒÙ…Ù† Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª Ù„Ø§Ú¯ Ú©Ø±Ø¯Ù†.
    """
    if seconds > 0:
        message = f"Ø¯Ø± Ø­Ø§Ù„ ØªØ§Ø®ÛŒØ± Ø¨Ù‡ Ù…Ø¯Øª {seconds:.2f} Ø«Ø§Ù†ÛŒÙ‡..."
        if log_message:
            message += f" ({log_message})"
        logger.debug(message)
        time.sleep(seconds)

# -----------------------------------------------------------
# ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ ÙÙ†ÛŒ (Technical Indicator Calculation Utilities)
# -----------------------------------------------------------

def calculate_sma(series: pd.Series, period: int) -> pd.Series:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú© Ø³Ø§Ø¯Ù‡ (SMA)."""
    return series.rolling(window=period, min_periods=1).mean()

def calculate_volume_ma(series: pd.Series, period: int) -> pd.Series:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªØ­Ø±Ú© Ø­Ø¬Ù…."""
    return series.rolling(window=period, min_periods=1).mean()

def calculate_rsi(series: pd.Series, period: int = 14) -> pd.Series:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ RSI."""
    delta = series.diff()
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Gain/Loss Ø¨Ù‡ Ø±ÙˆØ´ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ RSI
    gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()

    # Ù…Ø¯ÛŒØ±ÛŒØª ØªÙ‚Ø³ÛŒÙ… Ø¨Ø± ØµÙØ±
    with np.errstate(divide='ignore', invalid='ignore'):
        rs = gain / loss

    rsi = 100 - (100 / (1 + rs))
    return rsi

def calculate_macd(series: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9) -> Tuple[pd.Series, pd.Series, pd.Series]:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ MACD (Ø®Ø· MACDØŒ Ø®Ø· Ø³ÛŒÚ¯Ù†Ø§Ù„ Ùˆ Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù…)."""
    ema_fast = series.ewm(span=fast, adjust=False, min_periods=1).mean()
    ema_slow = series.ewm(span=slow, adjust=False, min_periods=1).mean()
    macd = ema_fast - ema_slow
    macd_signal = macd.ewm(span=signal, adjust=False, min_periods=1).mean()
    macd_histogram = macd - macd_signal
    return macd, macd_signal, macd_histogram

def calculate_bollinger_bands(series: pd.Series, period: int = 20, std_dev: int = 2) -> Tuple[pd.Series, pd.Series, pd.Series]:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Bollinger Bands (Ø¨Ø§Ù„Ø§ÛŒÛŒØŒ Ù…ÛŒØ§Ù†ÛŒ Ùˆ Ù¾Ø§ÛŒÛŒÙ†ÛŒ)."""
    middle = series.rolling(window=period, min_periods=1).mean()
    std = series.rolling(window=period, min_periods=1).std()
    upper = middle + (std * std_dev)
    lower = middle - (std * std_dev)
    return upper, middle, lower

def calculate_atr(high: pd.Series, low: pd.Series, close: pd.Series, period: int = 14) -> pd.Series:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ ATR (Average True Range)."""
    tr1 = high - low
    tr2 = abs(high - close.shift())
    tr3 = abs(low - close.shift())
    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
    # ATR Ø§Ø² TR Ø¨Ù‡ Ø¯Ø³Øª Ù…ÛŒâ€ŒØ¢ÛŒØ¯ØŒ Ø¨Ø§ÛŒØ¯ Ø§Ø² EMA ÛŒØ§ SMA Ø¨Ø±Ø§ÛŒ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯.
    atr = tr.ewm(span=period, adjust=False, min_periods=1).mean()
    return atr

def calculate_smart_money_flow(df: pd.DataFrame) -> pd.DataFrame:
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø¬Ø±ÛŒØ§Ù† Ù¾ÙˆÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ.
    Args:
        df (pd.DataFrame): DataFrame Ø´Ø§Ù…Ù„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ 'buy_i_volume', 'sell_i_volume',
                         'buy_count_i', 'sell_count_i', 'value'.
    Returns:
        pd.DataFrame: DataFrameØ§ÛŒ Ø­Ø§ÙˆÛŒ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¯Ù‡.
    """
    required_cols = ['buy_i_volume', 'sell_i_volume', 'buy_count_i', 'sell_count_i', 'value']
    missing_columns = [col for col in required_cols if col not in df.columns]
    
    df_copy = df.copy()
    if missing_columns:
        logger.warning(f"Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¬Ø±ÛŒØ§Ù† Ù¾ÙˆÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯ ÛŒØ§ÙØª Ù†Ø´Ø¯Ù†Ø¯: {missing_columns}.")
        for col in missing_columns:
            df_copy[col] = np.nan
    
    for col in required_cols:
        df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce').fillna(0)

    df_copy['individual_buy_power'] = df_copy['buy_i_volume'] / df_copy['sell_i_volume'].replace(0, np.nan)
    df_copy['individual_buy_power'] = df_copy['individual_buy_power'].replace([np.inf, -np.inf], np.nan).fillna(0)

    df_copy['individual_net_flow'] = df_copy['buy_i_volume'] - df_copy['sell_i_volume']

    df_copy['individual_buy_per_trade'] = df_copy['buy_i_volume'] / df_copy['buy_count_i'].replace(0, np.nan)
    df_copy['individual_sell_per_trade'] = df_copy['sell_i_volume'] / df_copy['sell_count_i'].replace(0, np.nan)
    df_copy['individual_buy_per_trade'] = df_copy['individual_buy_per_trade'].replace([np.inf, -np.inf], np.nan).fillna(0)
    df_copy['individual_sell_per_trade'] = df_copy['individual_sell_per_trade'].replace([np.inf, -np.inf], np.nan).fillna(0)

    if 'jdate' in df_copy.columns:
        return df_copy[['jdate', 'individual_buy_power', 'individual_net_flow', 'individual_buy_per_trade', 'individual_sell_per_trade']].copy()
    else:
        return df_copy[['individual_buy_power', 'individual_net_flow', 'individual_buy_per_trade', 'individual_sell_per_trade']].copy()

def calculate_z_score(series: pd.Series) -> Optional[float]:
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ Z-Score Ø¨Ø±Ø§ÛŒ ÛŒÚ© pandas Series.
    Args:
        series (pd.Series): Ø³Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ.
    Returns:
        Optional[float]: Ù…Ù‚Ø¯Ø§Ø± Z-Score Ø¢Ø®Ø±ÛŒÙ† Ù†Ù‚Ø·Ù‡ Ø¯Ø§Ø¯Ù‡ ÛŒØ§ None.
    """
    series_cleaned = pd.to_numeric(series, errors='coerce').dropna()
    if series_cleaned.empty or len(series_cleaned) < 2:
        return None
    
    mean = series_cleaned.mean()
    std = series_cleaned.std()
    
    if std == 0:
        return 0.0
        
    z_score = (series_cleaned.iloc[-1] - mean) / std
    return float(z_score)

# --- ØªÙˆØ§Ø¨Ø¹ Ø¬Ø¯ÛŒØ¯ ---

def calculate_stochastic(high: pd.Series, low: pd.Series, close: pd.Series, window: int = 14, smooth_k: int = 3, smooth_d: int = 3) -> Tuple[pd.Series, pd.Series]:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Stochastic Oscillator (%K Ùˆ %D)."""
    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ø§ÙÛŒ Ù‡Ø³ØªÙ†Ø¯
    if len(close.dropna()) < window:
        nan_series = pd.Series([np.nan] * len(close), index=close.index)
        return nan_series, nan_series

    low_min = low.rolling(window=window).min()
    high_max = high.rolling(window=window).max()

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ %K
    k = 100 * ((close - low_min) / (high_max - low_min).replace(0, np.nan))

    # Ù‡Ù…ÙˆØ§Ø±Ø³Ø§Ø²ÛŒ %K Ø¨Ø±Ø§ÛŒ %D
    d = k.rolling(window=smooth_k).mean()

    return k, d

def calculate_squeeze_momentum(df: pd.DataFrame, bb_window=20, bb_std=2, kc_window=20, kc_mult=1.5) -> Tuple[pd.Series, pd.Series]:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Squeeze Momentum Indicator."""
    close = pd.to_numeric(df['close'].squeeze(), errors='coerce')
    high = pd.to_numeric(df['high'].squeeze(), errors='coerce')
    low = pd.to_numeric(df['low'].squeeze(), errors='coerce')

    # 1. Bollinger Bands
    bb_ma = calculate_sma(close, bb_window)
    bb_std_dev = close.rolling(window=bb_window).std()
    bb_upper = bb_ma + (bb_std_dev * bb_std)
    bb_lower = bb_ma - (bb_std_dev * bb_std)

    # 2. Keltner Channels
    atr = calculate_atr(high, low, close, kc_window)
    kc_ma = calculate_sma(close, kc_window)
    kc_upper = kc_ma + (atr * kc_mult)
    kc_lower = kc_ma - (atr * kc_mult)

    # 3. Squeeze condition (1: Squeeze ON, 0: Squeeze OFF)
    squeeze_on = (bb_lower > kc_lower) & (bb_upper < kc_upper)

    # 4. Momentum (Ø¨Ø± Ø§Ø³Ø§Ø³ Ù‚ÛŒÙ…Øª Ø¨Ø³ØªÙ‡â€ŒØ´Ø¯Ù† Ùˆ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ†-Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ±ÛŒÙ†)
    # TTM Momentum (Ø³Ø§Ø¯Ù‡ Ø´Ø¯Ù‡): Ø§Ø² ÛŒÚ© Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø³Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡ Ø§Ø² Ù‚ÛŒÙ…Øªâ€ŒÙ‡Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    avg_price = (high + low + close) / 3
    momentum = avg_price - calculate_sma(avg_price, bb_window)

    return squeeze_on.astype(int).reindex(df.index), momentum.reindex(df.index)

def calculate_halftrend(df: pd.DataFrame, amplitude=2, channel_deviation=2) -> Tuple[pd.Series, pd.Series]:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± HalfTrend (Ø±ÙˆÙ†Ø¯ Ùˆ Ø³ÛŒÚ¯Ù†Ø§Ù„)."""
    try:
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
        high = pd.to_numeric(df['high'].squeeze(), errors='coerce')
        low = pd.to_numeric(df['low'].squeeze(), errors='coerce')
        close = pd.to_numeric(df['close'].squeeze(), errors='coerce')

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ATR Ø¨Ø§ Ù¾Ø±ÛŒÙˆØ¯ Ù¾ÛŒØ´ ÙØ±Ø¶ 14 (Ø§Ø² 100 Ø¯Ø± Ú©Ø¯ Ù‚Ø¨Ù„ÛŒ ØµØ±Ù Ù†Ø¸Ø± Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ø¨Ø§Ø´Ø¯)
        atr = calculate_atr(high, low, close, period=14)

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ MA Ø§Ø² High Ùˆ Low Ø¯Ø± Ø¨Ø§Ø²Ù‡ Amplitude
        high_ma = high.rolling(window=amplitude).mean()
        low_ma = low.rolling(window=amplitude).mean()

        # Ø¢Ù…Ø§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆÙ‚Øª
        trend = np.zeros(len(df), dtype=int)
        next_trend = np.zeros(len(df), dtype=int)

        close_list = close.to_list()
        low_ma_list = low_ma.to_list()
        high_ma_list = high_ma.to_list()

        for i in range(1, len(df)):
            # Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ù‚Ø§Ø¯ÛŒØ± NaN
            prev_low_ma = low_ma_list[i-1] if i > 0 and not pd.isna(low_ma_list[i-1]) else close_list[i-1] if i > 0 else close_list[i]
            prev_high_ma = high_ma_list[i-1] if i > 0 and not pd.isna(high_ma_list[i-1]) else close_list[i-1] if i > 0 else close_list[i]

            # Ù…Ù†Ø·Ù‚ Ø§ØµÙ„ÛŒ (Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒâ€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø¯Ú¯ÛŒ Ùˆ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ù†Ø·Ù‚ Ø§ØµÙ„ÛŒ HalfTrend)
            # 1 = ØµØ¹ÙˆØ¯ÛŒ (Buy/Long), -1 = Ù†Ø²ÙˆÙ„ÛŒ (Sell/Short)

            if next_trend[i-1] == 1:
                if close_list[i] < prev_low_ma:
                    trend[i] = -1
                else:
                    trend[i] = 1
            else: # next_trend[i-1] == -1
                if close_list[i] > prev_high_ma:
                    trend[i] = 1
                else:
                    trend[i] = -1

            # ØªØ¹ÛŒÛŒÙ† Ø±ÙˆÙ†Ø¯ Ø¨Ø¹Ø¯ÛŒ
            if trend[i] == trend[i-1]:
                next_trend[i] = trend[i-1]
            else:
                next_trend[i] = trend[i]

        halftrend_trend = pd.Series(next_trend, index=df.index, dtype=int)

        # Ø³ÛŒÚ¯Ù†Ø§Ù„: ØªØºÛŒÛŒØ± Ø§Ø² -1 Ø¨Ù‡ 1 (Ø®Ø±ÛŒØ¯) ÛŒØ§ Ø¨Ø§Ù„Ø¹Ú©Ø³ (ÙØ±ÙˆØ´).
        halftrend_signal = (halftrend_trend != halftrend_trend.shift(1)).astype(int)

        return halftrend_trend, halftrend_signal

    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ÛŒ Ø¨Ø­Ø±Ø§Ù†ÛŒ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ HalfTrend Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ù†Ù…Ø§Ø¯: {e}", exc_info=True)
        nan_series = pd.Series([np.nan] * len(df), index=df.index)
        return nan_series, nan_series

def calculate_support_resistance_break(df: pd.DataFrame, window=50) -> Tuple[pd.Series, pd.Series]:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø³Ø§Ø¯Ù‡ Ø´Ú©Ø³Øª Ù…Ù‚Ø§ÙˆÙ…Øª (Ù…Ù‚Ø§ÙˆÙ…Øª 50 Ø±ÙˆØ²Ù‡)."""
    close = pd.to_numeric(df['close'].squeeze(), errors='coerce')
    high = pd.to_numeric(df['high'].squeeze(), errors='coerce')

    # Ù…Ù‚Ø§ÙˆÙ…Øª: Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† High Ø¯Ø± N Ø±ÙˆØ² Ú¯Ø°Ø´ØªÙ‡ (Ø¨Ù‡ Ø¬Ø² Ø§Ù…Ø±ÙˆØ²)
    resistance = high.shift(1).rolling(window=window).max()

    # Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø´Ú©Ø³Øª: Ù‚ÛŒÙ…Øª Ù¾Ø§ÛŒØ§Ù†ÛŒ Ø§Ù…Ø±ÙˆØ² > Ù…Ù‚Ø§ÙˆÙ…Øª
    resistance_broken = close > resistance

    # 1: Ø´Ú©Ø³Øª Ø±Ø® Ø¯Ø§Ø¯Ù‡ØŒ 0: Ø´Ú©Ø³Øª Ø±Ø® Ù†Ø¯Ø§Ø¯Ù‡
    resistance_broken_int = resistance_broken.astype(int)

    return resistance.astype(float).reindex(df.index), resistance_broken_int.reindex(df.index)

# -----------------------------------------------------------
# ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ ØªØ¬Ù…ÛŒØ¹ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ - Ù†Ø³Ø®Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡
# -----------------------------------------------------------
def calculate_all_indicators(df: pd.DataFrame) -> pd.DataFrame:
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªÙ…Ø§Ù… Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ùˆ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ùˆ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¢Ù†Ù‡Ø§ Ø¨Ù‡ DataFrame.
    """
    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø®Ø§Ù„ÛŒ Ù†ÛŒØ³Øª Ùˆ Ø¯Ø§Ø±Ø§ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ Ø§Ø³Øª
    required_cols = {'open', 'high', 'low', 'close', 'volume'}
    if df.empty or not required_cols.issubset(df.columns):
        logger.warning("DataFrame Ø®Ø§Ù„ÛŒ Ø§Ø³Øª ÛŒØ§ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ø²Ù… Ø±Ø§ Ù†Ø¯Ø§Ø±Ø¯.")
        return df

    try:
        # Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù¾ÛŒ Ø§Ø² DataFrame Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² SettingWithCopyWarning
        df_result = df.copy()
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ Ø¨Ù‡ Ù†ÙˆØ¹ Ø¹Ø¯Ø¯ÛŒ Ùˆ Ø­Ø°Ù Ù…Ù‚Ø§Ø¯ÛŒØ± Ù†Ø§Ù…Ø¹ØªØ¨Ø±
        for col in required_cols:
            if not pd.api.types.is_numeric_dtype(df_result[col]):
                df_result[col] = pd.to_numeric(df_result[col], errors='coerce')
        
        # Ø­Ø°Ù Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¶Ø±ÙˆØ±ÛŒ Ù†Ø¯Ø§Ø±Ù†Ø¯
        df_result.dropna(subset=list(required_cols), inplace=True)

        if df_result.empty:
            logger.warning("Ù¾Ø³ Ø§Ø² ØªØ¨Ø¯ÛŒÙ„ Ùˆ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒØŒ Ø¯ÛŒØªØ§ÛŒ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ Ø¨Ø§Ù‚ÛŒ Ù†Ù…Ø§Ù†Ø¯.")
            return df

        # --- Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ ---
        df_result['RSI'] = calculate_rsi(df_result['close'])

        macd, signal, histogram = calculate_macd(df_result['close'])
        df_result['MACD'] = macd
        df_result['MACD_Signal'] = signal
        df_result['MACD_Histogram'] = histogram

        df_result['SMA_20'] = calculate_sma(df_result['close'], 20)
        df_result['SMA_50'] = calculate_sma(df_result['close'], 50)

        upper, middle, lower = calculate_bollinger_bands(df_result['close'])
        df_result['Bollinger_Upper'] = upper
        df_result['Bollinger_Middle'] = middle
        df_result['Bollinger_Lower'] = lower

        df_result['Volume_MA_20'] = calculate_volume_ma(df_result['volume'], 20)
        df_result['ATR'] = calculate_atr(df_result['high'], df_result['low'], df_result['close'])

        # --- Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ ---
        stochastic_k, stochastic_d = calculate_stochastic(df_result['high'], df_result['low'], df_result['close'])
        df_result['Stochastic_K'] = stochastic_k
        df_result['Stochastic_D'] = stochastic_d

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Squeeze Momentum
        squeeze_on, squeeze_momentum = calculate_squeeze_momentum(df_result)
        df_result['squeeze_on'] = squeeze_on
        df_result['squeeze_momentum'] = squeeze_momentum

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ HalfTrend
        halftrend_trend, halftrend_signal = calculate_halftrend(df_result)
        df_result['halftrend_trend'] = halftrend_trend
        df_result['halftrend_signal'] = halftrend_signal

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ú©Ø³Øª Ù…Ù‚Ø§ÙˆÙ…Øª
        resistance_level, resistance_broken = calculate_support_resistance_break(df_result)
        df_result['resistance_level_50d'] = resistance_level
        df_result['resistance_broken'] = resistance_broken

        # --- Ù…Ø­Ø§Ø³Ø¨Ø§Øª ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„ Ùˆ Ø³Ù†ØªÛŒÙ…Ù†Øª ---
        df_result = calculate_fundamental_metrics(df_result)
        df_result = calculate_market_sentiment(df_result)
        df_result = detect_anomalies(df_result)

        logger.info("âœ… Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯")
        return df_result

    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ÛŒ Ø¨Ø­Ø±Ø§Ù†ÛŒ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§: {e}", exc_info=True)
        # Ø¯Ø± ØµÙˆØ±Øª Ø¨Ø±ÙˆØ² Ø®Ø·Ø§ÛŒ Ø¨Ø­Ø±Ø§Ù†ÛŒØŒ DataFrame Ø§ØµÙ„ÛŒ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†ÛŒÙ…
        return df

# -----------------------------------------------------------
# ØªÙˆØ§Ø¨Ø¹ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ (Candlestick Pattern Detection) - Ù†Ø³Ø®Ù‡ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ
# -----------------------------------------------------------

def check_candlestick_patterns(today_record: dict, yesterday_record: dict, historical_df: pd.DataFrame) -> List[str]:
    """
    ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ Ø¨Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ùˆ Ù…Ù†Ø·Ù‚ Ø¨Ø§Ø²Ø§Ø± Ø³Ø±Ù…Ø§ÛŒÙ‡.
    """
    patterns = []
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ù…Ø±ÙˆØ² Ùˆ Ø¯ÛŒØ±ÙˆØ²
    close = float(today_record['close'])
    open_ = float(today_record['open'])
    high = float(today_record['high'])
    low = float(today_record['low'])
    volume = float(today_record['volume'])
    
    prev_close = float(yesterday_record['close'])
    prev_open = float(yesterday_record['open'])
    prev_high = float(yesterday_record['high'])
    prev_low = float(yesterday_record['low'])
    
    # Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ù¾Ø§ÛŒÙ‡
    body = abs(close - open_)
    total_range = high - low
    lower_shadow = min(open_, close) - low
    upper_shadow = high - max(open_, close)
    
    # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø­Ø¬Ù… 20 Ø±ÙˆØ²Ù‡ Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„ØªØ± Ù†ÙˆÛŒØ²
    volume_ma_20 = historical_df['volume'].tail(20).mean() if len(historical_df) >= 20 else volume
    
    # ğŸ”§ ÙÛŒÙ„ØªØ± Ø§ÙˆÙ„ÛŒÙ‡: Ø­Ø°Ù Ø³Ù‡Ù…â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ø­Ø¬Ù… Ø¨Ø³ÛŒØ§Ø± Ù¾Ø§ÛŒÛŒÙ† (Ù†ÙˆÛŒØ²)
    if volume < volume_ma_20 * 0.3:
        return patterns  # Ø­Ø¬Ù… Ø¨Ø³ÛŒØ§Ø± Ú©Ù… - Ø§Ø­ØªÙ…Ø§Ù„ Ù†ÙˆÛŒØ² Ø¨Ø§Ù„Ø§
    
    # 1. **Ø¯ÙˆØ¬ÛŒ (Doji) - ØªØ¹Ø±ÛŒÙ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±**
    if total_range > 0:
        body_to_range_ratio = body / total_range
        
        # Ø¯ÙˆØ¬ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ: Ø¨Ø¯Ù†Ù‡ Ø¨Ø³ÛŒØ§Ø± Ú©ÙˆÚ†Ú© (Ú©Ù…ØªØ± Ø§Ø² 5% Ø§Ø² Ú©Ù„ range)
        if body_to_range_ratio < 0.05:
            # ÙÛŒÙ„ØªØ± Ø­Ø¬Ù…: Ø¯ÙˆØ¬ÛŒ Ø¨Ø§ Ø­Ø¬Ù… Ø¨Ø§Ù„Ø§ Ù…Ø¹ØªØ¨Ø±ØªØ± Ø§Ø³Øª
            if volume >= volume_ma_20 * 0.7:
                # ØªØ´Ø®ÛŒØµ Ù†ÙˆØ¹ Ø¯ÙˆØ¬ÛŒ
                if upper_shadow > lower_shadow * 2:
                    patterns.append("Gravestone_Doji (Ø¯ÙˆØ¬ÛŒ Ø³Ù†Ú¯ Ù‚Ø¨Ø±)")
                elif lower_shadow > upper_shadow * 2:
                    patterns.append("Dragonfly_Doji (Ø¯ÙˆØ¬ÛŒ Ø³Ù†Ø¬Ø§Ù‚Ú©)")
                else:
                    patterns.append("Doji (Ø¯ÙˆØ¬ÛŒ)")
    
    # 2. **Ú†Ú©Ø´ (Hammer) Ùˆ Ù…Ø±Ø¯ Ø¢ÙˆÛŒØ²Ø§Ù† (Hanging Man)**
    if total_range > 0 and body > 0:
        lower_shadow_ratio = lower_shadow / total_range
        upper_shadow_ratio = upper_shadow / total_range
        body_ratio = body / total_range
        
        # Ú†Ú©Ø´: Ø³Ø§ÛŒÙ‡ Ù¾Ø§ÛŒÛŒÙ†ÛŒ Ø­Ø¯Ø§Ù‚Ù„ 2 Ø¨Ø±Ø§Ø¨Ø± Ø¨Ø¯Ù†Ù‡ØŒ Ø³Ø§ÛŒÙ‡ Ø¨Ø§Ù„Ø§ÛŒÛŒ Ú©ÙˆÚ†Ú©
        is_hammer_shape = (lower_shadow_ratio >= 0.6 and 
                          upper_shadow_ratio <= 0.1 and 
                          body_ratio <= 0.3)
        
        if is_hammer_shape:
            # ØªØ´Ø®ÛŒØµ Ø±ÙˆÙ†Ø¯ Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§ÛŒØ² Ú†Ú©Ø´ Ùˆ Ù…Ø±Ø¯ Ø¢ÙˆÛŒØ²Ø§Ù†
            if len(historical_df) >= 5:
                short_trend = calculate_trend(historical_df.tail(5))
                if short_trend < -0.1:  # Ø±ÙˆÙ†Ø¯ Ù†Ø²ÙˆÙ„ÛŒ
                    patterns.append("Hammer (Ú†Ú©Ø´)")
                elif short_trend > 0.1:  # Ø±ÙˆÙ†Ø¯ ØµØ¹ÙˆØ¯ÛŒ  
                    patterns.append("Hanging_Man (Ù…Ø±Ø¯ Ø¢ÙˆÛŒØ²Ø§Ù†)")
    
    # 3. **Ù¾ÙˆØ´Ø§ÛŒ ØµØ¹ÙˆØ¯ÛŒ (Bullish Engulfing) - Ù…Ù†Ø·Ù‚ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±**
    if (prev_close < prev_open and  # Ø±ÙˆØ² Ù‚Ø¨Ù„ Ù†Ø²ÙˆÙ„ÛŒ
        close > open_ and           # Ø§Ù…Ø±ÙˆØ² ØµØ¹ÙˆØ¯ÛŒ
        open_ < prev_close and      # Ø¨Ø§Ø² Ø´Ø¯Ù† Ø§Ù…Ø±ÙˆØ² Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ± Ø§Ø² Ø¨Ø³ØªÙ‡ Ø´Ø¯Ù† Ø¯ÛŒØ±ÙˆØ²
        close > prev_open):         # Ø¨Ø³ØªÙ‡ Ø´Ø¯Ù† Ø§Ù…Ø±ÙˆØ² Ø¨Ø§Ù„Ø§ØªØ± Ø§Ø² Ø¨Ø§Ø² Ø´Ø¯Ù† Ø¯ÛŒØ±ÙˆØ²
        
        # ÙÛŒÙ„ØªØ± Ø§Ù†Ø¯Ø§Ø²Ù‡: Ø¨Ø¯Ù†Ù‡ Ø§Ù…Ø±ÙˆØ² Ø¨Ø§ÛŒØ¯ Ø­Ø¯Ø§Ù‚Ù„ 1.5 Ø¨Ø±Ø§Ø¨Ø± Ø¨Ø¯Ù†Ù‡ Ø¯ÛŒØ±ÙˆØ² Ø¨Ø§Ø´Ø¯
        today_body = close - open_
        yesterday_body = prev_open - prev_close  # Ù…Ù†ÙÛŒ Ú†ÙˆÙ† Ù†Ø²ÙˆÙ„ÛŒ Ø§Ø³Øª
        if today_body > abs(yesterday_body) * 1.5:
            patterns.append("Bullish_Engulfing (Ù¾ÙˆØ´Ø§ÛŒ ØµØ¹ÙˆØ¯ÛŒ)")
    
    # 4. **Ù¾ÙˆØ´Ø§ÛŒ Ù†Ø²ÙˆÙ„ÛŒ (Bearish Engulfing)**
    if (prev_close > prev_open and  # Ø±ÙˆØ² Ù‚Ø¨Ù„ ØµØ¹ÙˆØ¯ÛŒ
        close < open_ and           # Ø§Ù…Ø±ÙˆØ² Ù†Ø²ÙˆÙ„ÛŒ
        open_ > prev_close and      # Ø¨Ø§Ø² Ø´Ø¯Ù† Ø§Ù…Ø±ÙˆØ² Ø¨Ø§Ù„Ø§ØªØ± Ø§Ø² Ø¨Ø³ØªÙ‡ Ø´Ø¯Ù† Ø¯ÛŒØ±ÙˆØ²  
        close < prev_open):         # Ø¨Ø³ØªÙ‡ Ø´Ø¯Ù† Ø§Ù…Ø±ÙˆØ² Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ± Ø§Ø² Ø¨Ø§Ø² Ø´Ø¯Ù† Ø¯ÛŒØ±ÙˆØ²
        
        today_body = open_ - close
        yesterday_body = prev_close - prev_open
        if today_body > yesterday_body * 1.5:
            patterns.append("Bearish_Engulfing (Ù¾ÙˆØ´Ø§ÛŒ Ù†Ø²ÙˆÙ„ÛŒ)")
    
    # 5. **Ø³ØªØ§Ø±Ù‡ ØµØ¨Ø­Ú¯Ø§Ù‡ÛŒ (Morning Star) - Ù…Ù†Ø·Ù‚ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±**
    if len(historical_df) >= 3:
        day3 = historical_df.iloc[-1]  # Ø§Ù…Ø±ÙˆØ²
        day2 = historical_df.iloc[-2]  # Ø¯ÛŒØ±ÙˆØ²  
        day1 = historical_df.iloc[-3]  # Ù¾Ø±ÛŒØ±ÙˆØ²
        
        day1_close = float(day1['close'])
        day1_open = float(day1['open'])
        day2_close = float(day2['close']) 
        day2_open = float(day2['open'])
        day3_close = float(day3['close'])
        day3_open = float(day3['open'])
        
        # ğŸ”§ Ø§ØµÙ„Ø§Ø­: ØªØ¹Ø±ÛŒÙ day3_range
        day1_range = float(day1['high']) - float(day1['low'])
        day2_range = float(day2['high']) - float(day2['low'])
        day3_range = float(day3['high']) - float(day3['low'])  # Ø§ÛŒÙ† Ø®Ø· Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
        
        # Ø±ÙˆØ² Ø§ÙˆÙ„: Ø´Ù…Ø¹ Ø¨Ø²Ø±Ú¯ Ù†Ø²ÙˆÙ„ÛŒ
        day1_body = abs(day1_close - day1_open)
        is_day1_bearish = (day1_close < day1_open and 
                          day1_body > day1_range * 0.6)
        
        # Ø±ÙˆØ² Ø¯ÙˆÙ…: Ø´Ù…Ø¹ Ú©ÙˆÚ†Ú© Ø¨Ø§ Ú¯Ù¾ Ù†Ø²ÙˆÙ„ÛŒ
        day2_body = abs(day2_close - day2_open)
        is_day2_small = (day2_body < day2_range * 0.3)
        has_gap_down = (day2_open < day1_close)  # Ú¯Ù¾ Ù†Ø²ÙˆÙ„ÛŒ
        
        # Ø±ÙˆØ² Ø³ÙˆÙ…: Ø´Ù…Ø¹ Ø¨Ø²Ø±Ú¯ ØµØ¹ÙˆØ¯ÛŒ Ú©Ù‡ ÙˆØ§Ø±Ø¯ Ø¨Ø¯Ù†Ù‡ Ø±ÙˆØ² Ø§ÙˆÙ„ Ø´ÙˆØ¯
        day3_body = day3_close - day3_open  # Ù…Ø«Ø¨Øª Ú†ÙˆÙ† ØµØ¹ÙˆØ¯ÛŒ
        is_day3_bullish = (day3_close > day3_open and 
                          day3_body > day3_range * 0.6)
        penetrates_day1 = (day3_close > (day1_open + day1_close) / 2)
        
        if is_day1_bearish and is_day2_small and has_gap_down and is_day3_bullish and penetrates_day1:
            patterns.append("Morning_Star (Ø³ØªØ§Ø±Ù‡ ØµØ¨Ø­Ú¯Ø§Ù‡ÛŒ)")
    
    # 6. **Ø³ØªØ§Ø±Ù‡ Ø¹ØµØ±Ú¯Ø§Ù‡ÛŒ (Evening Star)**
    if len(historical_df) >= 3:
        day3 = historical_df.iloc[-1]  # Ø§Ù…Ø±ÙˆØ²
        day2 = historical_df.iloc[-2]  # Ø¯ÛŒØ±ÙˆØ²
        day1 = historical_df.iloc[-3]  # Ù¾Ø±ÛŒØ±ÙˆØ²
        
        day1_close = float(day1['close'])
        day1_open = float(day1['open'])
        day2_close = float(day2['close'])
        day2_open = float(day2['open']) 
        day3_close = float(day3['close'])
        day3_open = float(day3['open'])
        
        # ğŸ”§ Ø§ØµÙ„Ø§Ø­: ØªØ¹Ø±ÛŒÙ day3_range
        day1_range = float(day1['high']) - float(day1['low'])
        day2_range = float(day2['high']) - float(day2['low'])
        day3_range = float(day3['high']) - float(day3['low'])  # Ø§ÛŒÙ† Ø®Ø· Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
        
        # Ø±ÙˆØ² Ø§ÙˆÙ„: Ø´Ù…Ø¹ Ø¨Ø²Ø±Ú¯ ØµØ¹ÙˆØ¯ÛŒ
        day1_body = day1_close - day1_open
        is_day1_bullish = (day1_close > day1_open and 
                          day1_body > day1_range * 0.6)
        
        # Ø±ÙˆØ² Ø¯ÙˆÙ…: Ø´Ù…Ø¹ Ú©ÙˆÚ†Ú© Ø¨Ø§ Ú¯Ù¾ ØµØ¹ÙˆØ¯ÛŒ  
        day2_body = abs(day2_close - day2_open)
        is_day2_small = (day2_body < day2_range * 0.3)
        has_gap_up = (day2_open > day1_close)  # Ú¯Ù¾ ØµØ¹ÙˆØ¯ÛŒ
        
        # Ø±ÙˆØ² Ø³ÙˆÙ…: Ø´Ù…Ø¹ Ø¨Ø²Ø±Ú¯ Ù†Ø²ÙˆÙ„ÛŒ Ú©Ù‡ ÙˆØ§Ø±Ø¯ Ø¨Ø¯Ù†Ù‡ Ø±ÙˆØ² Ø§ÙˆÙ„ Ø´ÙˆØ¯
        day3_body = day3_open - day3_close  # Ù…Ø«Ø¨Øª Ú†ÙˆÙ† Ù†Ø²ÙˆÙ„ÛŒ
        is_day3_bearish = (day3_close < day3_open and 
                          day3_body > day3_range * 0.6)
        penetrates_day1 = (day3_close < (day1_open + day1_close) / 2)
        
        if is_day1_bullish and is_day2_small and has_gap_up and is_day3_bearish and penetrates_day1:
            patterns.append("Evening_Star (Ø³ØªØ§Ø±Ù‡ Ø¹ØµØ±Ú¯Ø§Ù‡ÛŒ)")
    
    # 7. **Ø´ÙˆØªÛŒÙ†Ú¯ Ø§Ø³ØªØ§Ø± (Shooting Star)**
    if total_range > 0 and close < open_:  # Ø´Ù…Ø¹ Ù†Ø²ÙˆÙ„ÛŒ
        upper_shadow_ratio = upper_shadow / total_range
        body_ratio = body / total_range
        
        if (upper_shadow_ratio >= 0.6 and  # Ø³Ø§ÛŒÙ‡ Ø¨Ø§Ù„Ø§ÛŒÛŒ Ø¨Ù„Ù†Ø¯
            body_ratio <= 0.3 and          # Ø¨Ø¯Ù†Ù‡ Ú©ÙˆÚ†Ú©
            lower_shadow < body):          # Ø³Ø§ÛŒÙ‡ Ù¾Ø§ÛŒÛŒÙ†ÛŒ Ú©ÙˆØªØ§Ù‡
            
            patterns.append("Shooting_Star (Ø´ÙˆØªÛŒÙ†Ú¯ Ø§Ø³ØªØ§Ø±)")

    # 8. **Ù‡Ø§Ø±Ø§Ù…ÛŒ (Harami) - Ø§Ù„Ú¯ÙˆÛŒ Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ Ú©ÙˆÚ†Ú©**
    if (abs(prev_close - prev_open) > 0 and  # Ú©Ù†Ø¯Ù„ Ù‚Ø¨Ù„ÛŒ Ø¨Ø¯Ù†Ù‡ Ø¨Ø²Ø±Ú¯ÛŒ Ø¯Ø§Ø±Ø¯
        body < abs(prev_close - prev_open) * 0.5 and  # Ú©Ù†Ø¯Ù„ Ø§Ù…Ø±ÙˆØ² Ø¯Ø§Ø®Ù„ Ø¨Ø¯Ù†Ù‡ Ø¯ÛŒØ±ÙˆØ² Ø§Ø³Øª
        min(open_, close) > min(prev_open, prev_close) and
        max(open_, close) < max(prev_open, prev_close)):
        
        if prev_close < prev_open and close > open_:  # Ù‡Ø§Ø±Ø§Ù…ÛŒ ØµØ¹ÙˆØ¯ÛŒ
            patterns.append("Bullish_Harami (Ù‡Ø§Ø±Ø§Ù…ÛŒ ØµØ¹ÙˆØ¯ÛŒ)")
        elif prev_close > prev_open and close < open_:  # Ù‡Ø§Ø±Ø§Ù…ÛŒ Ù†Ø²ÙˆÙ„ÛŒ
            patterns.append("Bearish_Harami (Ù‡Ø§Ø±Ø§Ù…ÛŒ Ù†Ø²ÙˆÙ„ÛŒ)")
    
    # 9. **Ù¾ÛŒØ±Ø³ÛŒÙ†Ú¯ Ù„Ø§ÛŒÙ† (Piercing Line) Ùˆ Ø¯Ø§Ø±Ú© Ú©Ù„ÙˆØ¯ Ú©Ø§ÙˆØ± (Dark Cloud Cover)**
    if prev_close < prev_open:  # Ø±ÙˆØ² Ù‚Ø¨Ù„ Ù†Ø²ÙˆÙ„ÛŒ
        today_body = close - open_
        yesterday_body = prev_open - prev_close
        penetration = (close - prev_close) / (prev_open - prev_close)
        if penetration > 0.5:  # Ú©Ù†Ø¯Ù„ Ø§Ù…Ø±ÙˆØ² Ø­Ø¯Ø§Ù‚Ù„ 50% Ú©Ù†Ø¯Ù„ Ø¯ÛŒØ±ÙˆØ² Ø±Ø§ Ù¾ÙˆØ´Ø´ Ø¯Ù‡Ø¯
            patterns.append("Piercing_Line (Ù¾ÛŒØ±Ø³ÛŒÙ†Ú¯ Ù„Ø§ÛŒÙ†)")
    
    elif prev_close > prev_open:  # Ø±ÙˆØ² Ù‚Ø¨Ù„ ØµØ¹ÙˆØ¯ÛŒ
        today_body = open_ - close
        yesterday_body = prev_close - prev_open
        penetration = (prev_close - close) / (prev_close - prev_open)
        if penetration > 0.5:  # Ú©Ù†Ø¯Ù„ Ø§Ù…Ø±ÙˆØ² Ø­Ø¯Ø§Ù‚Ù„ 50% Ú©Ù†Ø¯Ù„ Ø¯ÛŒØ±ÙˆØ² Ø±Ø§ Ù¾ÙˆØ´Ø´ Ø¯Ù‡Ø¯
            patterns.append("Dark_Cloud_Cover (Ø¯Ø§Ø±Ú© Ú©Ù„ÙˆØ¯ Ú©Ø§ÙˆØ±)")
    
    return patterns

def calculate_trend(df: pd.DataFrame, period: int = 5) -> float:
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø±ÙˆÙ†Ø¯ Ù‚ÛŒÙ…ØªÛŒ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø¬Ù‡Øª Ø¨Ø§Ø²Ø§Ø±.
    """
    if len(df) < period:
        return 0.0
    
    recent_data = df.tail(period)
    start_price = float(recent_data.iloc[0]['close'])
    end_price = float(recent_data.iloc[-1]['close'])
    
    if start_price > 0:
        return (end_price - start_price) / start_price
    return 0.0

def is_valid_pattern(patterns: List[str], volume: float, avg_volume: float) -> bool:
    """
    Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø­Ø¬Ù… Ùˆ Ù†ÙˆÛŒØ².
    """
    if not patterns:
        return False
    
    # ÙÛŒÙ„ØªØ± Ø­Ø¬Ù…: Ø§Ù„Ú¯Ùˆ Ø¨Ø§ Ø­Ø¬Ù… Ù¾Ø§ÛŒÛŒÙ† Ø§Ø¹ØªØ¨Ø§Ø± Ú©Ù…ØªØ±ÛŒ Ø¯Ø§Ø±Ø¯
    volume_ratio = volume / avg_volume if avg_volume > 0 else 1.0
    
    # Ø¨Ø±Ø§ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…Ù‡Ù…ØŒ Ø­Ø¬Ù… Ø¨Ø§ÛŒØ¯ Ø¨Ø§Ù„Ø§ØªØ± Ø§Ø² Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø¨Ø§Ø´Ø¯
    important_patterns = ['Bullish_Engulfing', 'Bearish_Engulfing', 'Morning_Star', 'Evening_Star']
    
    for pattern in patterns:
        if pattern.split(' ')[0] in important_patterns and volume_ratio < 0.8:
            return False
    
    return True

# -----------------------------------------------------------
# ØªØ§Ø¨Ø¹ Placeholder Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„
# -----------------------------------------------------------

def calculate_fundamental_metrics(df: pd.DataFrame, fundamental_df: Optional[pd.DataFrame] = None) -> pd.DataFrame:
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯.
    """
    df_copy = df.copy()
    
    try:
        # 1. Ù†Ø³Ø¨Øªâ€ŒÙ‡Ø§ÛŒ Ù‚Ø¯Ø±Øª Ø¨Ø§Ø²Ø§Ø±Ú¯Ø±Ø¯Ø§Ù†ÛŒ
        if all(col in df_copy.columns for col in ['buy_i_volume', 'sell_i_volume', 'buy_count_i', 'sell_count_i']):
            # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø­Ø¬Ù… Ù‡Ø± Ù…Ø¹Ø§Ù…Ù„Ù‡ Ø­Ù‚ÛŒÙ‚ÛŒ
            df_copy['Avg_Buy_Volume_Per_Trade'] = df_copy['buy_i_volume'] / df_copy['buy_count_i'].replace(0, np.nan)
            df_copy['Avg_Sell_Volume_Per_Trade'] = df_copy['sell_i_volume'] / df_copy['sell_count_i'].replace(0, np.nan)
            
            # Ù†Ø³Ø¨Øª Ù‚Ø¯Ø±Øª Ø®Ø±ÛŒØ¯Ø§Ø± Ø¨Ù‡ ÙØ±ÙˆØ´Ù†Ø¯Ù‡
            df_copy['Buy_Sell_Power_Ratio'] = (df_copy['buy_i_volume'] / df_copy['sell_i_volume'].replace(0, np.nan)).fillna(1)
            
            # Ø´Ø§Ø®Øµ ØªÙ…Ø±Ú©Ø² Ø®Ø±ÛŒØ¯/ÙØ±ÙˆØ´
            df_copy['Trade_Concentration'] = (df_copy['buy_i_volume'] + df_copy['sell_i_volume']) / df_copy['volume'].replace(0, np.nan)
        
        # 2. Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒ Ù†Ù‚Ø¯Ø´ÙˆÙ†Ø¯Ú¯ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡
        if 'volume' in df_copy.columns and 'final' in df_copy.columns:
            # Ø§Ø±Ø²Ø´ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø±ÙˆØ²Ø§Ù†Ù‡
            df_copy['Daily_Trade_Value'] = df_copy['volume'] * df_copy['final']
            
            # Ù†ÙˆØ³Ø§Ù† Ù‚ÛŒÙ…ØªÛŒ Ø±ÙˆØ²Ø§Ù†Ù‡
            df_copy['Daily_Price_Range'] = (df_copy['high'] - df_copy['low']) / df_copy['final'].replace(0, np.nan)
            
            # Ø´Ø§Ø®Øµ Ù†Ù‚Ø¯Ø´ÙˆÙ†Ø¯Ú¯ÛŒ (Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø±Ø²Ø´ Ùˆ Ø­Ø¬Ù…)
            df_copy['Liquidity_Score'] = (df_copy['Daily_Trade_Value'] * df_copy['volume']) / (df_copy['high'] - df_copy['low']).replace(0, np.nan)
        
        # 3. Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆÙ…Ù†ØªÙˆÙ… ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„
        if 'close' in df_copy.columns:
            # Ø¨Ø§Ø²Ø¯Ù‡ Ø±ÙˆØ²Ø§Ù†Ù‡
            df_copy['Daily_Return'] = df_copy['close'].pct_change()
            
            # Ù†ÙˆØ³Ø§Ù† Ø¨Ø§Ø²Ø¯Ù‡ (20 Ø±ÙˆØ²Ù‡)
            df_copy['Return_Volatility_20d'] = df_copy['Daily_Return'].rolling(window=20).std()
            
            # Ø´Ø§Ø®Øµ Ù‚Ø¯Ø±Øª Ù†Ø³Ø¨ÛŒ (RSI) Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ø¨Ø§Ø²Ø¯Ù‡
            df_copy['Return_RSI'] = calculate_rsi(df_copy['close'])
        
        # 4. Ø§Ø¯ØºØ§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„ Ø®Ø§Ø±Ø¬ÛŒ Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯
        if fundamental_df is not None:
            # Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ EPS, P/E, P/B Ø±Ø§ Ø§Ø¯ØºØ§Ù… Ú©Ù†Ø¯
            # ÙØ¹Ù„Ø§Ù‹ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† placeholder Ø¨Ø§Ù‚ÛŒ Ù…ÛŒâ€ŒÙ…Ø§Ù†Ø¯
            pass
            
        # 5. Ø´Ø§Ø®Øµ Ø³Ù„Ø§Ù…Øª Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ
        if all(col in df_copy.columns for col in ['num_trades', 'volume']):
            # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø­Ø¬Ù… Ù‡Ø± Ù…Ø¹Ø§Ù…Ù„Ù‡
            df_copy['Avg_Volume_Per_Trade'] = df_copy['volume'] / df_copy['num_trades'].replace(0, np.nan)
            
            # Ø´Ø§Ø®Øµ ÙØ¹Ø§Ù„ÛŒØª Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ
            df_copy['Trade_Activity_Index'] = df_copy['num_trades'] / df_copy['num_trades'].rolling(window=20).mean()
        
        # 6. Z-Score Ø¨Ø±Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ outliers
        if 'volume' in df_copy.columns:
            df_copy['Volume_Z_Score'] = calculate_z_score(df_copy['volume'])
        
        # 7. Ø´Ø§Ø®Øµ ÙØ´Ø§Ø± Ø®Ø±ÛŒØ¯/ÙØ±ÙˆØ´
        if all(col in df_copy.columns for col in ['plc', 'volume']):
            # ÙØ´Ø§Ø± Ù‚ÛŒÙ…Øª-Ø­Ø¬Ù…
            df_copy['Price_Volume_Pressure'] = df_copy['plc'] * df_copy['volume'] / df_copy['volume'].rolling(window=20).mean()
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„: {e}")
    
    return df_copy

# -----------------------------------------------------------
# ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒØ´Ø±ÙØªÙ‡
# -----------------------------------------------------------

def calculate_market_sentiment(df: pd.DataFrame) -> pd.DataFrame:
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒ Ø³Ù†ØªÛŒÙ…Ù†Øª Ø¨Ø§Ø²Ø§Ø±.
    """
    df_copy = df.copy()
    
    # 1. Ø´Ø§Ø®Øµ Ù‚Ø¯Ø±Øª Ø®Ø±ÛŒØ¯Ø§Ø±Ø§Ù† Ø­Ù‚ÛŒÙ‚ÛŒ
    if all(col in df_copy.columns for col in ['buy_i_volume', 'sell_i_volume']):
        df_copy['Individual_Net_Flow'] = df_copy['buy_i_volume'] - df_copy['sell_i_volume']
        df_copy['Individual_Net_Flow_MA'] = df_copy['Individual_Net_Flow'].rolling(window=5).mean()
    
    # 2. Ø´Ø§Ø®Øµ ØªÙ…Ø±Ú©Ø² Ù…Ø¹Ø§Ù…Ù„Ø§Øª
    if all(col in df_copy.columns for col in ['buy_i_volume', 'buy_n_volume', 'volume']):
        df_copy['Individual_Dominance'] = (df_copy['buy_i_volume'] + df_copy['sell_i_volume']) / df_copy['volume'].replace(0, np.nan)
    
    # 3. Ø´Ø§Ø®Øµ Ù…ÙˆÙ…Ù†ØªÙˆÙ… Ø¬Ù…Ø¹ÛŒ
    if 'close' in df_copy.columns:
        df_copy['Price_Momentum'] = df_copy['close'] / df_copy['close'].shift(5) - 1
        df_copy['Volume_Momentum'] = df_copy['volume'] / df_copy['volume'].shift(5) - 1
    
    return df_copy

def detect_anomalies(df: pd.DataFrame) -> pd.DataFrame:
    """
    Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ anomalies Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ.
    """
    df_copy = df.copy()
    
    # 1. Ø¢Ù†ÙˆÙ…Ø§Ù„ÛŒ Ø­Ø¬Ù…
    if 'volume' in df_copy.columns:
        volume_ma = df_copy['volume'].rolling(window=20).mean()
        volume_std = df_copy['volume'].rolling(window=20).std()
        df_copy['Volume_Anomaly'] = (df_copy['volume'] > (volume_ma + 2 * volume_std)).astype(int)
    
    # 2. Ø¢Ù†ÙˆÙ…Ø§Ù„ÛŒ Ù‚ÛŒÙ…Øª
    if 'close' in df_copy.columns:
        returns = df_copy['close'].pct_change()
        returns_std = returns.rolling(window=20).std()
        df_copy['Price_Anomaly'] = (abs(returns) > (3 * returns_std)).astype(int)
    
    # 3. Ø¢Ù†ÙˆÙ…Ø§Ù„ÛŒ Ø§Ø±Ø²Ø´ Ù…Ø¹Ø§Ù…Ù„Ø§Øª
    if all(col in df_copy.columns for col in ['volume', 'final']):
        trade_value = df_copy['volume'] * df_copy['final']
        trade_value_ma = trade_value.rolling(window=20).mean()
        trade_value_std = trade_value.rolling(window=20).std()
        df_copy['Trade_Value_Anomaly'] = (trade_value > (trade_value_ma + 2 * trade_value_std)).astype(int)
    
    return df_copy

# Ù„ÛŒØ³Øª ØªÙˆØ§Ø¨Ø¹ÛŒ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ import Ø´Ø¯Ù† Ø¯Ø± Ø³Ø±ÙˆÛŒØ³ Ø§ØµÙ„ÛŒ Ù…Ø¬Ø§Ø² Ù‡Ø³ØªÙ†Ø¯
__all__ = [
    'calculate_all_indicators',
    'check_candlestick_patterns',
    'calculate_fundamental_metrics',
    'calculate_market_sentiment',
    'detect_anomalies',
    'calculate_sma',
    'calculate_rsi',
    'calculate_macd',
    'calculate_bollinger_bands',
    'calculate_atr',
    'calculate_stochastic',
    'calculate_z_score'
]
