# -*- coding: utf-8 -*-
# services/data_analysis_service.py
# Ù…Ø³Ø¦ÙˆÙ„ÛŒØª: Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ØŒ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„ Ùˆ Ø´Ù…Ø¹ÛŒ Ùˆ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†ØªØ§ÛŒØ¬ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³.

from sqlalchemy.orm import Session
from sqlalchemy import func, distinct
from sqlalchemy.exc import SQLAlchemyError
import pandas as pd
import numpy as np
import logging
from typing import Dict, List, Optional, Tuple

# ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… models.py Ùˆ technical_analysis_utils.py Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù‡Ø³ØªÙ†Ø¯
from models import HistoricalData, TechnicalIndicatorData, CandlestickPatternDetection, FundamentalData
from services.technical_analysis_utils import calculate_all_indicators, check_candlestick_patterns, calculate_fundamental_metrics
from datetime import datetime

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯ÛŒÙ†Ú¯
logger = logging.getLogger(__name__)

# -----------------------------------------------------------
# ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³
# -----------------------------------------------------------

def save_technical_indicators(db_session: Session, symbol_id: str, df_indicators: pd.DataFrame):
    """
    Ø°Ø®ÛŒØ±Ù‡/Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù†ØªØ§ÛŒØ¬ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ù†Ù…Ø§Ø¯.
    Ø§Ø² Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø­Ø°Ù Ùˆ Ø¯Ø±Ø¬ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÚ©Ø±Ø§Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ (Delete & Insert).
    """
    try:
        # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒØ´Ø§Ù† NaN Ù†ÛŒØ³Øª
        df_to_save = df_indicators.dropna(subset=['RSI', 'MACD', 'SMA_20', 'Bollinger_Middle'])

        if df_to_save.empty:
            # logger.warning(f"âš ï¸ Ù‡ÛŒÚ† Ø±Ú©ÙˆØ±Ø¯ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ {symbol_id} ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
            return 0
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± TechnicalIndicatorData
        indicators_columns = [col for col in df_to_save.columns if col not in HistoricalData.__table__.columns]
        
        records_to_insert = []
        now = datetime.now()

        for index, row in df_to_save.iterrows():
            # Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø±Ú©ÙˆØ±Ø¯ Ø¬Ø¯ÛŒØ¯
            record = {
                'symbol_id': symbol_id,
                'jdate': row['jdate'],
                'date': row['date'],
                'updated_at': now,
            }
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§
            for col in indicators_columns:
                # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ù…Ù‚Ø¯Ø§Ø± float Ø§Ø³Øª
                record[col.lower()] = float(row[col]) if pd.notna(row[col]) else None
            
            records_to_insert.append(record)
            
        if not records_to_insert:
            return 0

        # Ø­Ø°Ù Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§Ø¯ Ø¬Ø§Ø±ÛŒ
        db_session.query(TechnicalIndicatorData).filter(
            TechnicalIndicatorData.symbol_id == symbol_id
        ).delete(synchronize_session=False)

        # Ø¯Ø±Ø¬ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
        db_session.bulk_insert_mappings(TechnicalIndicatorData, records_to_insert)
        
        # db_session.commit() # âŒ COMMIT Ø¨Ø§ÛŒØ¯ Ø¯Ø± ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ run_technical_analysis Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯.
        return len(records_to_insert)

    except SQLAlchemyError as e:
        logger.error(f"âŒ Ø®Ø·Ø§ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ {symbol_id}: {e}", exc_info=True)
        raise # Ø§Ø¬Ø§Ø²Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ… Ø®Ø·Ø§ Ø¨Ù‡ ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±ÙˆØ¯ ØªØ§ rollback Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯


# -----------------------------------------------------------
# ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„
# -----------------------------------------------------------

def run_technical_analysis(db_session: Session, symbols_list: Optional[List[str]] = None, batch_size: int = 200) -> Tuple[int, str]:
    """
    Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ø¯Ø± Ø¨Ú†â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú© Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù…ØµØ±Ù Ø²ÛŒØ§Ø¯ Ø­Ø§ÙØ¸Ù‡.
    """
    try:
        logger.info("ğŸ“ˆ Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„...")

        # Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª ÛŒÚ©ØªØ§ Ø§Ø² Ù†Ù…Ø§Ø¯Ù‡Ø§
        symbol_ids_query = db_session.query(HistoricalData.symbol_id).distinct()
        if symbols_list:
            symbol_ids_query = symbol_ids_query.filter(HistoricalData.symbol_id.in_(symbols_list))

        all_symbols = [str(row[0]) for row in symbol_ids_query.all()]
        total_symbols = len(all_symbols)
        logger.info(f"ğŸ” Ù…Ø¬Ù…ÙˆØ¹ {total_symbols} Ù†Ù…Ø§Ø¯ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ ÛŒØ§ÙØª Ø´Ø¯.")

        processed_count = 0
        success_count = 0
        error_count = 0
        
        # ØªØ¹Ø±ÛŒÙ ØªÙ…Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„
        columns_to_fetch = [
            HistoricalData.symbol_id, HistoricalData.symbol_name, HistoricalData.date, HistoricalData.jdate, 
            HistoricalData.open, HistoricalData.close, HistoricalData.high, HistoricalData.low, 
            HistoricalData.volume, HistoricalData.final, HistoricalData.yesterday_price, 
            HistoricalData.buy_count_i, HistoricalData.buy_count_n, HistoricalData.sell_count_i, 
            HistoricalData.sell_count_n, HistoricalData.buy_i_volume, HistoricalData.buy_n_volume, 
            HistoricalData.sell_i_volume, HistoricalData.sell_n_volume
        ]

        # Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¯Ø± Ø¨Ú†â€ŒÙ‡Ø§ÛŒ 200ØªØ§ÛŒÛŒ
        for i in range(0, total_symbols, batch_size):
            batch_symbols = all_symbols[i:i + batch_size]
            logger.info(f"ğŸ“¦ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ú† {i // batch_size + 1}: Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ {i + 1} ØªØ§ {min(i + batch_size, total_symbols)}")

            # Ú©ÙˆØ¦Ø±ÛŒ Ø¨Ø±Ø§ÛŒ ÙÚ† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ú† Ø¬Ø§Ø±ÛŒ
            query = db_session.query(*columns_to_fetch).filter(
                HistoricalData.symbol_id.in_(batch_symbols)
            ).order_by(HistoricalData.symbol_id, HistoricalData.date)
            
            # ğŸ’¡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Pandas Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ§Ù†Ø¯Ù† Ú©ÙˆØ¦Ø±ÛŒ Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³ (Ø¨Ù‡ØªØ± Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯)
            # Ø§Ú¯Ø±Ú†Ù‡ Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø³Ø§Ø¯Ú¯ÛŒ Ù‡Ù…Ø§Ù† historical_data = query.all() Ø±Ø§ Ù†Ú¯Ù‡ Ù…ÛŒâ€ŒØ¯Ø§Ø±ÛŒÙ….
            historical_data = query.all()

            if not historical_data:
                logger.warning(f"âš ï¸ Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø¨Ú† ÛŒØ§ÙØª Ù†Ø´Ø¯.")
                continue

            # ØªØ¨Ø¯ÛŒÙ„ Ù†ØªØ§ÛŒØ¬ Ú©ÙˆØ¦Ø±ÛŒ Ø¨Ù‡ DataFrame
            df = pd.DataFrame(historical_data, columns=[col.key for col in columns_to_fetch])

            grouped = df.groupby('symbol_id')

            for symbol_id, group_df in grouped:
                try:
                    # 1. Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªÙ…Ø§Ù… Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ (ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ø§Ø² technical_analysis_utils)
                    df_indicators = calculate_all_indicators(group_df)
                    
                    # 2. Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
                    save_technical_indicators(db_session, str(symbol_id), df_indicators)
                    db_session.commit() # Commit Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù†Ù…Ø§Ø¯ (ÛŒØ§ Ù‡Ø± Ø¨Ú†) Ø§Ù…Ù†ÛŒØª Ø¨ÛŒØ´ØªØ±ÛŒ Ø¯Ø§Ø±Ø¯
                    
                    success_count += 1
                
                except Exception as e:
                    error_count += 1
                    logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„/Ø°Ø®ÛŒØ±Ù‡ Ù†Ù…Ø§Ø¯ {symbol_id}: {e}", exc_info=True)
                    db_session.rollback() # Rollback Ø¯Ø± ØµÙˆØ±Øª Ø¨Ø±ÙˆØ² Ø®Ø·Ø§ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§Ø¯ Ø®Ø§Øµ

                processed_count += 1
                if processed_count % 50 == 0:
                    logger.info(f"ğŸ“Š Ù¾ÛŒØ´Ø±ÙØª ØªØ­Ù„ÛŒÙ„: {processed_count}/{total_symbols} Ù†Ù…Ø§Ø¯")
            
            # ğŸ”¹ Ø¢Ø²Ø§Ø¯Ø³Ø§Ø²ÛŒ Ø­Ø§ÙØ¸Ù‡â€ŒÛŒ DataFrame Ø¨Ø¹Ø¯ Ø§Ø² Ù‡Ø± Ø¨Ú†
            del df
            import gc
            gc.collect()

        logger.info(f"âœ… ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ú©Ø§Ù…Ù„ Ø´Ø¯. Ù…ÙˆÙÙ‚: {success_count} | Ø®Ø·Ø§: {error_count}")
        return success_count, f"ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ Ø´Ø¯. {success_count} Ù…ÙˆÙÙ‚ØŒ {error_count} Ø®Ø·Ø§"

    except Exception as e:
        error_msg = f"âŒ Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ØªÚ©Ù†ÛŒÚ©Ø§Ù„: {e}"
        logger.error(error_msg, exc_info=True)
        db_session.rollback()
        return 0, error_msg

# -----------------------------------------------------------
# ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ
# -----------------------------------------------------------

def run_candlestick_detection(db_session: Session, symbols_list: Optional[List[str]] = None) -> Tuple[int, str]:
    """
    Ø§Ø¬Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§Ø¯Ù‡Ø§ Ø¨Ø§ ÙÚ† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø­Ø¯ÙˆØ¯ (Ù…Ø«Ù„Ø§Ù‹ Û³Û° Ø±ÙˆØ² Ø§Ø®ÛŒØ±) Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù†Ù…Ø§Ø¯.
    """
    try:
        logger.info("ğŸ•¯ï¸ Ø´Ø±ÙˆØ¹ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ...")
        
        # Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª symbol_id Ù‡Ø§ÛŒ ÙØ¹Ø§Ù„
        base_query = db_session.query(HistoricalData.symbol_id).distinct()
        if symbols_list:
            base_query = base_query.filter(HistoricalData.symbol_id.in_(symbols_list)) 
            
        symbol_ids_to_process = [str(s[0]) for s in base_query.all()]
        
        if not symbol_ids_to_process:
            logger.warning("âš ï¸ Ù‡ÛŒÚ† Ù†Ù…Ø§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            return 0, "Ù‡ÛŒÚ† Ù†Ù…Ø§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯."
            
        logger.info(f"ğŸ” ÛŒØ§ÙØª Ø´Ø¯ {len(symbol_ids_to_process)} Ù†Ù…Ø§Ø¯ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ.")

        success_count = 0
        records_to_insert = []
        
        processed_count = 0
        for symbol_id in symbol_ids_to_process:
            try:
                # ğŸ’¡ ÙÚ† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Û³Û° Ø±ÙˆØ² Ø§Ø®ÛŒØ± Ù†Ù…Ø§Ø¯ Ø¬Ø§Ø±ÛŒ
                historical_data_query = db_session.query(HistoricalData).filter(
                    HistoricalData.symbol_id == symbol_id
                ).order_by(HistoricalData.date.desc()).limit(30) 
                
                historical_data = historical_data_query.all() 
                
                if len(historical_data) < 5: 
                    continue 

                # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ DataFrame Ùˆ Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ§Ø±ÛŒØ® ØµØ¹ÙˆØ¯ÛŒ
                df = pd.DataFrame([row.__dict__ for row in historical_data])
                df.sort_values(by='date', inplace=True) 
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ú©ÙˆØ±Ø¯ Ø§Ù…Ø±ÙˆØ² Ùˆ Ø¯ÛŒØ±ÙˆØ²
                today_record_dict = df.iloc[-1].to_dict()
                yesterday_record_dict = df.iloc[-2].to_dict()
                
                # ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ ØªØ§Ø¨Ø¹ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯Ùˆ (ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ø§Ø² technical_analysis_utils)
                patterns = check_candlestick_patterns(
                    today_record_dict, 
                    yesterday_record_dict, 
                    df # Ú©Ù„ DataFrame Ù…Ø­Ø¯ÙˆØ¯ Ø´Ø¯Ù‡ (Ù…Ø«Ù„Ø§Ù‹ Û³Û° Ø±ÙˆØ²Ù‡)
                )
                
                # Ø°Ø®ÛŒØ±Ù‡ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ÛŒØ§ÙØªâ€ŒØ´Ø¯Ù‡
                if patterns:
                    now = datetime.now()
                    current_jdate = today_record_dict['jdate']
                    for pattern in patterns:
                        records_to_insert.append({
                            'symbol_id': str(symbol_id),
                            'jdate': current_jdate,
                            'pattern_name': pattern,
                            'created_at': now, 
                            'updated_at': now
                        })
                    success_count += 1
                
                processed_count += 1
                if processed_count % 50 == 0:
                    logger.info(f"ğŸ•¯ï¸ Ù¾ÛŒØ´Ø±ÙØª ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ: {processed_count}/{len(symbol_ids_to_process)} Ù†Ù…Ø§Ø¯")

            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§Ø¯ {symbol_id}: {e}", exc_info=True)
                # Rollback Ù†Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú†ÙˆÙ† bulk insert Ø¯Ø± Ù¾Ø§ÛŒØ§Ù† Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯
                
        logger.info(f"âœ… ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ Ø¨Ø±Ø§ÛŒ {success_count} Ù†Ù…Ø§Ø¯ Ø¨Ø§ Ø§Ù„Ú¯Ùˆ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")
                
        # ------------------
        # 3. Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ (Delete & Insert)
        # ------------------
        if records_to_insert:
            # Ø§Ù„Ù) Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ§Ø±ÛŒØ® Ùˆ Ù„ÛŒØ³Øª Ù†Ù…Ø§Ø¯Ù‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡
            # Ø§Ø² Ø¢Ù†Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø±ÙˆØ²Ø§Ù†Ù‡ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… jdate Ù‡Ù…Ù‡ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ ÛŒÚ©Ø³Ø§Ù† Ø§Ø³Øª.
            last_jdate = records_to_insert[0]['jdate'] 
            processed_symbol_ids = list({record['symbol_id'] for record in records_to_insert})
            
            # Ø¨) Ø­Ø°Ù Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ
            try:
                db_session.query(CandlestickPatternDetection).filter(
                    CandlestickPatternDetection.symbol_id.in_(processed_symbol_ids),
                    CandlestickPatternDetection.jdate == last_jdate
                ).delete(synchronize_session=False) 
                
                db_session.commit()
                logger.info(f"ğŸ—‘ï¸ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ Ù‚Ø¨Ù„ÛŒ ({len(processed_symbol_ids)} Ù†Ù…Ø§Ø¯) Ø¨Ø±Ø§ÛŒ {last_jdate} Ø­Ø°Ù Ø´Ø¯Ù†Ø¯.")
                
            except Exception as e:
                db_session.rollback()
                logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø­Ø°Ù Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ: {e}", exc_info=True)
                return success_count, "Ø®Ø·Ø§ Ø¯Ø± Ø­Ø°Ù Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ"
                
            # Ø¬) Ø¯Ø±Ø¬ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
            db_session.bulk_insert_mappings(CandlestickPatternDetection, records_to_insert)
            db_session.commit()
            logger.info(f"âœ… {len(records_to_insert)} Ø§Ù„Ú¯ÙˆÛŒ Ø´Ù…Ø¹ÛŒ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø±Ø¬ Ø´Ø¯.")
        else:
            logger.info("â„¹ï¸ Ù‡ÛŒÚ† Ø§Ù„Ú¯ÙˆÛŒ Ø´Ù…Ø¹ÛŒ Ø¬Ø¯ÛŒØ¯ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")

        return success_count, f"ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯. {success_count} Ù†Ù…Ø§Ø¯ Ø¯Ø§Ø±Ø§ÛŒ Ø§Ù„Ú¯Ùˆ"

    except Exception as e:
        error_msg = f"âŒ Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù…Ø¹ÛŒ: {e}"
        logger.error(error_msg, exc_info=True)
        db_session.rollback()
        return 0, error_msg

# -----------------------------------------------------------
# ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„ (Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯)
# -----------------------------------------------------------

def run_fundamental_analysis(db_session: Session, symbols_list: Optional[List[str]] = None, batch_size: int = 200) -> Tuple[int, str]:
    """
    Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„ (Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± HistoricalData) Ùˆ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†ØªØ§ÛŒØ¬.
    """
    try:
        logger.info("ğŸ’° Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„ (Ù‚Ø¯Ø±Øª Ø®Ø±ÛŒØ¯Ø§Ø±/Ø­Ø¬Ù…)...")

        # Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø§Ø² Ù‡Ù…Ø§Ù† Ù…Ù†Ø·Ù‚ Ø¨Ú† run_technical_analysis Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
        symbol_ids_query = db_session.query(HistoricalData.symbol_id).distinct()
        if symbols_list:
            symbol_ids_query = symbol_ids_query.filter(HistoricalData.symbol_id.in_(symbols_list))

        all_symbols = [str(row[0]) for row in symbol_ids_query.all()]
        total_symbols = len(all_symbols)
        logger.info(f"ğŸ” Ù…Ø¬Ù…ÙˆØ¹ {total_symbols} Ù†Ù…Ø§Ø¯ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„ ÛŒØ§ÙØª Ø´Ø¯.")

        processed_count = 0
        success_count = 0
        error_count = 0
        
        # Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„ (Ù‚Ø¯Ø±Øª Ø®Ø±ÛŒØ¯Ø§Ø±/Ø­Ø¬Ù…)
        columns_to_fetch = [
            HistoricalData.symbol_id, HistoricalData.date, HistoricalData.jdate, 
            HistoricalData.volume, HistoricalData.final, 
            HistoricalData.buy_count_i, HistoricalData.sell_count_i, 
            HistoricalData.buy_i_volume, HistoricalData.sell_i_volume
        ]

        for i in range(0, total_symbols, batch_size):
            batch_symbols = all_symbols[i:i + batch_size]
            
            query = db_session.query(*columns_to_fetch).filter(
                HistoricalData.symbol_id.in_(batch_symbols)
            ).order_by(HistoricalData.symbol_id, HistoricalData.date)
            
            historical_data = query.all()

            if not historical_data:
                continue

            df = pd.DataFrame(historical_data, columns=[col.key for col in columns_to_fetch])
            
            # Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Volume_MA_20 Ù†ÛŒØ§Ø² Ø¨Ù‡ Volume_MA_20 Ø¯Ø§Ø±ÛŒÙ… Ú©Ù‡ Ø¯Ø± calculate_all_indicators Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
            # Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒØŒ Ø§ÛŒÙ†Ø¬Ø§ Volume_MA_20 Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ø­Ø³Ø§Ø¨ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….
            
            df['Volume_MA_20'] = df.groupby('symbol_id')['volume'].transform(lambda x: calculate_volume_ma(x, 20))


            grouped = df.groupby('symbol_id')

            for symbol_id, group_df in grouped:
                try:
                    # 1. Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„/Ø¬Ø±ÛŒØ§Ù† Ø³Ø±Ù…Ø§ÛŒÙ‡ (ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ø§Ø² technical_analysis_utils)
                    df_metrics = calculate_fundamental_metrics(group_df)
                    
                    # 2. Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
                    records_to_insert = []
                    now = datetime.now()

                    # ÙÙ‚Ø· Ø¢Ø®Ø±ÛŒÙ† Ø±Ú©ÙˆØ±Ø¯ (Ø§Ù…Ø±ÙˆØ²) Ø±Ø§ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
                    last_row = df_metrics.iloc[-1]
                    
                    record = {
                        'symbol_id': str(symbol_id),
                        'jdate': last_row['jdate'],
                        'date': last_row['date'],
                        'real_power_ratio': float(last_row['Real_Power_Ratio']) if pd.notna(last_row['Real_Power_Ratio']) else None,
                        'volume_ratio_20d': float(last_row['Volume_Ratio_20d']) if pd.notna(last_row['Volume_Ratio_20d']) else None,
                        'daily_liquidity': float(last_row['Daily_Liquidity']) if pd.notna(last_row['Daily_Liquidity']) else None,
                        'updated_at': now,
                    }
                    records_to_insert.append(record)

                    # Ø­Ø°Ù Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ Ø§Ù…Ø±ÙˆØ²
                    db_session.query(FundamentalData).filter(
                        FundamentalData.symbol_id == str(symbol_id),
                        FundamentalData.jdate == last_row['jdate']
                    ).delete(synchronize_session=False)

                    # Ø¯Ø±Ø¬ Ø±Ú©ÙˆØ±Ø¯ Ø¬Ø¯ÛŒØ¯
                    db_session.bulk_insert_mappings(FundamentalData, records_to_insert)
                    db_session.commit()
                    
                    success_count += 1
                
                except Exception as e:
                    error_count += 1
                    logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„/Ø°Ø®ÛŒØ±Ù‡ Ù†Ù…Ø§Ø¯ {symbol_id}: {e}", exc_info=True)
                    db_session.rollback()

                processed_count += 1
            
            del df
            import gc
            gc.collect()

        logger.info(f"âœ… ØªØ­Ù„ÛŒÙ„ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„ Ú©Ø§Ù…Ù„ Ø´Ø¯. Ù…ÙˆÙÙ‚: {success_count} | Ø®Ø·Ø§: {error_count}")
        return success_count, f"ØªØ­Ù„ÛŒÙ„ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„ Ú©Ø§Ù…Ù„ Ø´Ø¯. {success_count} Ù…ÙˆÙÙ‚ØŒ {error_count} Ø®Ø·Ø§"

    except Exception as e:
        error_msg = f"âŒ Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„: {e}"
        logger.error(error_msg, exc_info=True)
        db_session.rollback()
        return 0, error_msg

# -----------------------------------------------------------
# Export functions for services
# -----------------------------------------------------------

__all__ = [
    'run_technical_analysis',
    'run_candlestick_detection',
    'run_fundamental_analysis'
]
